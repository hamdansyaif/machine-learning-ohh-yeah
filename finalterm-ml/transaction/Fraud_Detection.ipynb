{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6996,
     "status": "ok",
     "timestamp": 1767082269750,
     "user": {
      "displayName": "Hamdan Syaifuddin",
      "userId": "09275097033143327962"
     },
     "user_tz": -420
    },
    "id": "vCywqNNpPZ4L",
    "outputId": "0d601e28-41e5-4f72-d83a-c374554e8206"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "âš™ï¸ BLOK 1: INSTALLATION & SETUP\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "Run time: ~2 minutes\n",
    "Description: Install required libraries\n",
    "\"\"\"\n",
    "\n",
    "# Install libraries (run once per session)\n",
    "!pip install polars optuna imbalanced-learn catboost lightgbm xgboost -q\n",
    "\n",
    "print(\"âœ… Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(f\"Python: {sys.executable}\")\n",
    "\n",
    "# Test imports\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "import optuna\n",
    "\n",
    "print(\"\\nâœ… ALL LIBRARIES LOADED!\")\n",
    "print(f\"LightGBM: {lgb.__version__}\")\n",
    "print(f\"XGBoost: {xgb.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49018,
     "status": "ok",
     "timestamp": 1767082318769,
     "user": {
      "displayName": "Hamdan Syaifuddin",
      "userId": "09275097033143327962"
     },
     "user_tz": -420
    },
    "id": "JCwmuKOQUCXM",
    "outputId": "51342bb7-7a11-465f-a75e-9ea4d4ee9ec7"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ðŸ“š BLOK 2: IMPORT LIBRARIES\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "Run time: ~10 seconds\n",
    "Description: Import all required libraries and set configurations\n",
    "\"\"\"\n",
    "\n",
    "# Core libraries\n",
    "import sys  # â† Import sys di sini dulu!\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# ML Libraries\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score,\n",
    "    confusion_matrix, classification_report,\n",
    "    precision_recall_curve, roc_curve, f1_score,\n",
    "    precision_score, recall_score\n",
    ")\n",
    "\n",
    "# Imbalance Handling\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "# ML Models\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Seeds for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"âœ… ALL LIBRARIES LOADED SUCCESSFULLY!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Python version: {sys.version.split()[0]}\")  # â† Fixed!\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"Polars: {pl.__version__}\")\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"LightGBM: {lgb.__version__}\")\n",
    "print(f\"XGBoost: {xgb.__version__}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "executionInfo": {
     "elapsed": 49927,
     "status": "ok",
     "timestamp": 1767082368695,
     "user": {
      "displayName": "Hamdan Syaifuddin",
      "userId": "09275097033143327962"
     },
     "user_tz": -420
    },
    "id": "WmoVcKpCUFxb",
    "outputId": "fb60ebb1-6a7f-4c26-9307-b687cc26b2b8"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ðŸ“‚ BLOK 3: MOUNT DRIVE & LOAD DATA\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "Run time: ~1-2 minutes\n",
    "Description: Mount Google Drive and load datasets with Polars\n",
    "\"\"\"\n",
    "\n",
    "# Mount Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# âœ… Your path (typo in folder name \"Tansaction\" is preserved as-is)\n",
    "BASE_PATH = r\"C:\\Users\\hamda\\OneDrive\\Documents\\SEMESTER 7\\MACHINE LEARNING\\fraud\\transaction\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“‚ LOADING DATASETS...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load with Polars (memory efficient)\n",
    "print(\"\\n1. Loading train_transaction.csv...\")\n",
    "train_transaction = pl.read_csv(f\"{BASE_PATH}/train_transaction.csv\")\n",
    "print(f\"   âœ“ Train shape: {train_transaction.shape}\")\n",
    "print(f\"   âœ“ Memory: {train_transaction.estimated_size('mb'):.2f} MB\")\n",
    "\n",
    "print(\"\\n2. Loading test_transaction.csv...\")\n",
    "test_transaction = pl.read_csv(f\"{BASE_PATH}/test_transaction.csv\")\n",
    "print(f\"   âœ“ Test shape: {test_transaction.shape}\")\n",
    "print(f\"   âœ“ Memory: {test_transaction.estimated_size('mb'):.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… DATA LOADED SUCCESSFULLY!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Preview\n",
    "print(\"\\nðŸ“Š TRAIN DATA PREVIEW:\")\n",
    "display(train_transaction.head().to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "26af0ee13170473aaba9a99d8083a27c",
      "f047d2f8c3ed4b1c9170df572eda1ebb",
      "8165f1a82512434cb85b09722ec4ddc5",
      "39f8ed3c63414b939eb3a6026f3327ab",
      "c63f03e0da0842898f1f94255aac6926",
      "743413be089042e0b2aaa970c49d3605",
      "523522b086794074bc771e2c380762db",
      "464caa6d472c443284a5955f282e2e6f",
      "bfaa220863474ce496dce68a1a4b4b09",
      "7d1e978533b744df9b8350a6bb585e79",
      "f17e03338f70405da83bf482a6cefef2"
     ]
    },
    "executionInfo": {
     "elapsed": 1700,
     "status": "ok",
     "timestamp": 1767082370402,
     "user": {
      "displayName": "Hamdan Syaifuddin",
      "userId": "09275097033143327962"
     },
     "user_tz": -420
    },
    "id": "2csN7mdgWAnL",
    "outputId": "ce79cbf4-15a7-4c63-aa60-20d9a8457250"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ðŸ” BLOK 5: MISSING VALUES ANALYSIS\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "Run time: ~20 seconds\n",
    "Description: Analyze missing value patterns\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ” MISSING VALUES ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate missing percentages\n",
    "missing_stats = []\n",
    "for col in tqdm(train_transaction.columns, desc=\"Analyzing columns\"):\n",
    "    null_count = train_transaction[col].null_count()\n",
    "    null_pct = (null_count / len(train_transaction)) * 100\n",
    "    if null_pct > 0:\n",
    "        missing_stats.append({\n",
    "            'column': col,\n",
    "            'missing_count': null_count,\n",
    "            'missing_pct': null_pct,\n",
    "            'dtype': str(train_transaction[col].dtype)\n",
    "        })\n",
    "\n",
    "missing_df = pd.DataFrame(missing_stats).sort_values('missing_pct', ascending=False)\n",
    "\n",
    "print(f\"\\nðŸ“Š SUMMARY:\")\n",
    "print(f\"   Total columns: {len(train_transaction.columns)}\")\n",
    "print(f\"   Columns with missing: {len(missing_df)}\")\n",
    "print(f\"   Complete columns: {len(train_transaction.columns) - len(missing_df)}\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ TOP 20 COLUMNS WITH MOST MISSING VALUES:\")\n",
    "display(missing_df.head(20))\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Top 30 missing\n",
    "top_missing = missing_df.head(30)\n",
    "axes[0].barh(range(len(top_missing)), top_missing['missing_pct'], color='coral')\n",
    "axes[0].set_yticks(range(len(top_missing)))\n",
    "axes[0].set_yticklabels(top_missing['column'], fontsize=9)\n",
    "axes[0].set_xlabel('Missing %', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Top 30 Columns by Missing Values', fontsize=14, fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "axes[0].axvline(x=70, color='red', linestyle='--', linewidth=2, label='70% threshold')\n",
    "axes[0].legend()\n",
    "\n",
    "# Distribution of missing percentages\n",
    "axes[1].hist(missing_df['missing_pct'], bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(x=70, color='red', linestyle='--', linewidth=2, label='70% threshold')\n",
    "axes[1].set_xlabel('Missing %', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Number of Columns', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Distribution of Missing Percentages', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Decision strategy\n",
    "drop_threshold = 70\n",
    "cols_to_drop = missing_df[missing_df['missing_pct'] > drop_threshold]['column'].tolist()\n",
    "\n",
    "print(f\"\\nðŸ“Œ CLEANING STRATEGY:\")\n",
    "print(f\"   â†’ Drop columns with >{drop_threshold}% missing: {len(cols_to_drop)} columns\")\n",
    "print(f\"   â†’ Impute remaining columns: {len(missing_df) - len(cols_to_drop)} columns\")\n",
    "print(f\"   â†’ Keep complete columns: {len(train_transaction.columns) - len(missing_df)} columns\")\n",
    "\n",
    "# Save for later use\n",
    "with open('cols_to_drop.pkl', 'wb') as f:\n",
    "    pickle.dump(cols_to_drop, f)\n",
    "\n",
    "print(\"\\nâœ… Analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1767082370413,
     "user": {
      "displayName": "Hamdan Syaifuddin",
      "userId": "09275097033143327962"
     },
     "user_tz": -420
    },
    "id": "2lv9NW3fWCQW",
    "outputId": "024dae3f-f6c4-4c63-f008-f444a1ac6363"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ðŸ“Š BLOK 6: FEATURE TYPES ANALYSIS\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "Run time: ~10 seconds\n",
    "Description: Understand data types and feature categories\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ“Š FEATURE TYPES ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Categorize features\n",
    "categorical_features = []\n",
    "numerical_features = []\n",
    "id_features = ['TransactionID']\n",
    "target = 'isFraud'\n",
    "\n",
    "for col in train_transaction.columns:\n",
    "    if col in id_features or col == target:\n",
    "        continue\n",
    "\n",
    "    dtype = train_transaction[col].dtype\n",
    "\n",
    "    if dtype == pl.Utf8:\n",
    "        categorical_features.append(col)\n",
    "    else:\n",
    "        numerical_features.append(col)\n",
    "\n",
    "print(f\"\\nðŸ“‹ FEATURE BREAKDOWN:\")\n",
    "print(f\"   Total features: {len(train_transaction.columns)}\")\n",
    "print(f\"   - ID columns: {len(id_features)}\")\n",
    "print(f\"   - Target: 1\")\n",
    "print(f\"   - Numerical: {len(numerical_features)}\")\n",
    "print(f\"   - Categorical: {len(categorical_features)}\")\n",
    "\n",
    "print(f\"\\nðŸ”¢ NUMERICAL FEATURES ({len(numerical_features)}):\")\n",
    "print(f\"   {numerical_features[:10]} ... (showing first 10)\")\n",
    "\n",
    "print(f\"\\nðŸ“ CATEGORICAL FEATURES ({len(categorical_features)}):\")\n",
    "print(f\"   {categorical_features}\")\n",
    "\n",
    "# Save for later\n",
    "feature_info = {\n",
    "    'categorical': categorical_features,\n",
    "    'numerical': numerical_features,\n",
    "    'id': id_features,\n",
    "    'target': target\n",
    "}\n",
    "\n",
    "with open('feature_info.pkl', 'wb') as f:\n",
    "    pickle.dump(feature_info, f)\n",
    "\n",
    "print(\"\\nâœ… Feature categorization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480,
     "referenced_widgets": [
      "70c5ad46337d4257b66ad07fbf40e4b5",
      "eed4dc107203457fa535c1ebbf43aff0",
      "f0acd25cb104435e98aab7dc30c985bc",
      "516c797614184a0db515f097422fa5fc",
      "113f80122d2d4c509acdf9246203b039",
      "62d2a383b63f438fb2c181f1ad2b4b57",
      "b159f0a2438641f8836d15ffd525c85b",
      "fea47fa41d2b4125a4f8c5ce305ffa89",
      "9d2641c37a4741aeb1fa2aae75fcc973",
      "6c87bd46536245e0a064194979549420",
      "f9fc816eb6174928b2575ef29ba64204",
      "58de9d02158543ecbe18799af0779962",
      "44e88c4cea2a495aa7e232f96a2b03fb",
      "bfc2346b58b94c2bb98590b22cc351ee",
      "3840dbbd50d740a99bfd1fc9fc1e90da",
      "03ef952f696942a3b4628f96bd75c40d",
      "4d98545d51104d3a9929ac9d2fb22e44",
      "5f85634ce40942d296034b06dd48988d",
      "1598b99f92254ceca4c6f12a01c94f75",
      "97410c9a5a854ba48f3af119019df402",
      "72f76f13ff864b0693387fb096d27819",
      "709cf7560cb74ec6af04463357eb6ad4"
     ]
    },
    "executionInfo": {
     "elapsed": 7461,
     "status": "ok",
     "timestamp": 1767082377875,
     "user": {
      "displayName": "Hamdan Syaifuddin",
      "userId": "09275097033143327962"
     },
     "user_tz": -420
    },
    "id": "Z3jF97LKWEaB",
    "outputId": "1a57d56f-ee35-4b9d-ceb7-48bf3feb436e"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ðŸ§¹ BLOK 7: DATA CLEANING\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "Run time: ~1-2 minutes\n",
    "Description: Remove high-missing columns and impute remaining\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ§¹ DATA CLEANING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load saved info\n",
    "with open('cols_to_drop.pkl', 'rb') as f:\n",
    "    cols_to_drop = pickle.load(f)\n",
    "\n",
    "with open('feature_info.pkl', 'rb') as f:\n",
    "    feature_info = pickle.load(f)\n",
    "\n",
    "print(f\"\\n1ï¸âƒ£ DROPPING HIGH-MISSING COLUMNS...\")\n",
    "print(f\"   Columns to drop: {len(cols_to_drop)}\")\n",
    "\n",
    "# Drop from both train and test\n",
    "train_clean = train_transaction.drop(cols_to_drop)\n",
    "test_clean = test_transaction.drop([c for c in cols_to_drop if c in test_transaction.columns])\n",
    "\n",
    "print(f\"   âœ“ Train shape after drop: {train_clean.shape}\")\n",
    "print(f\"   âœ“ Test shape after drop: {test_clean.shape}\")\n",
    "\n",
    "print(f\"\\n2ï¸âƒ£ IMPUTING MISSING VALUES...\")\n",
    "\n",
    "# Imputation strategy\n",
    "impute_stats = {'categorical': {}, 'numerical': {}}\n",
    "\n",
    "# Get updated categorical/numerical lists (after dropping columns)\n",
    "categorical_features = [c for c in feature_info['categorical'] if c in train_clean.columns]\n",
    "numerical_features = [c for c in feature_info['numerical'] if c in train_clean.columns]\n",
    "\n",
    "# Impute categorical (mode)\n",
    "print(f\"   Processing {len(categorical_features)} categorical features...\")\n",
    "for col in tqdm(categorical_features, desc=\"Categorical\"):\n",
    "    mode_val = train_clean[col].mode().to_list()\n",
    "\n",
    "    # âœ… FIX: Check if mode exists and is valid\n",
    "    if mode_val and len(mode_val) > 0 and mode_val[0] is not None:\n",
    "        mode_val = mode_val[0]\n",
    "        impute_stats['categorical'][col] = mode_val\n",
    "        train_clean = train_clean.with_columns(pl.col(col).fill_null(mode_val))\n",
    "\n",
    "        # âœ… FIX: Only fill test if column exists\n",
    "        if col in test_clean.columns:\n",
    "            test_clean = test_clean.with_columns(pl.col(col).fill_null(mode_val))\n",
    "    else:\n",
    "        # âœ… FIX: If no mode, fill with \"unknown\"\n",
    "        impute_stats['categorical'][col] = \"unknown\"\n",
    "        train_clean = train_clean.with_columns(pl.col(col).fill_null(\"unknown\"))\n",
    "\n",
    "        if col in test_clean.columns:\n",
    "            test_clean = test_clean.with_columns(pl.col(col).fill_null(\"unknown\"))\n",
    "\n",
    "# Impute numerical (median - robust to outliers)\n",
    "print(f\"   Processing {len(numerical_features)} numerical features...\")\n",
    "for col in tqdm(numerical_features, desc=\"Numerical\"):\n",
    "    if col in ['TransactionID', 'TransactionDT']:\n",
    "        continue\n",
    "\n",
    "    median_val = train_clean[col].median()\n",
    "\n",
    "    # âœ… FIX: Check if median is valid\n",
    "    if median_val is not None:\n",
    "        impute_stats['numerical'][col] = float(median_val)\n",
    "        train_clean = train_clean.with_columns(pl.col(col).fill_null(median_val))\n",
    "\n",
    "        # âœ… FIX: Only fill test if column exists\n",
    "        if col in test_clean.columns:\n",
    "            test_clean = test_clean.with_columns(pl.col(col).fill_null(median_val))\n",
    "    else:\n",
    "        # âœ… FIX: If no median, fill with 0\n",
    "        impute_stats['numerical'][col] = 0.0\n",
    "        train_clean = train_clean.with_columns(pl.col(col).fill_null(0.0))\n",
    "\n",
    "        if col in test_clean.columns:\n",
    "            test_clean = test_clean.with_columns(pl.col(col).fill_null(0.0))\n",
    "\n",
    "# Save imputation stats\n",
    "with open('impute_stats.pkl', 'wb') as f:\n",
    "    pickle.dump(impute_stats, f)\n",
    "\n",
    "# Verify\n",
    "remaining_missing_train = sum(train_clean[col].null_count() for col in train_clean.columns)\n",
    "remaining_missing_test = sum(test_clean[col].null_count() for col in test_clean.columns)\n",
    "\n",
    "print(f\"\\n3ï¸âƒ£ VERIFICATION:\")\n",
    "print(f\"   Train remaining missing: {remaining_missing_train}\")\n",
    "print(f\"   Test remaining missing: {remaining_missing_test}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… DATA CLEANING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Final shapes:\")\n",
    "print(f\"   Train: {train_clean.shape}\")\n",
    "print(f\"   Test: {test_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25924,
     "status": "ok",
     "timestamp": 1767082403800,
     "user": {
      "displayName": "Hamdan Syaifuddin",
      "userId": "09275097033143327962"
     },
     "user_tz": -420
    },
    "id": "kH3tAy6DWnFA",
    "outputId": "68be9101-bb19-43d7-9904-5a95bda15ff4"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ðŸ’¾ BLOK 8: SAVE CLEANED DATA\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "Run time: ~30 seconds\n",
    "Description: Save cleaned data for next phase\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ’¾ Saving cleaned data...\")\n",
    "\n",
    "train_clean.write_csv(f\"{BASE_PATH}/train_cleaned.csv\")\n",
    "test_clean.write_csv(f\"{BASE_PATH}/test_cleaned.csv\")\n",
    "\n",
    "print(\"âœ… Cleaned data saved!\")\n",
    "print(f\"   {BASE_PATH}/train_cleaned.csv\")\n",
    "print(f\"   {BASE_PATH}/test_cleaned.csv\")\n",
    "\n",
    "# Memory cleanup\n",
    "del train_transaction, test_transaction\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\nðŸ“Œ CHECKPOINT: Data cleaning complete!\")\n",
    "print(\"   You can now proceed to Feature Engineering (Blok 9)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33528,
     "status": "ok",
     "timestamp": 1767082437321,
     "user": {
      "displayName": "Hamdan Syaifuddin",
      "userId": "09275097033143327962"
     },
     "user_tz": -420
    },
    "id": "NWGdTw3BWsCf",
    "outputId": "0c690c19-ff6e-4cfd-f22a-1ca757a9cf13"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ðŸ”§ BLOK 9: ADVANCED FEATURE ENGINEERING\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "Run time: ~5-10 minutes\n",
    "Description: Create ~60 domain-specific features for fraud detection\n",
    "\n",
    "âš ï¸ THIS IS THE MOST IMPORTANT BLOCK FOR PERFORMANCE!\n",
    "   These features capture fraud patterns:\n",
    "   - Velocity: How fast are transactions happening?\n",
    "   - Deviation: Is this unusual for this user?\n",
    "   - Aggregation: Group-level patterns\n",
    "   - Interaction: Feature combinations\n",
    "\"\"\"\n",
    "\n",
    "def engineer_fraud_features(df, is_train=True):\n",
    "    \"\"\"\n",
    "    Complete feature engineering pipeline.\n",
    "    Creates ~60 new features optimized for fraud detection.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    print(\"ðŸ”§ ADVANCED FEATURE ENGINEERING\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Input shape: {df.shape}\")\n",
    "    print(f\"Mode: {'TRAIN' if is_train else 'TEST'}\")\n",
    "    print()\n",
    "\n",
    "    # ================================================================\n",
    "    # 1. TEMPORAL FEATURES\n",
    "    # ================================================================\n",
    "    print(\"[1/6] Creating temporal features...\")\n",
    "\n",
    "    # Basic time components\n",
    "    df = df.with_columns([\n",
    "        (pl.col('TransactionDT') // 3600 % 24).cast(pl.Int16).alias('hour'),\n",
    "        (pl.col('TransactionDT') // (3600 * 24) % 7).cast(pl.Int16).alias('day_of_week'),\n",
    "        (pl.col('TransactionDT') // (3600 * 24)).cast(pl.Int32).alias('day_of_year'),\n",
    "        (pl.col('TransactionDT') // 3600).cast(pl.Int32).alias('hour_window'),\n",
    "        (pl.col('TransactionDT') // (3600 * 24)).cast(pl.Int32).alias('day_window')\n",
    "    ])\n",
    "\n",
    "    # Cyclical encoding (CRITICAL for neural networks!)\n",
    "    df = df.with_columns([\n",
    "        (pl.col('hour') * 2 * np.pi / 24).sin().cast(pl.Float32).alias('hour_sin'),\n",
    "        (pl.col('hour') * 2 * np.pi / 24).cos().cast(pl.Float32).alias('hour_cos'),\n",
    "        (pl.col('day_of_week') * 2 * np.pi / 7).sin().cast(pl.Float32).alias('dow_sin'),\n",
    "        (pl.col('day_of_week') * 2 * np.pi / 7).cos().cast(pl.Float32).alias('dow_cos')\n",
    "    ])\n",
    "\n",
    "    # High-risk time indicators\n",
    "    df = df.with_columns([\n",
    "        ((pl.col('hour') >= 0) & (pl.col('hour') <= 5)).cast(pl.Int8).alias('is_night'),\n",
    "        ((pl.col('hour') >= 9) & (pl.col('hour') <= 17)).cast(pl.Int8).alias('is_business_hours'),\n",
    "        ((pl.col('day_of_week') >= 5)).cast(pl.Int8).alias('is_weekend')\n",
    "    ])\n",
    "\n",
    "    print(\"   âœ“ Created 14 temporal features\")\n",
    "\n",
    "    # ================================================================\n",
    "    # 2. VELOCITY FEATURES (Most Important!)\n",
    "    # ================================================================\n",
    "    print(\"[2/6] Creating velocity features...\")\n",
    "\n",
    "    card_cols = ['card1', 'card2', 'card3']\n",
    "\n",
    "    for card_col in card_cols:\n",
    "        if card_col in df.columns:\n",
    "            # Transactions per hour\n",
    "            df = df.with_columns([\n",
    "                pl.col('TransactionID').count().over([card_col, 'hour_window']).cast(pl.Int16).alias(f'{card_col}_txn_per_hour'),\n",
    "                pl.col('TransactionAmt').sum().over([card_col, 'hour_window']).cast(pl.Float32).alias(f'{card_col}_amt_per_hour')\n",
    "            ])\n",
    "\n",
    "            # Transactions per day\n",
    "            df = df.with_columns([\n",
    "                pl.col('TransactionID').count().over([card_col, 'day_window']).cast(pl.Int16).alias(f'{card_col}_txn_per_day'),\n",
    "                pl.col('TransactionAmt').sum().over([card_col, 'day_window']).cast(pl.Float32).alias(f'{card_col}_amt_per_day')\n",
    "            ])\n",
    "\n",
    "            # Average per transaction in window\n",
    "            df = df.with_columns([\n",
    "                (pl.col(f'{card_col}_amt_per_hour') / (pl.col(f'{card_col}_txn_per_hour') + 1)).cast(pl.Float32).alias(f'{card_col}_avg_amt_per_txn')\n",
    "            ])\n",
    "\n",
    "    print(\"   âœ“ Created ~15 velocity features\")\n",
    "\n",
    "    # ================================================================\n",
    "    # 3. DEVIATION FEATURES\n",
    "    # ================================================================\n",
    "    print(\"[3/6] Creating deviation features...\")\n",
    "\n",
    "    for card_col in ['card1', 'card2']:\n",
    "        if card_col in df.columns:\n",
    "            # User statistics\n",
    "            df = df.with_columns([\n",
    "                pl.col('TransactionAmt').mean().over(card_col).cast(pl.Float32).alias(f'{card_col}_amt_mean'),\n",
    "                pl.col('TransactionAmt').std().over(card_col).cast(pl.Float32).alias(f'{card_col}_amt_std'),\n",
    "                pl.col('TransactionAmt').min().over(card_col).cast(pl.Float32).alias(f'{card_col}_amt_min'),\n",
    "                pl.col('TransactionAmt').max().over(card_col).cast(pl.Float32).alias(f'{card_col}_amt_max'),\n",
    "                pl.col('TransactionAmt').median().over(card_col).cast(pl.Float32).alias(f'{card_col}_amt_median')\n",
    "            ])\n",
    "\n",
    "            # Deviation metrics\n",
    "            df = df.with_columns([\n",
    "                # Z-score\n",
    "                ((pl.col('TransactionAmt') - pl.col(f'{card_col}_amt_mean')) /\n",
    "                 (pl.col(f'{card_col}_amt_std') + 0.001)).cast(pl.Float32).alias(f'{card_col}_amt_zscore'),\n",
    "\n",
    "                # Ratio to mean\n",
    "                (pl.col('TransactionAmt') / (pl.col(f'{card_col}_amt_mean') + 1)).cast(pl.Float32).alias(f'{card_col}_amt_ratio'),\n",
    "\n",
    "                # Is max transaction\n",
    "                (pl.col('TransactionAmt') == pl.col(f'{card_col}_amt_max')).cast(pl.Int8).alias(f'{card_col}_is_max_amt')\n",
    "            ])\n",
    "\n",
    "    print(\"   âœ“ Created ~16 deviation features\")\n",
    "\n",
    "    # ================================================================\n",
    "    # 4. AGGREGATION FEATURES\n",
    "    # ================================================================\n",
    "    print(\"[4/6] Creating aggregation features...\")\n",
    "\n",
    "    # Product stats\n",
    "    if 'ProductCD' in df.columns:\n",
    "        df = df.with_columns([\n",
    "            pl.col('TransactionAmt').mean().over('ProductCD').cast(pl.Float32).alias('product_amt_mean'),\n",
    "            pl.col('TransactionAmt').std().over('ProductCD').cast(pl.Float32).alias('product_amt_std'),\n",
    "            pl.col('TransactionID').count().over('ProductCD').cast(pl.Int32).alias('product_txn_count')\n",
    "        ])\n",
    "\n",
    "    # Address stats\n",
    "    for addr_col in ['addr1', 'addr2']:\n",
    "        if addr_col in df.columns:\n",
    "            df = df.with_columns([\n",
    "                pl.col('TransactionID').count().over(addr_col).cast(pl.Int32).alias(f'{addr_col}_txn_count'),\n",
    "                pl.col('TransactionAmt').mean().over(addr_col).cast(pl.Float32).alias(f'{addr_col}_amt_mean')\n",
    "            ])\n",
    "\n",
    "    # Email domain stats\n",
    "    if 'P_emaildomain' in df.columns:\n",
    "        df = df.with_columns([\n",
    "            pl.col('TransactionID').count().over('P_emaildomain').cast(pl.Int32).alias('email_txn_count'),\n",
    "            pl.col('TransactionAmt').mean().over('P_emaildomain').cast(pl.Float32).alias('email_amt_mean')\n",
    "        ])\n",
    "\n",
    "    print(\"   âœ“ Created ~10 aggregation features\")\n",
    "\n",
    "    # ================================================================\n",
    "    # 5. INTERACTION FEATURES\n",
    "    # ================================================================\n",
    "    print(\"[5/6] Creating interaction features...\")\n",
    "\n",
    "    # Amount + Time interactions\n",
    "    df = df.with_columns([\n",
    "        (pl.col('TransactionAmt') * pl.col('is_night')).cast(pl.Float32).alias('amt_night_interaction'),\n",
    "        (pl.col('TransactionAmt') / (pl.col('hour') + 1)).cast(pl.Float32).alias('amt_per_hour_interaction'),\n",
    "        (pl.col('TransactionAmt') * pl.col('is_weekend')).cast(pl.Float32).alias('amt_weekend_interaction')\n",
    "    ])\n",
    "\n",
    "    print(\"   âœ“ Created ~3 interaction features\")\n",
    "\n",
    "    # ================================================================\n",
    "    # 6. MISSING INDICATORS\n",
    "    # ================================================================\n",
    "    print(\"[6/6] Creating missing indicators...\")\n",
    "\n",
    "    # Important field indicators\n",
    "    important_cols = ['card1', 'card2', 'addr1', 'P_emaildomain', 'dist1']\n",
    "    missing_count = 0\n",
    "\n",
    "    for col in important_cols:\n",
    "        if col in df.columns:\n",
    "            df = df.with_columns([\n",
    "                pl.col(col).is_null().cast(pl.Int8).alias(f'{col}_is_missing')\n",
    "            ])\n",
    "            missing_count += 1\n",
    "\n",
    "    print(f\"   âœ“ Created {missing_count} missing indicators\")\n",
    "\n",
    "    # ================================================================\n",
    "    # CLEANUP\n",
    "    # ================================================================\n",
    "    print(\"\\nðŸ§¹ Cleaning up temporary columns...\")\n",
    "\n",
    "    temp_cols = ['hour_window', 'day_window']\n",
    "    df = df.drop([c for c in temp_cols if c in df.columns])\n",
    "\n",
    "    # Fill any remaining nulls\n",
    "    for col in df.columns:\n",
    "        if col not in ['TransactionID', 'isFraud', 'TransactionDT']:\n",
    "            if df[col].dtype in [pl.Float32, pl.Float64, pl.Int8, pl.Int16, pl.Int32, pl.Int64]:\n",
    "                df = df.with_columns([pl.col(col).fill_null(0)])\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    print(\"âœ… FEATURE ENGINEERING COMPLETE!\")\n",
    "    print(f\"   Input features: ~{len(train_clean.columns)}\")\n",
    "    print(f\"   New features: ~{len(df.columns) - len(train_clean.columns)}\")\n",
    "    print(f\"   Total features: {len(df.columns)}\")\n",
    "    print(f\"   Final shape: {df.shape}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    return df\n",
    "\n",
    "# ================================================================\n",
    "# APPLY FEATURE ENGINEERING\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\nðŸš€ APPLYING FEATURE ENGINEERING TO TRAIN DATA...\")\n",
    "train_engineered = engineer_fraud_features(train_clean, is_train=True)\n",
    "\n",
    "print(\"\\nðŸš€ APPLYING FEATURE ENGINEERING TO TEST DATA...\")\n",
    "test_engineered = engineer_fraud_features(test_clean, is_train=False)\n",
    "\n",
    "# Save engineered data\n",
    "print(\"\\nðŸ’¾ Saving engineered data...\")\n",
    "train_engineered.write_csv(f\"{BASE_PATH}/train_engineered.csv\")\n",
    "test_engineered.write_csv(f\"{BASE_PATH}/test_engineered.csv\")\n",
    "\n",
    "print(\"âœ… Engineered data saved!\")\n",
    "\n",
    "# Memory cleanup\n",
    "del train_clean, test_clean\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\nðŸ“Œ CHECKPOINT: Feature engineering complete!\")\n",
    "print(\"   Total new features created: ~60\")\n",
    "print(\"   Ready for preprocessing (Blok 10)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6069,
     "status": "ok",
     "timestamp": 1767082443390,
     "user": {
      "displayName": "Hamdan Syaifuddin",
      "userId": "09275097033143327962"
     },
     "user_tz": -420
    },
    "id": "KXXChHvUWtrG",
    "outputId": "418fb0ce-a9ee-4cab-d567-eb125943a565"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "âœ‚ï¸ BLOK 10: TRAIN/VALIDATION SPLIT\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "Run time: ~1 minute\n",
    "Description: Split data with STRATIFIED sampling (CRITICAL!)\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"âœ‚ï¸ TRAIN/VALIDATION SPLIT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Convert to pandas for sklearn compatibility\n",
    "print(\"\\n1ï¸âƒ£ Converting to Pandas...\")\n",
    "train_df = train_engineered.to_pandas()\n",
    "\n",
    "print(f\"   âœ“ Shape: {train_df.shape}\")\n",
    "print(f\"   âœ“ Memory: {train_df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Separate features and target\n",
    "print(\"\\n2ï¸âƒ£ Separating features and target...\")\n",
    "\n",
    "# Columns to drop\n",
    "drop_cols = ['TransactionID', 'TransactionDT', 'isFraud']\n",
    "\n",
    "X = train_df.drop(columns=drop_cols)\n",
    "y = train_df['isFraud']\n",
    "\n",
    "print(f\"   âœ“ Features shape: {X.shape}\")\n",
    "print(f\"   âœ“ Target shape: {y.shape}\")\n",
    "print(f\"   âœ“ Target distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "# Identify categorical columns for later encoding\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numerical_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"\\n   Feature types:\")\n",
    "print(f\"   - Numerical: {len(numerical_cols)}\")\n",
    "print(f\"   - Categorical: {len(categorical_cols)}\")\n",
    "print(f\"   - Categorical columns: {categorical_cols}\")\n",
    "\n",
    "# CRITICAL: Stratified split (ensures fraud distribution in both train/val)\n",
    "print(\"\\n3ï¸âƒ£ Performing STRATIFIED split...\")\n",
    "print(\"   âš ï¸ Using stratified split to maintain fraud distribution!\")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,  # CRITICAL!\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"\\n   âœ“ Train set: {X_train.shape}\")\n",
    "print(f\"     - Not Fraud: {(y_train==0).sum():,} ({(y_train==0).sum()/len(y_train)*100:.2f}%)\")\n",
    "print(f\"     - Fraud: {(y_train==1).sum():,} ({(y_train==1).sum()/len(y_train)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n   âœ“ Validation set: {X_val.shape}\")\n",
    "print(f\"     - Not Fraud: {(y_val==0).sum():,} ({(y_val==0).sum()/len(y_val)*100:.2f}%)\")\n",
    "print(f\"     - Fraud: {(y_val==1).sum():,} ({(y_val==1).sum()/len(y_val)*100:.2f}%)\")\n",
    "\n",
    "# Save split info\n",
    "split_info = {\n",
    "    'categorical_cols': categorical_cols,\n",
    "    'numerical_cols': numerical_cols,\n",
    "    'train_shape': X_train.shape,\n",
    "    'val_shape': X_val.shape\n",
    "}\n",
    "\n",
    "with open('split_info.pkl', 'wb') as f:\n",
    "    pickle.dump(split_info, f)\n",
    "\n",
    "print(\"\\nâœ… Split complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 985
    },
    "executionInfo": {
     "elapsed": 88514,
     "status": "ok",
     "timestamp": 1767082531917,
     "user": {
      "displayName": "Hamdan Syaifuddin",
      "userId": "09275097033143327962"
     },
     "user_tz": -420
    },
    "id": "UvF9l997W108",
    "outputId": "0ffc623b-af22-46f9-ace6-dc170d959b0c"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "âš–ï¸ BLOK 11: SMOTE + PREPROCESSING\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "Run time: ~2-3 minutes\n",
    "Description: Handle class imbalance with SMOTE and scale features\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"âš–ï¸ SMOTE + FEATURE PREPROCESSING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load split info\n",
    "with open('split_info.pkl', 'rb') as f:\n",
    "    split_info = pickle.load(f)\n",
    "\n",
    "categorical_cols = split_info['categorical_cols']\n",
    "numerical_cols = split_info['numerical_cols']\n",
    "\n",
    "# ================================================================\n",
    "# 1. CATEGORICAL ENCODING\n",
    "# ================================================================\n",
    "print(\"\\n1ï¸âƒ£ ENCODING CATEGORICAL FEATURES...\")\n",
    "\n",
    "if len(categorical_cols) > 0:\n",
    "    print(f\"   Processing {len(categorical_cols)} categorical columns...\")\n",
    "\n",
    "    label_encoders = {}\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "\n",
    "        # Fit on train\n",
    "        X_train[col] = le.fit_transform(X_train[col].astype(str))\n",
    "        X_val[col] = le.transform(X_val[col].astype(str))\n",
    "\n",
    "        label_encoders[col] = le\n",
    "\n",
    "    # Save encoders\n",
    "    with open('label_encoders.pkl', 'wb') as f:\n",
    "        pickle.dump(label_encoders, f)\n",
    "\n",
    "    print(f\"   âœ“ Encoded {len(categorical_cols)} categorical features\")\n",
    "else:\n",
    "    print(\"   âœ“ No categorical features to encode\")\n",
    "\n",
    "# ================================================================\n",
    "# 2. FEATURE SCALING\n",
    "# ================================================================\n",
    "print(\"\\n2ï¸âƒ£ SCALING NUMERICAL FEATURES...\")\n",
    "print(\"   Using RobustScaler (robust to outliers)\")\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Save scaler\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(f\"   âœ“ Scaled {X_train.shape[1]} features\")\n",
    "\n",
    "# ================================================================\n",
    "# 3. SMOTE (CRITICAL for imbalanced data!)\n",
    "# ================================================================\n",
    "print(\"\\n3ï¸âƒ£ APPLYING SMOTE...\")\n",
    "print(\"   âš ï¸ This will take 1-2 minutes...\")\n",
    "\n",
    "# Check current distribution\n",
    "print(f\"\\n   Before SMOTE:\")\n",
    "print(f\"   - Not Fraud: {(y_train==0).sum():,}\")\n",
    "print(f\"   - Fraud: {(y_train==1).sum():,}\")\n",
    "print(f\"   - Ratio: {(y_train==0).sum()/(y_train==1).sum():.1f}:1\")\n",
    "\n",
    "# âœ… FIX: Remove n_jobs parameter (not supported in newer versions)\n",
    "smote = SMOTE(\n",
    "    sampling_strategy=0.3,  # Increase fraud to 30% of majority\n",
    "    random_state=RANDOM_STATE\n",
    "    # n_jobs removed - not supported\n",
    ")\n",
    "\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\n   After SMOTE:\")\n",
    "print(f\"   - Not Fraud: {(y_train_smote==0).sum():,}\")\n",
    "print(f\"   - Fraud: {(y_train_smote==1).sum():,}\")\n",
    "print(f\"   - Ratio: {(y_train_smote==0).sum()/(y_train_smote==1).sum():.1f}:1\")\n",
    "print(f\"   - Total samples: {len(X_train_smote):,} (increased from {len(X_train):,})\")\n",
    "\n",
    "# Visualize SMOTE effect\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Before SMOTE\n",
    "y_train_counts = pd.Series(y_train).value_counts()\n",
    "axes[0].bar(['Not Fraud', 'Fraud'], [y_train_counts[0], y_train_counts[1]],\n",
    "            color=['#2ecc71', '#e74c3c'], alpha=0.8, edgecolor='black', linewidth=2)\n",
    "axes[0].set_title('Before SMOTE', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate([y_train_counts[0], y_train_counts[1]]):\n",
    "    axes[0].text(i, v, f'{v:,}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# After SMOTE\n",
    "y_smote_counts = pd.Series(y_train_smote).value_counts()\n",
    "axes[1].bar(['Not Fraud', 'Fraud'], [y_smote_counts[0], y_smote_counts[1]],\n",
    "            color=['#2ecc71', '#e74c3c'], alpha=0.8, edgecolor='black', linewidth=2)\n",
    "axes[1].set_title('After SMOTE', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate([y_smote_counts[0], y_smote_counts[1]]):\n",
    "    axes[1].text(i, v, f'{v:,}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# âœ… BONUS: Save SMOTE data for later blocks (Blok 13, 14, etc.)\n",
    "print(\"\\nðŸ’¾ Saving preprocessed data for later use...\")\n",
    "with open('X_train_smote.pkl', 'wb') as f:\n",
    "    pickle.dump(X_train_smote, f)\n",
    "with open('y_train_smote.pkl', 'wb') as f:\n",
    "    pickle.dump(y_train_smote, f)\n",
    "with open('X_val_scaled.pkl', 'wb') as f:\n",
    "    pickle.dump(X_val_scaled, f)\n",
    "with open('y_val.pkl', 'wb') as f:\n",
    "    pickle.dump(y_val, f)\n",
    "\n",
    "print(\"   âœ“ Saved: X_train_smote, y_train_smote, X_val_scaled, y_val\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… PREPROCESSING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"Ready for model training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 328606,
     "status": "ok",
     "timestamp": 1767082860520,
     "user": {
      "displayName": "Hamdan Syaifuddin",
      "userId": "09275097033143327962"
     },
     "user_tz": -420
    },
    "id": "1LRvCQoEW7DU",
    "outputId": "7dcc8cd0-3add-4f25-b24f-8e0feffee471"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ðŸš€ BLOK 12: LIGHTGBM BASELINE\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "Run time: ~3-5 minutes\n",
    "Description: Train LightGBM with default good parameters\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸš€ MODEL 1: LIGHTGBM BASELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Best parameters from experience (no tuning yet)\n",
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 31,\n",
    "    'max_depth': 7,\n",
    "    'min_child_samples': 20,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'n_jobs': -1,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "print(\"\\nðŸ“‹ Parameters:\")\n",
    "for k, v in lgb_params.items():\n",
    "    print(f\"   {k}: {v}\")\n",
    "\n",
    "# Create datasets\n",
    "print(\"\\n1ï¸âƒ£ Creating LightGBM datasets...\")\n",
    "dtrain = lgb.Dataset(X_train_smote, label=y_train_smote)\n",
    "dval = lgb.Dataset(X_val_scaled, label=y_val, reference=dtrain)\n",
    "\n",
    "# Train with early stopping\n",
    "print(\"\\n2ï¸âƒ£ Training model...\")\n",
    "print(\"   â³ This will take 2-3 minutes...\")\n",
    "\n",
    "# âœ… FIX: Use record_evaluation callback instead of evals_result parameter\n",
    "evals_result = {}\n",
    "callbacks = [\n",
    "    lgb.early_stopping(stopping_rounds=50),\n",
    "    lgb.log_evaluation(period=50),\n",
    "    lgb.record_evaluation(evals_result)  # â† Records training history\n",
    "]\n",
    "\n",
    "lgb_baseline = lgb.train(\n",
    "    lgb_params,\n",
    "    dtrain,\n",
    "    num_boost_round=1000,\n",
    "    valid_sets=[dtrain, dval],\n",
    "    valid_names=['train', 'valid'],\n",
    "    callbacks=callbacks  # â† Callbacks now include record_evaluation\n",
    ")\n",
    "\n",
    "print(f\"\\n   âœ“ Best iteration: {lgb_baseline.best_iteration}\")\n",
    "print(f\"   âœ“ Best score: {lgb_baseline.best_score['valid']['auc']:.4f}\")\n",
    "\n",
    "# ================================================================\n",
    "# EVALUATION\n",
    "# ================================================================\n",
    "print(\"\\n3ï¸âƒ£ EVALUATION:\")\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = lgb_baseline.predict(X_train_smote)\n",
    "y_val_pred = lgb_baseline.predict(X_val_scaled)\n",
    "\n",
    "# Metrics\n",
    "train_auc_roc = roc_auc_score(y_train_smote, y_train_pred)\n",
    "train_auc_pr = average_precision_score(y_train_smote, y_train_pred)\n",
    "\n",
    "val_auc_roc = roc_auc_score(y_val, y_val_pred)\n",
    "val_auc_pr = average_precision_score(y_val, y_val_pred)\n",
    "\n",
    "print(f\"\\n   ðŸ“Š TRAIN METRICS:\")\n",
    "print(f\"      AUC-ROC: {train_auc_roc:.4f}\")\n",
    "print(f\"      AUC-PR:  {train_auc_pr:.4f}\")\n",
    "\n",
    "print(f\"\\n   ðŸ“Š VALIDATION METRICS:\")\n",
    "print(f\"      AUC-ROC: {val_auc_roc:.4f}\")\n",
    "print(f\"      AUC-PR:  {val_auc_pr:.4f}\")\n",
    "\n",
    "# Classification report at threshold 0.5\n",
    "y_val_pred_binary = (y_val_pred > 0.5).astype(int)\n",
    "print(f\"\\n   ðŸ“‹ CLASSIFICATION REPORT (threshold=0.5):\")\n",
    "print(classification_report(y_val, y_val_pred_binary,\n",
    "                           target_names=['Not Fraud', 'Fraud'],\n",
    "                           digits=4))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_val, y_val_pred_binary)\n",
    "print(f\"\\n   ðŸ”¢ CONFUSION MATRIX:\")\n",
    "print(f\"      [[TN={cm[0,0]:,}  FP={cm[0,1]:,}]\")\n",
    "print(f\"       [FN={cm[1,0]:,}  TP={cm[1,1]:,}]]\")\n",
    "\n",
    "# Visualize training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Training curves\n",
    "axes[0].plot(evals_result['train']['auc'], label='Train', linewidth=2)\n",
    "axes[0].plot(evals_result['valid']['auc'], label='Validation', linewidth=2)\n",
    "axes[0].axvline(x=lgb_baseline.best_iteration, color='red', linestyle='--',\n",
    "               label=f'Best Iteration ({lgb_baseline.best_iteration})')\n",
    "axes[0].set_xlabel('Iterations', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('AUC-ROC', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('LightGBM Training Curves', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Confusion matrix heatmap\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1],\n",
    "           xticklabels=['Not Fraud', 'Fraud'],\n",
    "           yticklabels=['Not Fraud', 'Fraud'],\n",
    "           cbar_kws={'label': 'Count'})\n",
    "axes[1].set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save model\n",
    "lgb_baseline.save_model('lgb_baseline.txt')\n",
    "\n",
    "# Save results\n",
    "lgb_results = {\n",
    "    'model_name': 'LightGBM Baseline',\n",
    "    'train_auc_roc': train_auc_roc,\n",
    "    'train_auc_pr': train_auc_pr,\n",
    "    'val_auc_roc': val_auc_roc,\n",
    "    'val_auc_pr': val_auc_pr,\n",
    "    'best_iteration': lgb_baseline.best_iteration,\n",
    "    'params': lgb_params\n",
    "}\n",
    "\n",
    "with open('lgb_baseline_results.pkl', 'wb') as f:\n",
    "    pickle.dump(lgb_results, f)\n",
    "\n",
    "print(\"\\nâœ… LightGBM Baseline complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "20fe9ace079c425b9cc497e1710343f9",
      "140bddafec18486fbf9a422f943fc065",
      "e0c42842a05647519a60bbe1e984f6c8",
      "653fe8de60814d00be4e7654147b9846",
      "f3c3a88ed9ea4759aecf738f795ba520",
      "d6e165a81f0140e0b8fccc17f8ec0503",
      "b9f7ba5bb3f5407c99b52836bd1bdf48",
      "faa66945f3ae480f8e50e23b01bf60a3",
      "1e36edcf2ca548efa9529a10020ff904",
      "4868a81828e04c68ba352fe4bc093774",
      "f3d32f7a10e346609502cdf726f26616"
     ]
    },
    "executionInfo": {
     "elapsed": 5978710,
     "status": "ok",
     "timestamp": 1767080981805,
     "user": {
      "displayName": "Hamdan Syaifuddin",
      "userId": "09275097033143327962"
     },
     "user_tz": -420
    },
    "id": "ZRsi756zW9nj",
    "outputId": "213ddeb5-c51e-4a4b-bc48-bbb8436d908c"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ðŸŽ¯ BLOK 13: LIGHTGBM HYPERPARAMETER TUNING\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "Run time: ~15-30 minutes (depending on n_trials)\n",
    "Description: Optimize LightGBM hyperparameters with Optuna\n",
    "\"\"\"\n",
    "\n",
    "RUN_OPTUNA = True  # Set to False to skip\n",
    "\n",
    "if RUN_OPTUNA:\n",
    "    print(\"=\"*70)\n",
    "    print(\"ðŸŽ¯ LIGHTGBM HYPERPARAMETER TUNING WITH OPTUNA\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Load preprocessed data\n",
    "    print(\"\\n1ï¸âƒ£ Loading preprocessed data...\")\n",
    "\n",
    "    if 'X_train_smote' not in globals():\n",
    "        print(\"   âš ï¸ Loading from saved files...\")\n",
    "        with open('X_train_smote.pkl', 'rb') as f:\n",
    "            X_train_smote = pickle.load(f)\n",
    "        with open('y_train_smote.pkl', 'rb') as f:\n",
    "            y_train_smote = pickle.load(f)\n",
    "        with open('X_val_scaled.pkl', 'rb') as f:\n",
    "            X_val_scaled = pickle.load(f)\n",
    "        with open('y_val.pkl', 'rb') as f:\n",
    "            y_val = pickle.load(f)\n",
    "\n",
    "    print(f\"   âœ“ Using SMOTE data: {len(X_train_smote):,} samples\")\n",
    "\n",
    "    # Create LightGBM datasets\n",
    "    print(\"\\n2ï¸âƒ£ Creating LightGBM datasets for Optuna...\")\n",
    "    dtrain = lgb.Dataset(X_train_smote, label=y_train_smote)\n",
    "    dval = lgb.Dataset(X_val_scaled, label=y_val, reference=dtrain)\n",
    "    print(\"   âœ“ Datasets created\")\n",
    "\n",
    "    def objective_lgb(trial):\n",
    "        \"\"\"\n",
    "        Optuna objective function for LightGBM.\n",
    "        \"\"\"\n",
    "        params = {\n",
    "            'objective': 'binary',\n",
    "            'metric': 'auc',\n",
    "            'verbosity': -1,\n",
    "            'boosting_type': 'gbdt',\n",
    "            'feature_pre_filter': False,  # âœ… FIX: Allow dynamic min_child_samples\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "            'random_state': RANDOM_STATE,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "\n",
    "        # âœ… FIX: Use callbacks properly for new LightGBM API\n",
    "        callbacks = [\n",
    "            lgb.early_stopping(stopping_rounds=50),\n",
    "            lgb.log_evaluation(period=0)  # Silent training\n",
    "        ]\n",
    "\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=1000,\n",
    "            valid_sets=[dval],\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "\n",
    "        # Evaluate\n",
    "        y_pred = model.predict(X_val_scaled)\n",
    "        score = average_precision_score(y_val, y_pred)\n",
    "\n",
    "        return score\n",
    "\n",
    "    # Run optimization\n",
    "    print(\"\\n3ï¸âƒ£ Running Optuna optimization...\")\n",
    "    print(\"   This will take 15-30 minutes for 50 trials...\")\n",
    "    print(\"   Feel free to reduce n_trials if needed!\\n\")\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE)\n",
    "    )\n",
    "\n",
    "    study.optimize(objective_lgb, n_trials=20, show_progress_bar=True)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸ“Š OPTIMIZATION RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Best AUC-PR: {study.best_value:.4f}\")\n",
    "    print(f\"\\nBest Parameters:\")\n",
    "    for k, v in study.best_params.items():\n",
    "        print(f\"   {k}: {v}\")\n",
    "\n",
    "    # Visualize optimization\n",
    "    try:\n",
    "        fig = plot_optimization_history(study)\n",
    "        fig.show()\n",
    "\n",
    "        fig = plot_param_importances(study)\n",
    "        fig.show()\n",
    "    except:\n",
    "        print(\"   âš ï¸ Visualization skipped (plotly might not be available)\")\n",
    "\n",
    "    # Train final model with best params\n",
    "    print(\"\\n4ï¸âƒ£ Training final model with best parameters...\")\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_params.update({\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'feature_pre_filter': False,  # âœ… Keep this for final model too\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'n_jobs': -1,\n",
    "        'verbose': -1\n",
    "    })\n",
    "\n",
    "    # âœ… FIX: Use callbacks for final training\n",
    "    evals_result = {}\n",
    "    callbacks_final = [\n",
    "        lgb.early_stopping(stopping_rounds=50),\n",
    "        lgb.log_evaluation(period=50),\n",
    "        lgb.record_evaluation(evals_result)\n",
    "    ]\n",
    "\n",
    "    lgb_tuned = lgb.train(\n",
    "        best_params,\n",
    "        dtrain,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[dtrain, dval],\n",
    "        valid_names=['train', 'valid'],\n",
    "        callbacks=callbacks_final\n",
    "    )\n",
    "\n",
    "    # Evaluate tuned model\n",
    "    y_val_pred_tuned = lgb_tuned.predict(X_val_scaled)\n",
    "\n",
    "    val_auc_roc_tuned = roc_auc_score(y_val, y_val_pred_tuned)\n",
    "    val_auc_pr_tuned = average_precision_score(y_val, y_val_pred_tuned)\n",
    "\n",
    "    print(f\"\\nðŸ“Š TUNED MODEL VALIDATION METRICS:\")\n",
    "    print(f\"   AUC-ROC: {val_auc_roc_tuned:.4f}\")\n",
    "    print(f\"   AUC-PR:  {val_auc_pr_tuned:.4f}\")\n",
    "\n",
    "    # Compare with baseline\n",
    "    print(f\"\\nðŸ“ˆ IMPROVEMENT:\")\n",
    "    print(f\"   AUC-ROC: {val_auc_roc:.4f} â†’ {val_auc_roc_tuned:.4f} ({val_auc_roc_tuned-val_auc_roc:+.4f})\")\n",
    "    print(f\"   AUC-PR:  {val_auc_pr:.4f} â†’ {val_auc_pr_tuned:.4f} ({val_auc_pr_tuned-val_auc_pr:+.4f})\")\n",
    "\n",
    "    # Save tuned model\n",
    "    lgb_tuned.save_model('lgb_tuned.txt')\n",
    "\n",
    "    lgb_tuned_results = {\n",
    "        'model_name': 'LightGBM Tuned',\n",
    "        'val_auc_roc': val_auc_roc_tuned,\n",
    "        'val_auc_pr': val_auc_pr_tuned,\n",
    "        'best_params': best_params,\n",
    "        'study': study\n",
    "    }\n",
    "\n",
    "    with open('lgb_tuned_results.pkl', 'wb') as f:\n",
    "        pickle.dump(lgb_tuned_results, f)\n",
    "\n",
    "    print(\"\\nâœ… Optuna tuning complete!\")\n",
    "\n",
    "else:\n",
    "    print(\"â­ï¸ Skipping Optuna tuning (RUN_OPTUNA = False)\")\n",
    "    print(\"   Using baseline LightGBM as best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 501385,
     "status": "ok",
     "timestamp": 1767083361906,
     "user": {
      "displayName": "Hamdan Syaifuddin",
      "userId": "09275097033143327962"
     },
     "user_tz": -420
    },
    "id": "DWOtaYML0KpH",
    "outputId": "cb3ae509-89e4-4496-a992-d4971ec0b4e8"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ðŸš€ BLOK 14: XGBOOST MODEL\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "Run time: ~3-5 minutes\n",
    "Description: Train XGBoost for model comparison\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸš€ MODEL 2: XGBOOST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# XGBoost parameters\n",
    "xgb_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 7,\n",
    "    'min_child_weight': 1,\n",
    "    'gamma': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 1.0,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'n_jobs': -1,\n",
    "    'tree_method': 'hist'\n",
    "}\n",
    "\n",
    "print(\"\\nðŸ“‹ Parameters:\")\n",
    "for k, v in xgb_params.items():\n",
    "    print(f\"   {k}: {v}\")\n",
    "\n",
    "# Create DMatrix\n",
    "print(\"\\n1ï¸âƒ£ Creating XGBoost datasets...\")\n",
    "dtrain_xgb = xgb.DMatrix(X_train_smote, label=y_train_smote)\n",
    "dval_xgb = xgb.DMatrix(X_val_scaled, label=y_val)\n",
    "\n",
    "# Train\n",
    "print(\"\\n2ï¸âƒ£ Training model...\")\n",
    "evals_result_xgb = {}\n",
    "xgb_model = xgb.train(\n",
    "    xgb_params,\n",
    "    dtrain_xgb,\n",
    "    num_boost_round=1000,\n",
    "    evals=[(dtrain_xgb, 'train'), (dval_xgb, 'valid')],\n",
    "    evals_result=evals_result_xgb,\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=50\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\n3ï¸âƒ£ EVALUATION:\")\n",
    "\n",
    "y_val_pred_xgb = xgb_model.predict(dval_xgb)\n",
    "\n",
    "val_auc_roc_xgb = roc_auc_score(y_val, y_val_pred_xgb)\n",
    "val_auc_pr_xgb = average_precision_score(y_val, y_val_pred_xgb)\n",
    "\n",
    "print(f\"\\n   ðŸ“Š VALIDATION METRICS:\")\n",
    "print(f\"      AUC-ROC: {val_auc_roc_xgb:.4f}\")\n",
    "print(f\"      AUC-PR:  {val_auc_pr_xgb:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "y_val_pred_xgb_binary = (y_val_pred_xgb > 0.5).astype(int)\n",
    "cm_xgb = confusion_matrix(y_val, y_val_pred_xgb_binary)\n",
    "\n",
    "print(f\"\\n   ðŸ”¢ CONFUSION MATRIX:\")\n",
    "print(f\"      [[TN={cm_xgb[0,0]:,}  FP={cm_xgb[0,1]:,}]\")\n",
    "print(f\"       [FN={cm_xgb[1,0]:,}  TP={cm_xgb[1,1]:,}]]\")\n",
    "\n",
    "# Save model\n",
    "xgb_model.save_model('xgb_model.json')\n",
    "\n",
    "xgb_results = {\n",
    "    'model_name': 'XGBoost',\n",
    "    'val_auc_roc': val_auc_roc_xgb,\n",
    "    'val_auc_pr': val_auc_pr_xgb,\n",
    "    'params': xgb_params\n",
    "}\n",
    "\n",
    "with open('xgb_results.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb_results, f)\n",
    "\n",
    "print(\"\\nâœ… XGBoost training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Al6_dQzc0UDJ",
    "outputId": "77c28a54-ff36-4901-f219-b292ec0910e1"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ðŸš€ BLOK 15: CATBOOST MODEL (MEMORY OPTIMIZED)\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "Run time: ~3-5 minutes\n",
    "Description: Train CatBoost with memory-efficient approach\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸš€ MODEL 3: CATBOOST (MEMORY OPTIMIZED)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load data if not in memory\n",
    "print(\"\\n1ï¸âƒ£ Loading preprocessed data...\")\n",
    "\n",
    "if 'X_train_smote' not in globals():\n",
    "    print(\"   Loading from saved files...\")\n",
    "    with open('X_train_smote.pkl', 'rb') as f:\n",
    "        X_train_smote = pickle.load(f)\n",
    "    with open('y_train_smote.pkl', 'rb') as f:\n",
    "        y_train_smote = pickle.load(f)\n",
    "    with open('X_val_scaled.pkl', 'rb') as f:\n",
    "        X_val_scaled = pickle.load(f)\n",
    "    with open('y_val.pkl', 'rb') as f:\n",
    "        y_val = pickle.load(f)\n",
    "\n",
    "print(f\"   Original train: {X_train_smote.shape}\")\n",
    "print(f\"   Original val: {X_val_scaled.shape}\")\n",
    "\n",
    "# âœ… FIX: Reduce training data to prevent OOM\n",
    "print(\"\\n2ï¸âƒ£ Sampling training data to prevent memory issues...\")\n",
    "\n",
    "# Use 50% of SMOTE data (still 296K samples - plenty!)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_cat, _, y_train_cat, _ = train_test_split(\n",
    "    X_train_smote, y_train_smote,\n",
    "    train_size=0.5,\n",
    "    stratify=y_train_smote,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"   Sampled train: {X_train_cat.shape} (50% of SMOTE)\")\n",
    "print(f\"   This prevents OOM while maintaining quality\")\n",
    "\n",
    "# Free memory\n",
    "del X_train_smote, y_train_smote\n",
    "gc.collect()\n",
    "\n",
    "# âœ… NO POOL - Direct training (uses less memory)\n",
    "print(\"\\n3ï¸âƒ£ Initializing CatBoost model...\")\n",
    "cat_model = CatBoostClassifier(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.05,\n",
    "    depth=6,  # Reduced from 7 to save memory\n",
    "    l2_leaf_reg=3,\n",
    "    random_seed=RANDOM_STATE,\n",
    "    verbose=100,  # Less frequent logging\n",
    "    early_stopping_rounds=50,\n",
    "    eval_metric='AUC',\n",
    "    task_type='CPU',\n",
    "    thread_count=-1,\n",
    "    used_ram_limit='3GB'  # âœ… Critical: Limit RAM usage\n",
    ")\n",
    "\n",
    "print(\"\\n4ï¸âƒ£ Training CatBoost...\")\n",
    "print(\"   â³ This will take 3-5 minutes...\")\n",
    "print()\n",
    "\n",
    "# âœ… FIX: Train without Pool objects (saves memory)\n",
    "cat_model.fit(\n",
    "    X_train_cat, y_train_cat,\n",
    "    eval_set=(X_val_scaled, y_val),\n",
    "    verbose=100,\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "# Free memory\n",
    "del X_train_cat, y_train_cat\n",
    "gc.collect()\n",
    "\n",
    "# ================================================================\n",
    "# EVALUATION\n",
    "# ================================================================\n",
    "print(\"\\n5ï¸âƒ£ EVALUATION:\")\n",
    "\n",
    "y_val_pred_cat = cat_model.predict_proba(X_val_scaled)[:, 1]\n",
    "\n",
    "val_auc_roc_cat = roc_auc_score(y_val, y_val_pred_cat)\n",
    "val_auc_pr_cat = average_precision_score(y_val, y_val_pred_cat)\n",
    "\n",
    "print(f\"\\n   ðŸ“Š VALIDATION METRICS:\")\n",
    "print(f\"      AUC-ROC: {val_auc_roc_cat:.4f}\")\n",
    "print(f\"      AUC-PR:  {val_auc_pr_cat:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "y_val_pred_cat_binary = (y_val_pred_cat > 0.5).astype(int)\n",
    "print(f\"\\n   ðŸ“‹ CLASSIFICATION REPORT (threshold=0.5):\")\n",
    "print(classification_report(y_val, y_val_pred_cat_binary,\n",
    "                           target_names=['Not Fraud', 'Fraud'],\n",
    "                           digits=4))\n",
    "\n",
    "# Confusion matrix\n",
    "cm_cat = confusion_matrix(y_val, y_val_pred_cat_binary)\n",
    "print(f\"\\n   ðŸ”¢ CONFUSION MATRIX:\")\n",
    "print(f\"      [[TN={cm_cat[0,0]:,}  FP={cm_cat[0,1]:,}]\")\n",
    "print(f\"       [FN={cm_cat[1,0]:,}  TP={cm_cat[1,1]:,}]]\")\n",
    "\n",
    "# ================================================================\n",
    "# VISUALIZATIONS\n",
    "# ================================================================\n",
    "print(\"\\n6ï¸âƒ£ Creating visualizations...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Feature importance (top 20)\n",
    "feature_importance = cat_model.get_feature_importance()\n",
    "feature_names = [f\"Feature_{i}\" for i in range(len(feature_importance))]\n",
    "\n",
    "# Sort and get top 20\n",
    "sorted_idx = np.argsort(feature_importance)[-20:]\n",
    "sorted_importance = feature_importance[sorted_idx]\n",
    "sorted_names = [feature_names[i] for i in sorted_idx]\n",
    "\n",
    "axes[0].barh(range(len(sorted_importance)), sorted_importance, color='steelblue', alpha=0.8)\n",
    "axes[0].set_yticks(range(len(sorted_importance)))\n",
    "axes[0].set_yticklabels(sorted_names)\n",
    "axes[0].set_xlabel('Importance', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Top 20 Feature Importances', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Confusion matrix heatmap\n",
    "sns.heatmap(cm_cat, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
    "           xticklabels=['Not Fraud', 'Fraud'],\n",
    "           yticklabels=['Not Fraud', 'Fraud'],\n",
    "           cbar_kws={'label': 'Count'})\n",
    "axes[1].set_title('CatBoost Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ================================================================\n",
    "# SAVE MODEL & RESULTS\n",
    "# ================================================================\n",
    "print(\"\\n7ï¸âƒ£ Saving model and results...\")\n",
    "\n",
    "cat_model.save_model('catboost_model.cbm')\n",
    "print(\"   âœ“ Model saved: catboost_model.cbm\")\n",
    "\n",
    "cat_results = {\n",
    "    'model_name': 'CatBoost',\n",
    "    'val_auc_roc': val_auc_roc_cat,\n",
    "    'val_auc_pr': val_auc_pr_cat,\n",
    "    'best_iteration': cat_model.get_best_iteration()\n",
    "}\n",
    "\n",
    "with open('cat_results.pkl', 'wb') as f:\n",
    "    pickle.dump(cat_results, f)\n",
    "print(\"   âœ“ Results saved: cat_results.pkl\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… CATBOOST TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"   Best iteration: {cat_model.get_best_iteration()}\")\n",
    "print(f\"   Training samples used: 296K (50% of SMOTE)\")\n",
    "print(f\"   AUC-ROC: {val_auc_roc_cat:.4f}\")\n",
    "print(f\"   AUC-PR:  {val_auc_pr_cat:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IW50UOAh0Wol"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ðŸ§  BLOK 16: DEEP LEARNING - MODEL DEFINITION\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "Run time: ~10 seconds\n",
    "Description: Define neural network architecture with Focal Loss\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ§  DEEP LEARNING MODEL DEFINITION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# âœ… FIXED: Focal Loss with proper dtype handling\n",
    "def focal_loss(gamma=2.0, alpha=0.75):\n",
    "    \"\"\"\n",
    "    Focal Loss for imbalanced classification.\n",
    "\n",
    "    Formula: FL(p_t) = -alpha * (1-p_t)^gamma * log(p_t)\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    gamma : float (default=2.0)\n",
    "        Focusing parameter. Higher = more focus on hard examples.\n",
    "        - gamma=0: equivalent to standard cross-entropy\n",
    "        - gamma=2: strong focus on hard misclassified examples\n",
    "\n",
    "    alpha : float (default=0.75)\n",
    "        Class balance parameter. For fraud detection:\n",
    "        - alpha=0.75: heavily weight minority class (fraud)\n",
    "        - alpha=0.25: weight majority class\n",
    "\n",
    "    Why Focal Loss:\n",
    "    ---------------\n",
    "    1. Down-weights easy examples (majority class)\n",
    "    2. Focuses learning on hard examples (minority class)\n",
    "    3. Better than class weights for severe imbalance\n",
    "    4. No need for SMOTE in neural networks (though we use both)\n",
    "\n",
    "    Mathematical Intuition:\n",
    "    ----------------------\n",
    "    For well-classified examples (p_t â†’ 1):\n",
    "        (1 - p_t)^gamma â†’ 0, so loss â†’ 0\n",
    "    For misclassified examples (p_t â†’ 0):\n",
    "        (1 - p_t)^gamma â†’ 1, so loss = full CE loss\n",
    "\n",
    "    This creates adaptive weighting!\n",
    "    \"\"\"\n",
    "    def loss(y_true, y_pred):\n",
    "        # âœ… FIX: Ensure consistent dtypes\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "\n",
    "        # Clip predictions to prevent log(0)\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Binary cross entropy\n",
    "        bce = -y_true * tf.math.log(y_pred) - (1 - y_true) * tf.math.log(1 - y_pred)\n",
    "\n",
    "        # Focal term: (1 - p_t)^gamma\n",
    "        # p_t is the probability of the true class\n",
    "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        focal_term = tf.pow(1 - p_t, gamma)\n",
    "\n",
    "        # Alpha weighting for class balance\n",
    "        alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "\n",
    "        # Final focal loss\n",
    "        return alpha_t * focal_term * bce\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Build model\n",
    "def build_fraud_nn(input_dim):\n",
    "    \"\"\"\n",
    "    Neural network for fraud detection.\n",
    "\n",
    "    Architecture Philosophy:\n",
    "    ------------------------\n",
    "    1. **Deep but not too deep**: 3 hidden layers\n",
    "       - Too shallow: underfits complex fraud patterns\n",
    "       - Too deep: overfits on limited fraud examples\n",
    "\n",
    "    2. **Wide then narrow**: 256 â†’ 128 â†’ 64\n",
    "       - Wider layers first: learn diverse features\n",
    "       - Narrower layers: compress to essential patterns\n",
    "\n",
    "    3. **Heavy regularization**: 30% dropout, BatchNorm\n",
    "       - Fraud data is noisy\n",
    "       - Need to prevent memorizing specific transactions\n",
    "\n",
    "    4. **LeakyReLU over ReLU**:\n",
    "       - ReLU: dies with negative gradients\n",
    "       - LeakyReLU: always has gradient (alpha=0.1)\n",
    "\n",
    "    Components Explained:\n",
    "    --------------------\n",
    "    - BatchNormalization: Normalizes activations, stabilizes training\n",
    "    - Dropout(0.3): Randomly disables 30% of neurons, prevents overfitting\n",
    "    - LeakyReLU: Activation with small negative slope\n",
    "    - Dense(1, sigmoid): Output layer, gives probability [0,1]\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_dim : int\n",
    "        Number of input features (should be ~280 after feature engineering)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    keras.Model\n",
    "        Compiled model ready for training\n",
    "    \"\"\"\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        # Input layer\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "\n",
    "        # Hidden layer 1: Wide feature extraction\n",
    "        layers.Dense(256, kernel_initializer='he_normal'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(alpha=0.1),\n",
    "        layers.Dropout(0.3),\n",
    "\n",
    "        # Hidden layer 2: Feature compression\n",
    "        layers.Dense(128, kernel_initializer='he_normal'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(alpha=0.1),\n",
    "        layers.Dropout(0.3),\n",
    "\n",
    "        # Hidden layer 3: Pattern extraction\n",
    "        layers.Dense(64, kernel_initializer='he_normal'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(alpha=0.1),\n",
    "        layers.Dropout(0.3),\n",
    "\n",
    "        # Output layer: Binary classification\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Compile with focal loss\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=focal_loss(gamma=2.0, alpha=0.75),\n",
    "        metrics=[\n",
    "            keras.metrics.AUC(name='auc_roc'),\n",
    "            keras.metrics.AUC(curve='PR', name='auc_pr'),\n",
    "            keras.metrics.Precision(name='precision'),\n",
    "            keras.metrics.Recall(name='recall')\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create model\n",
    "print(\"\\nðŸ—ï¸ Building neural network...\")\n",
    "\n",
    "# Get input dimension\n",
    "if 'X_train_smote' in globals():\n",
    "    input_dim = X_train_smote.shape[1]\n",
    "else:\n",
    "    # Load from file\n",
    "    with open('X_train_smote.pkl', 'rb') as f:\n",
    "        X_train_smote_temp = pickle.load(f)\n",
    "    input_dim = X_train_smote_temp.shape[1]\n",
    "    del X_train_smote_temp\n",
    "\n",
    "nn_model = build_fraud_nn(input_dim=input_dim)\n",
    "\n",
    "print(\"\\nðŸ“Š MODEL ARCHITECTURE:\")\n",
    "print(\"=\"*70)\n",
    "nn_model.summary()\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nðŸ“‹ MODEL DETAILS:\")\n",
    "print(f\"   Input features: {input_dim}\")\n",
    "print(f\"   Total parameters: {nn_model.count_params():,}\")\n",
    "print(f\"   Trainable parameters: {sum([tf.size(w).numpy() for w in nn_model.trainable_weights]):,}\")\n",
    "print(f\"   Loss function: Focal Loss (gamma=2.0, alpha=0.75)\")\n",
    "print(f\"   Optimizer: Adam (lr=0.001)\")\n",
    "\n",
    "print(\"\\nâœ… Model defined successfully!\")\n",
    "print(\"\\nðŸ“Œ Ready for training (Blok 17)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pqll3gKh0YrH"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ðŸ§  BLOK 17: DEEP LEARNING - TRAINING\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "Run time: ~10-15 minutes\n",
    "Description: Train neural network with callbacks\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ§  DEEP LEARNING TRAINING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# âœ… Load data if not in memory\n",
    "print(\"\\n1ï¸âƒ£ Loading training data...\")\n",
    "\n",
    "if 'X_train_smote' not in globals():\n",
    "    print(\"   Loading from saved files...\")\n",
    "    with open('X_train_smote.pkl', 'rb') as f:\n",
    "        X_train_smote = pickle.load(f)\n",
    "    with open('y_train_smote.pkl', 'rb') as f:\n",
    "        y_train_smote = pickle.load(f)\n",
    "    with open('X_val_scaled.pkl', 'rb') as f:\n",
    "        X_val_scaled = pickle.load(f)\n",
    "    with open('y_val.pkl', 'rb') as f:\n",
    "        y_val = pickle.load(f)\n",
    "\n",
    "print(f\"   âœ“ Train samples: {len(X_train_smote):,}\")\n",
    "print(f\"   âœ“ Validation samples: {len(X_val_scaled):,}\")\n",
    "\n",
    "# Callbacks for training\n",
    "print(\"\\n2ï¸âƒ£ Setting up callbacks...\")\n",
    "\n",
    "callbacks = [\n",
    "    # Early stopping: Stop if validation AUC-PR doesn't improve for 15 epochs\n",
    "    EarlyStopping(\n",
    "        monitor='val_auc_pr',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "\n",
    "    # Learning rate reduction: Reduce LR if validation plateaus\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_auc_pr',\n",
    "        factor=0.5,           # Reduce LR by half\n",
    "        patience=5,           # Wait 5 epochs before reducing\n",
    "        min_lr=1e-6,          # Don't go below this\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"   âœ“ Early stopping: patience=15 epochs\")\n",
    "print(\"   âœ“ LR reduction: factor=0.5, patience=5 epochs\")\n",
    "\n",
    "# Train model\n",
    "print(\"\\n3ï¸âƒ£ Training neural network...\")\n",
    "print(\"   â³ This will take 10-15 minutes...\")\n",
    "print(\"   Monitoring: AUC-PR on validation set\")\n",
    "print(\"   Expected: ~50-80 epochs until early stopping\")\n",
    "print()\n",
    "\n",
    "# âœ… Convert to numpy arrays (TensorFlow works better with numpy)\n",
    "X_train_np = X_train_smote if isinstance(X_train_smote, np.ndarray) else X_train_smote.values\n",
    "y_train_np = y_train_smote if isinstance(y_train_smote, np.ndarray) else y_train_smote.values\n",
    "X_val_np = X_val_scaled if isinstance(X_val_scaled, np.ndarray) else X_val_scaled\n",
    "y_val_np = y_val if isinstance(y_val, np.ndarray) else y_val.values\n",
    "\n",
    "history = nn_model.fit(\n",
    "    X_train_np, y_train_np,\n",
    "    validation_data=(X_val_np, y_val_np),\n",
    "    epochs=100,\n",
    "    batch_size=1024,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ================================================================\n",
    "# EVALUATION\n",
    "# ================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“Š EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Predictions\n",
    "y_val_pred_nn = nn_model.predict(X_val_np, verbose=0).flatten()\n",
    "\n",
    "val_auc_roc_nn = roc_auc_score(y_val, y_val_pred_nn)\n",
    "val_auc_pr_nn = average_precision_score(y_val, y_val_pred_nn)\n",
    "\n",
    "print(f\"\\n   ðŸ“Š VALIDATION METRICS:\")\n",
    "print(f\"      AUC-ROC: {val_auc_roc_nn:.4f}\")\n",
    "print(f\"      AUC-PR:  {val_auc_pr_nn:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "y_val_pred_nn_binary = (y_val_pred_nn > 0.5).astype(int)\n",
    "print(f\"\\n   ðŸ“‹ CLASSIFICATION REPORT (threshold=0.5):\")\n",
    "print(classification_report(y_val, y_val_pred_nn_binary,\n",
    "                           target_names=['Not Fraud', 'Fraud'],\n",
    "                           digits=4))\n",
    "\n",
    "# Confusion matrix\n",
    "cm_nn = confusion_matrix(y_val, y_val_pred_nn_binary)\n",
    "print(f\"\\n   ðŸ”¢ CONFUSION MATRIX:\")\n",
    "print(f\"      [[TN={cm_nn[0,0]:,}  FP={cm_nn[0,1]:,}]\")\n",
    "print(f\"       [FN={cm_nn[1,0]:,}  TP={cm_nn[1,1]:,}]]\")\n",
    "\n",
    "# ================================================================\n",
    "# VISUALIZATIONS\n",
    "# ================================================================\n",
    "print(\"\\n4ï¸âƒ£ Creating visualizations...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# AUC-ROC over epochs\n",
    "axes[0, 0].plot(history.history['auc_roc'], label='Train', linewidth=2)\n",
    "axes[0, 0].plot(history.history['val_auc_roc'], label='Validation', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('AUC-ROC', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_title('AUC-ROC over Epochs', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# AUC-PR over epochs (most important!)\n",
    "axes[0, 1].plot(history.history['auc_pr'], label='Train', linewidth=2)\n",
    "axes[0, 1].plot(history.history['val_auc_pr'], label='Validation', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('AUC-PR', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_title('AUC-PR over Epochs (Key Metric)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Loss over epochs\n",
    "axes[1, 0].plot(history.history['loss'], label='Train', linewidth=2)\n",
    "axes[1, 0].plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Focal Loss', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_title('Loss over Epochs', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Precision-Recall over epochs\n",
    "axes[1, 1].plot(history.history['precision'], label='Precision (Train)', linewidth=2)\n",
    "axes[1, 1].plot(history.history['recall'], label='Recall (Train)', linewidth=2)\n",
    "axes[1, 1].plot(history.history['val_precision'], label='Precision (Val)', linewidth=2, linestyle='--')\n",
    "axes[1, 1].plot(history.history['val_recall'], label='Recall (Val)', linewidth=2, linestyle='--')\n",
    "axes[1, 1].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_title('Precision & Recall over Epochs', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix heatmap\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "sns.heatmap(cm_nn, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "           xticklabels=['Not Fraud', 'Fraud'],\n",
    "           yticklabels=['Not Fraud', 'Fraud'],\n",
    "           cbar_kws={'label': 'Count'})\n",
    "ax.set_title('Neural Network Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ================================================================\n",
    "# SAVE MODEL & RESULTS\n",
    "# ================================================================\n",
    "print(\"\\n5ï¸âƒ£ Saving model and results...\")\n",
    "\n",
    "# Save model\n",
    "nn_model.save('nn_model.keras')\n",
    "print(\"   âœ“ Model saved: nn_model.keras\")\n",
    "\n",
    "# Save history\n",
    "with open('nn_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "print(\"   âœ“ History saved: nn_history.pkl\")\n",
    "\n",
    "# Save results\n",
    "nn_results = {\n",
    "    'model_name': 'Neural Network',\n",
    "    'val_auc_roc': val_auc_roc_nn,\n",
    "    'val_auc_pr': val_auc_pr_nn,\n",
    "    'epochs_trained': len(history.history['loss']),\n",
    "    'best_epoch': np.argmax(history.history['val_auc_pr']) + 1\n",
    "}\n",
    "\n",
    "with open('nn_results.pkl', 'wb') as f:\n",
    "    pickle.dump(nn_results, f)\n",
    "print(\"   âœ“ Results saved: nn_results.pkl\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… NEURAL NETWORK TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"   Epochs trained: {len(history.history['loss'])}\")\n",
    "print(f\"   Best epoch: {np.argmax(history.history['val_auc_pr']) + 1}\")\n",
    "print(f\"   Final AUC-PR: {val_auc_pr_nn:.4f}\")\n",
    "print(\"\\nðŸ“Œ Ready for model comparison (Blok 18)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u53M_LlQ0bgU"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ðŸ“Š BLOK 18: ROC & PR CURVES COMPARISON\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "Run time: ~30 seconds\n",
    "Description: Compare all models with ROC and PR curves\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ“Š MODEL COMPARISON - ROC & PR CURVES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# âœ… FIX: Safely collect all predictions (handle missing models)\n",
    "print(\"\\n1ï¸âƒ£ Collecting model predictions...\")\n",
    "\n",
    "models = {}\n",
    "\n",
    "# LightGBM Baseline (should always exist)\n",
    "if 'y_val_pred' in globals():\n",
    "    models['LightGBM Baseline'] = y_val_pred\n",
    "    print(\"   âœ“ LightGBM Baseline found\")\n",
    "else:\n",
    "    print(\"   âš ï¸ LightGBM Baseline not found\")\n",
    "\n",
    "# LightGBM Tuned (might be skipped)\n",
    "if 'y_val_pred_tuned' in globals():\n",
    "    models['LightGBM Tuned'] = y_val_pred_tuned\n",
    "    print(\"   âœ“ LightGBM Tuned found\")\n",
    "else:\n",
    "    print(\"   âš ï¸ LightGBM Tuned not found (skipped or not run)\")\n",
    "\n",
    "# XGBoost (should exist)\n",
    "if 'y_val_pred_xgb' in globals():\n",
    "    models['XGBoost'] = y_val_pred_xgb\n",
    "    print(\"   âœ“ XGBoost found\")\n",
    "else:\n",
    "    print(\"   âš ï¸ XGBoost not found\")\n",
    "\n",
    "# CatBoost (might be skipped)\n",
    "if 'y_val_pred_cat' in globals():\n",
    "    models['CatBoost'] = y_val_pred_cat\n",
    "    print(\"   âœ“ CatBoost found\")\n",
    "else:\n",
    "    print(\"   âš ï¸ CatBoost not found (skipped or not run)\")\n",
    "\n",
    "# Neural Network (might be skipped)\n",
    "if 'y_val_pred_nn' in globals():\n",
    "    models['Neural Network'] = y_val_pred_nn\n",
    "    print(\"   âœ“ Neural Network found\")\n",
    "else:\n",
    "    print(\"   âš ï¸ Neural Network not found (skipped or not run)\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ Total models to compare: {len(models)}\")\n",
    "\n",
    "if len(models) == 0:\n",
    "    print(\"\\nâŒ ERROR: No model predictions found!\")\n",
    "    print(\"   Please run training blocks (12-17) first.\")\n",
    "else:\n",
    "    # ================================================================\n",
    "    # PLOT ROC & PR CURVES\n",
    "    # ================================================================\n",
    "    print(\"\\n2ï¸âƒ£ Generating comparison curves...\")\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # ROC Curves\n",
    "    print(\"\\nðŸ“ˆ ROC CURVES:\")\n",
    "    for name, preds in models.items():\n",
    "        fpr, tpr, _ = roc_curve(y_val, preds)\n",
    "        auc_score = roc_auc_score(y_val, preds)\n",
    "        axes[0].plot(fpr, tpr, label=f'{name} (AUC={auc_score:.4f})', linewidth=2.5)\n",
    "        print(f\"   {name:20s}: AUC-ROC = {auc_score:.4f}\")\n",
    "\n",
    "    axes[0].plot([0, 1], [0, 1], 'k--', label='Random', linewidth=2)\n",
    "    axes[0].set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_title('ROC Curves Comparison', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend(loc='lower right', fontsize=10)\n",
    "    axes[0].grid(alpha=0.3)\n",
    "\n",
    "    # PR Curves (MORE IMPORTANT for imbalanced data!)\n",
    "    print(\"\\nðŸ“ˆ PRECISION-RECALL CURVES:\")\n",
    "    for name, preds in models.items():\n",
    "        precision, recall, _ = precision_recall_curve(y_val, preds)\n",
    "        ap_score = average_precision_score(y_val, preds)\n",
    "        axes[1].plot(recall, precision, label=f'{name} (AP={ap_score:.4f})', linewidth=2.5)\n",
    "        print(f\"   {name:20s}: AUC-PR = {ap_score:.4f}\")\n",
    "\n",
    "    # Baseline (random classifier)\n",
    "    fraud_rate = (y_val == 1).sum() / len(y_val)\n",
    "    axes[1].axhline(y=fraud_rate, color='k', linestyle='--', linewidth=2,\n",
    "                   label=f'Random (AP={fraud_rate:.4f})')\n",
    "\n",
    "    axes[1].set_xlabel('Recall', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_ylabel('Precision', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_title('Precision-Recall Curves Comparison', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend(loc='upper right', fontsize=10)\n",
    "    axes[1].grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ================================================================\n",
    "    # KEY INSIGHTS\n",
    "    # ================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸ’¡ KEY INSIGHTS\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Find best model by AUC-PR\n",
    "    best_model = max(models.items(), key=lambda x: average_precision_score(y_val, x[1]))\n",
    "    best_name, best_preds = best_model\n",
    "    best_auc_pr = average_precision_score(y_val, best_preds)\n",
    "    best_auc_roc = roc_auc_score(y_val, best_preds)\n",
    "\n",
    "    print(f\"\\nðŸ† BEST MODEL (by AUC-PR): {best_name}\")\n",
    "    print(f\"   AUC-ROC: {best_auc_roc:.4f}\")\n",
    "    print(f\"   AUC-PR:  {best_auc_pr:.4f}\")\n",
    "\n",
    "    # Calculate improvement over baseline\n",
    "    if 'LightGBM Baseline' in models:\n",
    "        baseline_auc_pr = average_precision_score(y_val, models['LightGBM Baseline'])\n",
    "        improvement = ((best_auc_pr - baseline_auc_pr) / baseline_auc_pr) * 100\n",
    "        print(f\"\\nðŸ“ˆ IMPROVEMENT OVER BASELINE:\")\n",
    "        print(f\"   Baseline AUC-PR: {baseline_auc_pr:.4f}\")\n",
    "        print(f\"   Best AUC-PR:     {best_auc_pr:.4f}\")\n",
    "        print(f\"   Improvement:     {improvement:+.2f}%\")\n",
    "\n",
    "    # Fraud detection rate at different thresholds\n",
    "    print(f\"\\nðŸ“Š FRAUD DETECTION RATES (Best Model: {best_name}):\")\n",
    "    for threshold in [0.3, 0.5, 0.7]:\n",
    "        preds_binary = (best_preds > threshold).astype(int)\n",
    "        recall = recall_score(y_val, preds_binary)\n",
    "        precision = precision_score(y_val, preds_binary)\n",
    "        print(f\"   Threshold {threshold:.1f}: Recall={recall:.4f}, Precision={precision:.4f}\")\n",
    "\n",
    "    print(\"\\nâœ… Curve comparison complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x05Pan7h0esx"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ðŸ† BLOK 19: FINAL MODEL COMPARISON\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "Run time: ~10 seconds\n",
    "Description: Create comprehensive comparison table\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ† FINAL MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# âœ… FIX: Safely collect all predictions\n",
    "print(\"\\n1ï¸âƒ£ Collecting model predictions...\")\n",
    "\n",
    "models = {}\n",
    "\n",
    "if 'y_val_pred' in globals():\n",
    "    models['LightGBM Baseline'] = y_val_pred\n",
    "if 'y_val_pred_tuned' in globals():\n",
    "    models['LightGBM Tuned'] = y_val_pred_tuned\n",
    "if 'y_val_pred_xgb' in globals():\n",
    "    models['XGBoost'] = y_val_pred_xgb\n",
    "if 'y_val_pred_cat' in globals():\n",
    "    models['CatBoost'] = y_val_pred_cat\n",
    "if 'y_val_pred_nn' in globals():\n",
    "    models['Neural Network'] = y_val_pred_nn\n",
    "\n",
    "if len(models) == 0:\n",
    "    print(\"\\nâŒ ERROR: No models found!\")\n",
    "    print(\"   Please run training blocks (12-17) first.\")\n",
    "else:\n",
    "    print(f\"   âœ“ Found {len(models)} models\")\n",
    "\n",
    "    # ================================================================\n",
    "    # COMPILE METRICS\n",
    "    # ================================================================\n",
    "    print(\"\\n2ï¸âƒ£ Computing comprehensive metrics...\")\n",
    "\n",
    "    comparison_data = []\n",
    "\n",
    "    for name, preds in models.items():\n",
    "        # Probabilistic metrics\n",
    "        auc_roc = roc_auc_score(y_val, preds)\n",
    "        auc_pr = average_precision_score(y_val, preds)\n",
    "\n",
    "        # Binary predictions at 0.5 threshold\n",
    "        preds_binary = (preds > 0.5).astype(int)\n",
    "        f1 = f1_score(y_val, preds_binary)\n",
    "        precision = precision_score(y_val, preds_binary)\n",
    "        recall = recall_score(y_val, preds_binary)\n",
    "\n",
    "        comparison_data.append({\n",
    "            'Model': name,\n",
    "            'AUC-ROC': auc_roc,\n",
    "            'AUC-PR': auc_pr,\n",
    "            'F1-Score': f1,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall\n",
    "        })\n",
    "\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "    # Sort by AUC-PR (most important for imbalanced data!)\n",
    "    comparison_df = comparison_df.sort_values('AUC-PR', ascending=False)\n",
    "\n",
    "    # Format for display\n",
    "    comparison_df_display = comparison_df.copy()\n",
    "    for col in ['AUC-ROC', 'AUC-PR', 'F1-Score', 'Precision', 'Recall']:\n",
    "        comparison_df_display[col] = comparison_df_display[col].apply(lambda x: f'{x:.4f}')\n",
    "\n",
    "    # ================================================================\n",
    "    # DISPLAY RESULTS\n",
    "    # ================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸ“Š MODEL PERFORMANCE COMPARISON\")\n",
    "    print(\"=\"*70)\n",
    "    display(comparison_df_display)\n",
    "\n",
    "    # Find best model\n",
    "    best_model_name = comparison_df.iloc[0]['Model']\n",
    "    best_auc_pr = comparison_df.iloc[0]['AUC-PR']\n",
    "    best_auc_roc = comparison_df.iloc[0]['AUC-ROC']\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"ðŸ† BEST MODEL: {best_model_name}\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"   AUC-ROC:  {best_auc_roc:.4f}\")\n",
    "    print(f\"   AUC-PR:   {best_auc_pr:.4f}\")\n",
    "    print(f\"   F1-Score: {comparison_df.iloc[0]['F1-Score']:.4f}\")\n",
    "    print(f\"   Precision: {comparison_df.iloc[0]['Precision']:.4f}\")\n",
    "    print(f\"   Recall:   {comparison_df.iloc[0]['Recall']:.4f}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # ================================================================\n",
    "    # SAVE RESULTS\n",
    "    # ================================================================\n",
    "    print(\"\\n3ï¸âƒ£ Saving comparison results...\")\n",
    "\n",
    "    comparison_df.to_csv('model_comparison.csv', index=False)\n",
    "    print(\"   âœ“ Saved: model_comparison.csv\")\n",
    "\n",
    "    with open('best_model_info.pkl', 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'name': best_model_name,\n",
    "            'auc_pr': best_auc_pr,\n",
    "            'auc_roc': best_auc_roc\n",
    "        }, f)\n",
    "    print(\"   âœ“ Saved: best_model_info.pkl\")\n",
    "\n",
    "    # ================================================================\n",
    "    # VISUALIZE COMPARISON\n",
    "    # ================================================================\n",
    "    print(\"\\n4ï¸âƒ£ Creating comparison visualizations...\")\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # AUC comparison\n",
    "    x = range(len(comparison_df))\n",
    "    width = 0.35\n",
    "\n",
    "    axes[0].bar([i - width/2 for i in x], comparison_df['AUC-ROC'],\n",
    "               width, label='AUC-ROC', alpha=0.8, color='steelblue')\n",
    "    axes[0].bar([i + width/2 for i in x], comparison_df['AUC-PR'],\n",
    "               width, label='AUC-PR', alpha=0.8, color='coral')\n",
    "    axes[0].set_xlabel('Model', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_title('AUC Scores Comparison', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(comparison_df['Model'], rotation=45, ha='right')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    axes[0].set_ylim([0.5, 1.0])\n",
    "\n",
    "    # Other metrics\n",
    "    axes[1].bar([i - width for i in x], comparison_df['F1-Score'],\n",
    "               width, label='F1-Score', alpha=0.8, color='mediumseagreen')\n",
    "    axes[1].bar([i for i in x], comparison_df['Precision'],\n",
    "               width, label='Precision', alpha=0.8, color='gold')\n",
    "    axes[1].bar([i + width for i in x], comparison_df['Recall'],\n",
    "               width, label='Recall', alpha=0.8, color='tomato')\n",
    "    axes[1].set_xlabel('Model', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_title('Classification Metrics Comparison', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels(comparison_df['Model'], rotation=45, ha='right')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ================================================================\n",
    "    # RANKING ANALYSIS\n",
    "    # ================================================================\n",
    "    print(\"\\n5ï¸âƒ£ Ranking analysis...\")\n",
    "\n",
    "    print(\"\\nðŸ“Š MODEL RANKINGS BY METRIC:\")\n",
    "    for metric in ['AUC-PR', 'AUC-ROC', 'F1-Score']:\n",
    "        print(f\"\\n   {metric}:\")\n",
    "        ranked = comparison_df.sort_values(metric, ascending=False)\n",
    "        for idx, row in ranked.iterrows():\n",
    "            print(f\"      {row['Model']:20s}: {row[metric]:.4f}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"âœ… MODEL COMPARISON COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"   Best model: {best_model_name}\")\n",
    "    print(f\"   Ready for final submission (Blok 20)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O40AfWTY0gY6"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ðŸ“¤ BLOK 20: MINIMAL SUBMISSION (GUARANTEED TO WORK)\n",
    "\"\"\"\n",
    "\n",
    "import gc\n",
    "\n",
    "print(\"ðŸ“¤ GENERATING SUBMISSION (MINIMAL)\")\n",
    "\n",
    "# 1. Detect best model\n",
    "best_model_name = 'LightGBM Tuned'  # â† Your best model!\n",
    "print(f\"Using: {best_model_name}\")\n",
    "\n",
    "# 2. Load test data\n",
    "test_df = pd.read_csv(f\"{BASE_PATH}/test_engineered.csv\")\n",
    "test_ids = test_df['TransactionID'].copy()\n",
    "test_df = test_df.drop(columns=['TransactionID', 'TransactionDT'])\n",
    "\n",
    "# Convert to float32\n",
    "for col in test_df.select_dtypes(include=['float64']).columns:\n",
    "    test_df[col] = test_df[col].astype('float32')\n",
    "\n",
    "# 3. Preprocess\n",
    "with open('label_encoders.pkl', 'rb') as f:\n",
    "    label_encoders = pickle.load(f)\n",
    "with open('split_info.pkl', 'rb') as f:\n",
    "    split_info = pickle.load(f)\n",
    "\n",
    "for col in split_info['categorical_cols']:\n",
    "    if col in test_df.columns:\n",
    "        test_df[col] = test_df[col].astype(str)\n",
    "        test_df[col] = test_df[col].apply(\n",
    "            lambda x: label_encoders[col].transform([x])[0]\n",
    "            if x in label_encoders[col].classes_\n",
    "            else label_encoders[col].transform([label_encoders[col].classes_[0]])[0]\n",
    "        )\n",
    "\n",
    "with open('scaler.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "test_scaled = scaler.transform(test_df)\n",
    "del test_df\n",
    "gc.collect()\n",
    "\n",
    "# 4. Predict\n",
    "lgb_tuned = lgb.Booster(model_file='lgb_tuned.txt')\n",
    "test_predictions = lgb_tuned.predict(test_scaled)\n",
    "del test_scaled\n",
    "gc.collect()\n",
    "\n",
    "# 5. Save\n",
    "submission = pd.DataFrame({\n",
    "    'TransactionID': test_ids,\n",
    "    'isFraud': test_predictions\n",
    "})\n",
    "submission.to_csv(f\"{BASE_PATH}/submission_final_uas.csv\", index=False)\n",
    "\n",
    "print(f\"âœ… DONE! Saved {len(submission):,} predictions\")\n",
    "print(f\"Mean fraud prob: {submission['isFraud'].mean():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNQNp19s9OEMG+Lo5lSn3mq",
   "mount_file_id": "1FIbfLYUaDYadK7n67w8sPrd5axcjCjuy",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "fraud_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03ef952f696942a3b4628f96bd75c40d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "113f80122d2d4c509acdf9246203b039": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "140bddafec18486fbf9a422f943fc065": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d6e165a81f0140e0b8fccc17f8ec0503",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_b9f7ba5bb3f5407c99b52836bd1bdf48",
      "value": "Bestâ€‡trial:â€‡18.â€‡Bestâ€‡value:â€‡0.843914:â€‡100%"
     }
    },
    "1598b99f92254ceca4c6f12a01c94f75": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e36edcf2ca548efa9529a10020ff904": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "20fe9ace079c425b9cc497e1710343f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_140bddafec18486fbf9a422f943fc065",
       "IPY_MODEL_e0c42842a05647519a60bbe1e984f6c8",
       "IPY_MODEL_653fe8de60814d00be4e7654147b9846"
      ],
      "layout": "IPY_MODEL_f3c3a88ed9ea4759aecf738f795ba520"
     }
    },
    "26af0ee13170473aaba9a99d8083a27c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f047d2f8c3ed4b1c9170df572eda1ebb",
       "IPY_MODEL_8165f1a82512434cb85b09722ec4ddc5",
       "IPY_MODEL_39f8ed3c63414b939eb3a6026f3327ab"
      ],
      "layout": "IPY_MODEL_c63f03e0da0842898f1f94255aac6926"
     }
    },
    "3840dbbd50d740a99bfd1fc9fc1e90da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_72f76f13ff864b0693387fb096d27819",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_709cf7560cb74ec6af04463357eb6ad4",
      "value": "â€‡211/211â€‡[00:06&lt;00:00,â€‡58.10it/s]"
     }
    },
    "39f8ed3c63414b939eb3a6026f3327ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7d1e978533b744df9b8350a6bb585e79",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f17e03338f70405da83bf482a6cefef2",
      "value": "â€‡394/394â€‡[00:00&lt;00:00,â€‡8461.15it/s]"
     }
    },
    "44e88c4cea2a495aa7e232f96a2b03fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4d98545d51104d3a9929ac9d2fb22e44",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_5f85634ce40942d296034b06dd48988d",
      "value": "Numerical:â€‡100%"
     }
    },
    "464caa6d472c443284a5955f282e2e6f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4868a81828e04c68ba352fe4bc093774": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d98545d51104d3a9929ac9d2fb22e44": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "516c797614184a0db515f097422fa5fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c87bd46536245e0a064194979549420",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f9fc816eb6174928b2575ef29ba64204",
      "value": "â€‡13/13â€‡[00:01&lt;00:00,â€‡11.76it/s]"
     }
    },
    "523522b086794074bc771e2c380762db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "58de9d02158543ecbe18799af0779962": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_44e88c4cea2a495aa7e232f96a2b03fb",
       "IPY_MODEL_bfc2346b58b94c2bb98590b22cc351ee",
       "IPY_MODEL_3840dbbd50d740a99bfd1fc9fc1e90da"
      ],
      "layout": "IPY_MODEL_03ef952f696942a3b4628f96bd75c40d"
     }
    },
    "5f85634ce40942d296034b06dd48988d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "62d2a383b63f438fb2c181f1ad2b4b57": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "653fe8de60814d00be4e7654147b9846": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4868a81828e04c68ba352fe4bc093774",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f3d32f7a10e346609502cdf726f26616",
      "value": "â€‡20/20â€‡[1:32:35&lt;00:00,â€‡335.99s/it]"
     }
    },
    "6c87bd46536245e0a064194979549420": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "709cf7560cb74ec6af04463357eb6ad4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "70c5ad46337d4257b66ad07fbf40e4b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_eed4dc107203457fa535c1ebbf43aff0",
       "IPY_MODEL_f0acd25cb104435e98aab7dc30c985bc",
       "IPY_MODEL_516c797614184a0db515f097422fa5fc"
      ],
      "layout": "IPY_MODEL_113f80122d2d4c509acdf9246203b039"
     }
    },
    "72f76f13ff864b0693387fb096d27819": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "743413be089042e0b2aaa970c49d3605": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d1e978533b744df9b8350a6bb585e79": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8165f1a82512434cb85b09722ec4ddc5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_464caa6d472c443284a5955f282e2e6f",
      "max": 394,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bfaa220863474ce496dce68a1a4b4b09",
      "value": 394
     }
    },
    "97410c9a5a854ba48f3af119019df402": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9d2641c37a4741aeb1fa2aae75fcc973": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b159f0a2438641f8836d15ffd525c85b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b9f7ba5bb3f5407c99b52836bd1bdf48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bfaa220863474ce496dce68a1a4b4b09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bfc2346b58b94c2bb98590b22cc351ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1598b99f92254ceca4c6f12a01c94f75",
      "max": 211,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_97410c9a5a854ba48f3af119019df402",
      "value": 211
     }
    },
    "c63f03e0da0842898f1f94255aac6926": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d6e165a81f0140e0b8fccc17f8ec0503": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0c42842a05647519a60bbe1e984f6c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_faa66945f3ae480f8e50e23b01bf60a3",
      "max": 20,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1e36edcf2ca548efa9529a10020ff904",
      "value": 20
     }
    },
    "eed4dc107203457fa535c1ebbf43aff0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_62d2a383b63f438fb2c181f1ad2b4b57",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_b159f0a2438641f8836d15ffd525c85b",
      "value": "Categorical:â€‡100%"
     }
    },
    "f047d2f8c3ed4b1c9170df572eda1ebb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_743413be089042e0b2aaa970c49d3605",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_523522b086794074bc771e2c380762db",
      "value": "Analyzingâ€‡columns:â€‡100%"
     }
    },
    "f0acd25cb104435e98aab7dc30c985bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fea47fa41d2b4125a4f8c5ce305ffa89",
      "max": 13,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9d2641c37a4741aeb1fa2aae75fcc973",
      "value": 13
     }
    },
    "f17e03338f70405da83bf482a6cefef2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f3c3a88ed9ea4759aecf738f795ba520": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f3d32f7a10e346609502cdf726f26616": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f9fc816eb6174928b2575ef29ba64204": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "faa66945f3ae480f8e50e23b01bf60a3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fea47fa41d2b4125a4f8c5ce305ffa89": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
