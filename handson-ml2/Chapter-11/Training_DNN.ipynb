{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aac6c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\fraud_ml\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "======================================================================\n",
      "CHAPTER 11: TRAINING DEEP NEURAL NETWORKS\n",
      "Part 1-3: Foundations & Weight Initialization\n",
      "======================================================================\n",
      "\n",
      "üìÖ Execution Time: 2026-01-03 01:36:20\n",
      "üîß TensorFlow Version: 2.15.0\n",
      "\n",
      "======================================================================\n",
      "üî¨ EXPERIMENT 1: Demonstrating Vanishing Gradients\n",
      "======================================================================\n",
      "\n",
      "üìä Dataset Loaded:\n",
      "   ‚Ä¢ Training samples: 60,000\n",
      "   ‚Ä¢ Test samples: 10,000\n",
      "   ‚Ä¢ Image shape: (28, 28)\n",
      "   ‚Ä¢ Classes: 10\n",
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\fraud_ml\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "\n",
      "üèóÔ∏è  Model Architecture (Problematic):\n",
      "   ‚Ä¢ Total layers: 7\n",
      "   ‚Ä¢ Hidden layers: 5\n",
      "   ‚Ä¢ Activation: sigmoid (saturating)\n",
      "   ‚Ä¢ Initialization: RandomNormal(stddev=1.0) (poor)\n",
      "\n",
      "üìä Initial Weight Statistics:\n",
      "   Layer 1: mean=0.0007, std=0.9996, min=-4.0392, max=4.0634\n",
      "   Layer 2: mean=-0.0095, std=0.9837, min=-3.7405, max=4.0880\n",
      "   Layer 3: mean=-0.0081, std=1.0157, min=-3.9114, max=3.8969\n",
      "   Layer 4: mean=-0.0069, std=0.9923, min=-3.8749, max=3.9032\n",
      "   Layer 5: mean=0.0014, std=0.9978, min=-4.6634, max=3.5732\n",
      "\n",
      "üîç Activation Analysis (Forward Pass):\n",
      "   Layer 1: mean=0.4898, std=0.4596, saturated=65.6%\n",
      "   Layer 2: mean=0.4702, std=0.4424, saturated=53.0%\n",
      "   Layer 3: mean=0.4629, std=0.4395, saturated=50.8%\n",
      "   Layer 4: mean=0.4462, std=0.4207, saturated=40.5%\n",
      "   Layer 5: mean=0.5183, std=0.4393, saturated=48.4%\n",
      "\n",
      "‚ö†Ô∏è  PROBLEM DETECTED:\n",
      "   ‚Ä¢ Activations quickly saturate (approach 0 or 1)\n",
      "   ‚Ä¢ Gradients in saturated regions ‚âà 0\n",
      "   ‚Ä¢ Lower layers will barely learn\n",
      "\n",
      "======================================================================\n",
      "üî¨ EXPERIMENT 2: Initialization Comparison\n",
      "======================================================================\n",
      "\n",
      "üìä Weight Distribution Analysis:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Poor (N(0,1)):\n",
      "   Shape: (784, 100)\n",
      "   Mean:  -0.0060\n",
      "   Std:    0.9993\n",
      "   Var:    0.9986\n",
      "   Range: [-4.3965,  4.2745]\n",
      "\n",
      "Glorot:\n",
      "   Shape: (784, 100)\n",
      "   Mean:   0.0001\n",
      "   Std:    0.0477\n",
      "   Var:    0.0023\n",
      "   Range: [-0.0824,  0.0824]\n",
      "   Theoretical Var (Glorot): 0.002262\n",
      "\n",
      "He:\n",
      "   Shape: (784, 100)\n",
      "   Mean:  -0.0000\n",
      "   Std:    0.0505\n",
      "   Var:    0.0025\n",
      "   Range: [-0.1148,  0.1148]\n",
      "   Theoretical Var (He): 0.002551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\fraud_ml\\lib\\site-packages\\keras\\src\\initializers\\initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LeCun:\n",
      "   Shape: (784, 100)\n",
      "   Mean:   0.0001\n",
      "   Std:    0.0357\n",
      "   Var:    0.0013\n",
      "   Range: [-0.0812,  0.0812]\n",
      "   Theoretical Var (LeCun): 0.001276\n",
      "\n",
      "======================================================================\n",
      "üìã INITIALIZATION COMPARISON SUMMARY\n",
      "======================================================================\n",
      "       Method      Mean      Std       Min      Max      Var\n",
      "Poor (N(0,1)) -0.005963 0.999322 -4.396476 4.274479 0.998645\n",
      "       Glorot  0.000095 0.047675 -0.082385 0.082384 0.002273\n",
      "           He -0.000020 0.050481 -0.114838 0.114835 0.002548\n",
      "        LeCun  0.000061 0.035658 -0.081195 0.081161 0.001271\n",
      "\n",
      "======================================================================\n",
      "üî¨ EXPERIMENT 3: Training with Different Initializations\n",
      "======================================================================\n",
      "\n",
      "‚è≥ Training models (this will take a moment)...\n",
      "\n",
      "üèãÔ∏è Training with Poor initialization...\n",
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\fraud_ml\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\fraud_ml\\lib\\site-packages\\keras\\src\\initializers\\initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\fraud_ml\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\fraud_ml\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "   ‚úì Final Training Accuracy: 0.0942\n",
      "   ‚úì Final Validation Accuracy: 0.1000\n",
      "\n",
      "üèãÔ∏è Training with He initialization...\n",
      "   ‚úì Final Training Accuracy: 0.8616\n",
      "   ‚úì Final Validation Accuracy: 0.8490\n",
      "\n",
      "======================================================================\n",
      "üìä CONVERGENCE COMPARISON\n",
      "======================================================================\n",
      "\n",
      "Epoch-by-Epoch Validation Accuracy:\n",
      "----------------------------------------------------------------------\n",
      "Epoch    Poor Init       He Init         Improvement    \n",
      "----------------------------------------------------------------------\n",
      "1        0.1000          0.7395          +0.6395\n",
      "2        0.1000          0.7490          +0.6490\n",
      "3        0.1000          0.8020          +0.7020\n",
      "4        0.1000          0.7680          +0.6680\n",
      "5        0.1000          0.8130          +0.7130\n",
      "6        0.1000          0.8355          +0.7355\n",
      "7        0.1000          0.8275          +0.7275\n",
      "8        0.1000          0.8200          +0.7200\n",
      "9        0.1000          0.8395          +0.7395\n",
      "10       0.1000          0.8490          +0.7490\n",
      "\n",
      "======================================================================\n",
      "‚úÖ PART 1-3 COMPLETED!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CHAPTER 11: TRAINING DEEP NEURAL NETWORKS\n",
    "# Part 1-3: Introduction, Gradients Problem, and Weight Initialization\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CHAPTER 11: TRAINING DEEP NEURAL NETWORKS\")\n",
    "print(\"Part 1-3: Foundations & Weight Initialization\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìÖ Execution Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üîß TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT 1: Demonstrating Vanishing Gradients Problem\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üî¨ EXPERIMENT 1: Demonstrating Vanishing Gradients\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create a deep network with poor initialization\n",
    "def create_deep_network_bad():\n",
    "    \"\"\"Deep network with sigmoid and poor initialization\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Flatten(input_shape=[28, 28]),\n",
    "        layers.Dense(100, activation='sigmoid', \n",
    "                    kernel_initializer=keras.initializers.RandomNormal(stddev=1.0)),\n",
    "        layers.Dense(100, activation='sigmoid',\n",
    "                    kernel_initializer=keras.initializers.RandomNormal(stddev=1.0)),\n",
    "        layers.Dense(100, activation='sigmoid',\n",
    "                    kernel_initializer=keras.initializers.RandomNormal(stddev=1.0)),\n",
    "        layers.Dense(100, activation='sigmoid',\n",
    "                    kernel_initializer=keras.initializers.RandomNormal(stddev=1.0)),\n",
    "        layers.Dense(100, activation='sigmoid',\n",
    "                    kernel_initializer=keras.initializers.RandomNormal(stddev=1.0)),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Load Fashion MNIST\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "print(f\"\\nüìä Dataset Loaded:\")\n",
    "print(f\"   ‚Ä¢ Training samples: {X_train.shape[0]:,}\")\n",
    "print(f\"   ‚Ä¢ Test samples: {X_test.shape[0]:,}\")\n",
    "print(f\"   ‚Ä¢ Image shape: {X_train.shape[1:]}\") \n",
    "print(f\"   ‚Ä¢ Classes: {len(np.unique(y_train))}\")\n",
    "\n",
    "# Build and analyze the problematic model\n",
    "model_bad = create_deep_network_bad()\n",
    "print(f\"\\nüèóÔ∏è  Model Architecture (Problematic):\")\n",
    "print(f\"   ‚Ä¢ Total layers: {len(model_bad.layers)}\")\n",
    "print(f\"   ‚Ä¢ Hidden layers: {len(model_bad.layers) - 2}\")\n",
    "print(f\"   ‚Ä¢ Activation: sigmoid (saturating)\")\n",
    "print(f\"   ‚Ä¢ Initialization: RandomNormal(stddev=1.0) (poor)\")\n",
    "\n",
    "# Check weight distributions\n",
    "print(f\"\\nüìä Initial Weight Statistics:\")\n",
    "for i, layer in enumerate(model_bad.layers[1:6]):  # First 5 dense layers\n",
    "    weights = layer.get_weights()[0]\n",
    "    print(f\"   Layer {i+1}: mean={weights.mean():.4f}, std={weights.std():.4f}, \"\n",
    "          f\"min={weights.min():.4f}, max={weights.max():.4f}\")\n",
    "\n",
    "# Simulate forward pass to see activation distributions\n",
    "sample_batch = X_train[:1000]\n",
    "\n",
    "# Get intermediate activations\n",
    "print(f\"\\nüîç Activation Analysis (Forward Pass):\")\n",
    "activation_model = keras.Model(\n",
    "    inputs=model_bad.input,\n",
    "    outputs=[layer.output for layer in model_bad.layers[1:6]]\n",
    ")\n",
    "activations = activation_model.predict(sample_batch, verbose=0)\n",
    "\n",
    "for i, act in enumerate(activations):\n",
    "    mean_act = act.mean()\n",
    "    std_act = act.std()\n",
    "    pct_saturated = ((act < 0.01) | (act > 0.99)).mean() * 100\n",
    "    print(f\"   Layer {i+1}: mean={mean_act:.4f}, std={std_act:.4f}, \"\n",
    "          f\"saturated={pct_saturated:.1f}%\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  PROBLEM DETECTED:\")\n",
    "print(f\"   ‚Ä¢ Activations quickly saturate (approach 0 or 1)\")\n",
    "print(f\"   ‚Ä¢ Gradients in saturated regions ‚âà 0\")\n",
    "print(f\"   ‚Ä¢ Lower layers will barely learn\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT 2: Comparing Initialization Methods\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üî¨ EXPERIMENT 2: Initialization Comparison\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def create_model_with_init(initializer_name, activation='relu'):\n",
    "    \"\"\"Create model with specified initialization\"\"\"\n",
    "    if initializer_name == 'glorot':\n",
    "        init = 'glorot_uniform'\n",
    "    elif initializer_name == 'he':\n",
    "        init = 'he_normal'\n",
    "    elif initializer_name == 'lecun':\n",
    "        init = 'lecun_normal'\n",
    "    else:  # poor\n",
    "        init = keras.initializers.RandomNormal(stddev=1.0)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        layers.Flatten(input_shape=[28, 28]),\n",
    "        layers.Dense(100, activation=activation, kernel_initializer=init),\n",
    "        layers.Dense(100, activation=activation, kernel_initializer=init),\n",
    "        layers.Dense(100, activation=activation, kernel_initializer=init),\n",
    "        layers.Dense(100, activation=activation, kernel_initializer=init),\n",
    "        layers.Dense(100, activation=activation, kernel_initializer=init),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Test different initializations\n",
    "init_methods = {\n",
    "    'Poor (N(0,1))': 'poor',\n",
    "    'Glorot': 'glorot',\n",
    "    'He': 'he',\n",
    "    'LeCun': 'lecun'\n",
    "}\n",
    "\n",
    "print(\"\\nüìä Weight Distribution Analysis:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "results = []\n",
    "for name, init_type in init_methods.items():\n",
    "    model = create_model_with_init(init_type, activation='relu')\n",
    "    \n",
    "    # Analyze first hidden layer\n",
    "    first_layer = model.layers[1]\n",
    "    weights = first_layer.get_weights()[0]\n",
    "    \n",
    "    stats = {\n",
    "        'Method': name,\n",
    "        'Mean': weights.mean(),\n",
    "        'Std': weights.std(),\n",
    "        'Min': weights.min(),\n",
    "        'Max': weights.max(),\n",
    "        'Var': weights.var()\n",
    "    }\n",
    "    results.append(stats)\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"   Shape: {weights.shape}\")\n",
    "    print(f\"   Mean: {stats['Mean']:8.4f}\")\n",
    "    print(f\"   Std:  {stats['Std']:8.4f}\")\n",
    "    print(f\"   Var:  {stats['Var']:8.4f}\")\n",
    "    print(f\"   Range: [{stats['Min']:7.4f}, {stats['Max']:7.4f}]\")\n",
    "    \n",
    "    # Theoretical variance\n",
    "    fan_in = weights.shape[0]\n",
    "    fan_out = weights.shape[1]\n",
    "    if init_type == 'glorot':\n",
    "        theoretical_var = 2 / (fan_in + fan_out)\n",
    "        print(f\"   Theoretical Var (Glorot): {theoretical_var:.6f}\")\n",
    "    elif init_type == 'he':\n",
    "        theoretical_var = 2 / fan_in\n",
    "        print(f\"   Theoretical Var (He): {theoretical_var:.6f}\")\n",
    "    elif init_type == 'lecun':\n",
    "        theoretical_var = 1 / fan_in\n",
    "        print(f\"   Theoretical Var (LeCun): {theoretical_var:.6f}\")\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìã INITIALIZATION COMPARISON SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results.to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT 3: Impact on Training\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üî¨ EXPERIMENT 3: Training with Different Initializations\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚è≥ Training models (this will take a moment)...\")\n",
    "\n",
    "# Prepare data\n",
    "X_train_subset = X_train[:10000]  # Use subset for faster demo\n",
    "y_train_subset = y_train[:10000]\n",
    "X_val = X_test[:2000]\n",
    "y_val = y_test[:2000]\n",
    "\n",
    "training_results = {}\n",
    "\n",
    "for name, init_type in [('Poor', 'poor'), ('He', 'he')]:\n",
    "    print(f\"\\nüèãÔ∏è Training with {name} initialization...\")\n",
    "    \n",
    "    model = create_model_with_init(init_type, activation='relu')\n",
    "    model.compile(\n",
    "        optimizer='sgd',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_subset, y_train_subset,\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    training_results[name] = history.history\n",
    "    \n",
    "    final_train_acc = history.history['accuracy'][-1]\n",
    "    final_val_acc = history.history['val_accuracy'][-1]\n",
    "    \n",
    "    print(f\"   ‚úì Final Training Accuracy: {final_train_acc:.4f}\")\n",
    "    print(f\"   ‚úì Final Validation Accuracy: {final_val_acc:.4f}\")\n",
    "\n",
    "# Compare convergence\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä CONVERGENCE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nEpoch-by-Epoch Validation Accuracy:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Epoch':<8} {'Poor Init':<15} {'He Init':<15} {'Improvement':<15}\")\n",
    "print(\"-\" * 70)\n",
    "for epoch in range(10):\n",
    "    poor_acc = training_results['Poor']['val_accuracy'][epoch]\n",
    "    he_acc = training_results['He']['val_accuracy'][epoch]\n",
    "    improvement = he_acc - poor_acc\n",
    "    print(f\"{epoch+1:<8} {poor_acc:<15.4f} {he_acc:<15.4f} {improvement:+.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ PART 1-3 COMPLETED!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eb5e3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CHAPTER 11: Part 4-5\n",
      "Activation Functions & Batch Normalization\n",
      "======================================================================\n",
      "\n",
      "üìä Dataset:\n",
      "   Training: 20,000 samples\n",
      "   Validation: 2,000 samples\n",
      "\n",
      "======================================================================\n",
      "üî¨ EXPERIMENT 4: Activation Functions Comparison\n",
      "======================================================================\n",
      "\n",
      "‚è≥ Training models with different activations...\n",
      "   (10 epochs each, this will take a few minutes)\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è Training with ReLU...\n",
      "======================================================================\n",
      "   ‚úì Training Time: 8.37s\n",
      "   ‚úì Final Training Accuracy: 0.8655\n",
      "   ‚úì Final Validation Accuracy: 0.8525\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è Training with Leaky ReLU...\n",
      "======================================================================\n",
      "   ‚úì Training Time: 7.73s\n",
      "   ‚úì Final Training Accuracy: 0.8615\n",
      "   ‚úì Final Validation Accuracy: 0.8410\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è Training with ELU...\n",
      "======================================================================\n",
      "   ‚úì Training Time: 7.69s\n",
      "   ‚úì Final Training Accuracy: 0.8683\n",
      "   ‚úì Final Validation Accuracy: 0.8500\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è Training with SELU...\n",
      "======================================================================\n",
      "   ‚úì Training Time: 7.70s\n",
      "   ‚úì Final Training Accuracy: 0.8939\n",
      "   ‚úì Final Validation Accuracy: 0.8520\n",
      "\n",
      "======================================================================\n",
      "üìä ACTIVATION FUNCTIONS SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Activation  Train Acc  Val Acc  Time (s)\n",
      "      ReLU    0.86555   0.8525  8.371552\n",
      "      SELU    0.89390   0.8520  7.701264\n",
      "       ELU    0.86830   0.8500  7.691867\n",
      "Leaky ReLU    0.86155   0.8410  7.731290\n",
      "\n",
      "üèÜ BEST ACTIVATION: ReLU (0.8525)\n",
      "\n",
      "======================================================================\n",
      "üî¨ EXPERIMENT 5: Batch Normalization\n",
      "======================================================================\n",
      "\n",
      "üèóÔ∏è Model 1: WITHOUT Batch Normalization\n",
      "   Architecture: Dense ‚Üí ReLU ‚Üí Dense ‚Üí ReLU ‚Üí Dense ‚Üí ReLU ‚Üí Output\n",
      "\n",
      "üèóÔ∏è Model 2: WITH Batch Normalization (before activation)\n",
      "   Architecture: Dense ‚Üí BN ‚Üí ReLU ‚Üí Dense ‚Üí BN ‚Üí ReLU ‚Üí Dense ‚Üí BN ‚Üí ReLU ‚Üí Output\n",
      "\n",
      "üèóÔ∏è Model 3: WITH Batch Normalization (after activation)\n",
      "   Architecture: Dense ‚Üí ReLU ‚Üí BN ‚Üí Dense ‚Üí ReLU ‚Üí BN ‚Üí Dense ‚Üí ReLU ‚Üí BN ‚Üí Output\n",
      "\n",
      "======================================================================\n",
      "‚è≥ Training all models (10 epochs each)...\n",
      "======================================================================\n",
      "\n",
      "üèãÔ∏è Training: No BN...\n",
      "   ‚úì Training Time: 7.58s\n",
      "   ‚úì Final Training Accuracy: 0.8643\n",
      "   ‚úì Final Validation Accuracy: 0.8440\n",
      "\n",
      "üèãÔ∏è Training: BN Before Activation...\n",
      "   ‚úì Training Time: 9.62s\n",
      "   ‚úì Final Training Accuracy: 0.8831\n",
      "   ‚úì Final Validation Accuracy: 0.8570\n",
      "\n",
      "üèãÔ∏è Training: BN After Activation...\n",
      "   ‚úì Training Time: 9.64s\n",
      "   ‚úì Final Training Accuracy: 0.8862\n",
      "   ‚úì Final Validation Accuracy: 0.8535\n",
      "\n",
      "======================================================================\n",
      "üìä BATCH NORMALIZATION COMPARISON\n",
      "======================================================================\n",
      "\n",
      "               Model  Train Acc  Val Acc  Time (s)\n",
      "BN Before Activation    0.88315   0.8570  9.622123\n",
      " BN After Activation    0.88615   0.8535  9.639333\n",
      "               No BN    0.86430   0.8440  7.580908\n",
      "\n",
      "======================================================================\n",
      "üìà CONVERGENCE SPEED COMPARISON\n",
      "======================================================================\n",
      "\n",
      "Validation Accuracy by Epoch:\n",
      "----------------------------------------------------------------------\n",
      "Epoch    No BN           BN Before       BN After       \n",
      "----------------------------------------------------------------------\n",
      "1        0.7850          0.8170          0.8095         \n",
      "2        0.8110          0.8340          0.8295         \n",
      "3        0.8145          0.8490          0.8275         \n",
      "4        0.8345          0.8595          0.8435         \n",
      "5        0.8430          0.8585          0.8500         \n",
      "6        0.8405          0.8525          0.8500         \n",
      "7        0.8530          0.8610          0.8560         \n",
      "8        0.8465          0.8560          0.8455         \n",
      "9        0.8540          0.8605          0.8560         \n",
      "10       0.8440          0.8570          0.8535         \n",
      "\n",
      "üí° IMPROVEMENT WITH BN: +0.0130 (+1.30%)\n",
      "\n",
      "======================================================================\n",
      "‚úÖ PART 4-5 COMPLETED!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CHAPTER 11: Part 4-5\n",
    "# Activation Functions & Batch Normalization\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CHAPTER 11: Part 4-5\")\n",
    "print(\"Activation Functions & Batch Normalization\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load data\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "# Prepare subsets for faster experimentation\n",
    "X_train_subset = X_train[:20000]\n",
    "y_train_subset = y_train[:20000]\n",
    "X_val = X_test[:2000]\n",
    "y_val = y_test[:2000]\n",
    "\n",
    "print(f\"\\nüìä Dataset:\")\n",
    "print(f\"   Training: {X_train_subset.shape[0]:,} samples\")\n",
    "print(f\"   Validation: {X_val.shape[0]:,} samples\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT 4: Comparing Activation Functions\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üî¨ EXPERIMENT 4: Activation Functions Comparison\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def create_model_with_activation(activation, use_lecun_init=False):\n",
    "    \"\"\"Create model with specified activation function\"\"\"\n",
    "    if use_lecun_init:\n",
    "        init = 'lecun_normal'\n",
    "    else:\n",
    "        if activation in ['relu', 'elu']:\n",
    "            init = 'he_normal'\n",
    "        else:\n",
    "            init = 'glorot_uniform'\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        layers.Flatten(input_shape=[28, 28]),\n",
    "        layers.Dense(100, activation=activation, kernel_initializer=init),\n",
    "        layers.Dense(100, activation=activation, kernel_initializer=init),\n",
    "        layers.Dense(100, activation=activation, kernel_initializer=init),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Test different activations\n",
    "activations_to_test = {\n",
    "    'ReLU': 'relu',\n",
    "    'Leaky ReLU': layers.LeakyReLU(alpha=0.01),\n",
    "    'ELU': 'elu',\n",
    "    'SELU': 'selu'\n",
    "}\n",
    "\n",
    "print(\"\\n‚è≥ Training models with different activations...\")\n",
    "print(\"   (10 epochs each, this will take a few minutes)\")\n",
    "\n",
    "activation_results = {}\n",
    "\n",
    "for name, activation in activations_to_test.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üèãÔ∏è Training with {name}...\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # For SELU, need special handling\n",
    "    if name == 'SELU':\n",
    "        # Standardize inputs for SELU\n",
    "        X_train_std = (X_train_subset - X_train_subset.mean()) / X_train_subset.std()\n",
    "        X_val_std = (X_val - X_train_subset.mean()) / X_train_subset.std()\n",
    "        \n",
    "        model = keras.Sequential([\n",
    "            layers.Flatten(input_shape=[28, 28]),\n",
    "            layers.Dense(100, activation='selu', kernel_initializer='lecun_normal'),\n",
    "            layers.Dense(100, activation='selu', kernel_initializer='lecun_normal'),\n",
    "            layers.Dense(100, activation='selu', kernel_initializer='lecun_normal'),\n",
    "            layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        train_data = (X_train_std, y_train_subset)\n",
    "        val_data = (X_val_std, y_val)\n",
    "    else:\n",
    "        if isinstance(activation, str):\n",
    "            model = create_model_with_activation(activation)\n",
    "        else:  # LeakyReLU case\n",
    "            model = keras.Sequential([\n",
    "                layers.Flatten(input_shape=[28, 28]),\n",
    "                layers.Dense(100, kernel_initializer='he_normal'),\n",
    "                activation,\n",
    "                layers.Dense(100, kernel_initializer='he_normal'),\n",
    "                activation,\n",
    "                layers.Dense(100, kernel_initializer='he_normal'),\n",
    "                activation,\n",
    "                layers.Dense(10, activation='softmax')\n",
    "            ])\n",
    "        \n",
    "        train_data = (X_train_subset, y_train_subset)\n",
    "        val_data = (X_val, y_val)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='sgd',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_data[0], train_data[1],\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        validation_data=val_data,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    activation_results[name] = {\n",
    "        'history': history.history,\n",
    "        'time': training_time,\n",
    "        'final_train_acc': history.history['accuracy'][-1],\n",
    "        'final_val_acc': history.history['val_accuracy'][-1]\n",
    "    }\n",
    "    \n",
    "    print(f\"   ‚úì Training Time: {training_time:.2f}s\")\n",
    "    print(f\"   ‚úì Final Training Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "    print(f\"   ‚úì Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä ACTIVATION FUNCTIONS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary_data = []\n",
    "for name, results in activation_results.items():\n",
    "    summary_data.append({\n",
    "        'Activation': name,\n",
    "        'Train Acc': results['final_train_acc'],\n",
    "        'Val Acc': results['final_val_acc'],\n",
    "        'Time (s)': results['time']\n",
    "    })\n",
    "\n",
    "df_activations = pd.DataFrame(summary_data)\n",
    "df_activations = df_activations.sort_values('Val Acc', ascending=False)\n",
    "print(\"\\n\" + df_activations.to_string(index=False))\n",
    "\n",
    "# Find best activation\n",
    "best_activation = df_activations.iloc[0]['Activation']\n",
    "best_val_acc = df_activations.iloc[0]['Val Acc']\n",
    "print(f\"\\nüèÜ BEST ACTIVATION: {best_activation} ({best_val_acc:.4f})\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT 5: Batch Normalization Impact\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üî¨ EXPERIMENT 5: Batch Normalization\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Model WITHOUT Batch Normalization\n",
    "print(\"\\nüèóÔ∏è Model 1: WITHOUT Batch Normalization\")\n",
    "model_no_bn = keras.Sequential([\n",
    "    layers.Flatten(input_shape=[28, 28]),\n",
    "    layers.Dense(100, activation='relu', kernel_initializer='he_normal'),\n",
    "    layers.Dense(100, activation='relu', kernel_initializer='he_normal'),\n",
    "    layers.Dense(100, activation='relu', kernel_initializer='he_normal'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_no_bn.compile(\n",
    "    optimizer='sgd',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"   Architecture: Dense ‚Üí ReLU ‚Üí Dense ‚Üí ReLU ‚Üí Dense ‚Üí ReLU ‚Üí Output\")\n",
    "\n",
    "# Model WITH Batch Normalization (before activation)\n",
    "print(\"\\nüèóÔ∏è Model 2: WITH Batch Normalization (before activation)\")\n",
    "model_with_bn = keras.Sequential([\n",
    "    layers.Flatten(input_shape=[28, 28]),\n",
    "    layers.Dense(100, kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dense(100, kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dense(100, kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_with_bn.compile(\n",
    "    optimizer='sgd',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"   Architecture: Dense ‚Üí BN ‚Üí ReLU ‚Üí Dense ‚Üí BN ‚Üí ReLU ‚Üí Dense ‚Üí BN ‚Üí ReLU ‚Üí Output\")\n",
    "\n",
    "# Model WITH Batch Normalization (after activation)\n",
    "print(\"\\nüèóÔ∏è Model 3: WITH Batch Normalization (after activation)\")\n",
    "model_bn_after = keras.Sequential([\n",
    "    layers.Flatten(input_shape=[28, 28]),\n",
    "    layers.Dense(100, activation='relu', kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(100, activation='relu', kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(100, activation='relu', kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_bn_after.compile(\n",
    "    optimizer='sgd',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"   Architecture: Dense ‚Üí ReLU ‚Üí BN ‚Üí Dense ‚Üí ReLU ‚Üí BN ‚Üí Dense ‚Üí ReLU ‚Üí BN ‚Üí Output\")\n",
    "\n",
    "# Train all models\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚è≥ Training all models (10 epochs each)...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "bn_results = {}\n",
    "\n",
    "for model_name, model in [\n",
    "    ('No BN', model_no_bn),\n",
    "    ('BN Before Activation', model_with_bn),\n",
    "    ('BN After Activation', model_bn_after)\n",
    "]:\n",
    "    print(f\"\\nüèãÔ∏è Training: {model_name}...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_subset, y_train_subset,\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    bn_results[model_name] = {\n",
    "        'history': history.history,\n",
    "        'time': training_time,\n",
    "        'final_train_acc': history.history['accuracy'][-1],\n",
    "        'final_val_acc': history.history['val_accuracy'][-1]\n",
    "    }\n",
    "    \n",
    "    print(f\"   ‚úì Training Time: {training_time:.2f}s\")\n",
    "    print(f\"   ‚úì Final Training Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "    print(f\"   ‚úì Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "# Compare results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä BATCH NORMALIZATION COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "bn_summary = []\n",
    "for name, results in bn_results.items():\n",
    "    bn_summary.append({\n",
    "        'Model': name,\n",
    "        'Train Acc': results['final_train_acc'],\n",
    "        'Val Acc': results['final_val_acc'],\n",
    "        'Time (s)': results['time']\n",
    "    })\n",
    "\n",
    "df_bn = pd.DataFrame(bn_summary)\n",
    "df_bn = df_bn.sort_values('Val Acc', ascending=False)\n",
    "print(\"\\n\" + df_bn.to_string(index=False))\n",
    "\n",
    "# Epoch-by-epoch comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìà CONVERGENCE SPEED COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nValidation Accuracy by Epoch:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Epoch':<8} {'No BN':<15} {'BN Before':<15} {'BN After':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for epoch in range(10):\n",
    "    no_bn_acc = bn_results['No BN']['history']['val_accuracy'][epoch]\n",
    "    bn_before_acc = bn_results['BN Before Activation']['history']['val_accuracy'][epoch]\n",
    "    bn_after_acc = bn_results['BN After Activation']['history']['val_accuracy'][epoch]\n",
    "    \n",
    "    print(f\"{epoch+1:<8} {no_bn_acc:<15.4f} {bn_before_acc:<15.4f} {bn_after_acc:<15.4f}\")\n",
    "\n",
    "# Find improvement\n",
    "no_bn_final = bn_results['No BN']['final_val_acc']\n",
    "best_bn_final = max(bn_results['BN Before Activation']['final_val_acc'], \n",
    "                    bn_results['BN After Activation']['final_val_acc'])\n",
    "improvement = best_bn_final - no_bn_final\n",
    "\n",
    "print(f\"\\nüí° IMPROVEMENT WITH BN: {improvement:+.4f} ({improvement*100:+.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ PART 4-5 COMPLETED!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aea487cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CHAPTER 11: Part 6-7\n",
      "Transfer Learning & Gradient Clipping\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üî¨ EXPERIMENT 6: Gradient Clipping\n",
      "======================================================================\n",
      "\n",
      "üìä Dataset: 20,000 training samples\n",
      "\n",
      "üèóÔ∏è Testing Gradient Clipping Strategies:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üèãÔ∏è Training with: No Clipping\n",
      "   ‚úì Time: 12.94s\n",
      "   ‚úì Final Val Acc: 0.8620\n",
      "\n",
      "üèãÔ∏è Training with: Clip by Value (1.0)\n",
      "   ‚úì Time: 13.70s\n",
      "   ‚úì Final Val Acc: 0.8680\n",
      "\n",
      "üèãÔ∏è Training with: Clip by Norm (1.0)\n",
      "   ‚úì Time: 14.10s\n",
      "   ‚úì Final Val Acc: 0.8510\n",
      "\n",
      "======================================================================\n",
      "üìä GRADIENT CLIPPING COMPARISON\n",
      "======================================================================\n",
      "\n",
      "           Strategy  Val Acc  Time (s)\n",
      "Clip by Value (1.0)    0.868 13.700902\n",
      "        No Clipping    0.862 12.940013\n",
      " Clip by Norm (1.0)    0.851 14.095426\n",
      "\n",
      "======================================================================\n",
      "üî¨ EXPERIMENT 7: Transfer Learning\n",
      "======================================================================\n",
      "\n",
      "üìö SCENARIO:\n",
      "   Task A: Train on Fashion MNIST classes 0-4 (5 classes)\n",
      "   Task B: Fine-tune for classes 5-9 (5 different classes)\n",
      "   Simulate: Pre-training ‚Üí Transfer ‚Üí Fine-tuning\n",
      "\n",
      "üìä Task A Dataset (classes 0-4):\n",
      "   Training: 15,000 samples\n",
      "   Test: 1,000 samples\n",
      "\n",
      "üìä Task B Dataset (classes 5-9, LIMITED DATA):\n",
      "   Training: 5,000 samples (only 1/3 of Task A!)\n",
      "   Test: 1,000 samples\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "STEP 1: Pre-training on Task A\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üèãÔ∏è Pre-training on Task A (15 epochs)...\n",
      "   ‚úì Task A Validation Accuracy: 0.8920\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "STEP 2: Transfer Learning to Task B\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üèóÔ∏è BASELINE: Training from scratch on Task B (limited data)\n",
      "   ‚úì From Scratch Val Acc: 0.9510\n",
      "\n",
      "üèóÔ∏è TRANSFER: Reuse pre-trained layers, freeze them\n",
      "   Frozen layers: hidden1, hidden2, hidden3\n",
      "   Trainable: output_B only\n",
      "   ‚úì Transfer (Frozen) Val Acc: 0.8120\n",
      "\n",
      "üèóÔ∏è FINE-TUNE: Reuse pre-trained layers, train all\n",
      "   All layers trainable\n",
      "   Lower learning rate (0.001) for fine-tuning\n",
      "   ‚úì Transfer (Fine-tune) Val Acc: 0.8820\n",
      "\n",
      "======================================================================\n",
      "üìä TRANSFER LEARNING COMPARISON\n",
      "======================================================================\n",
      "\n",
      "               Strategy  Val Acc          Description\n",
      "From Scratch (baseline)    0.951      No pre-training\n",
      "   Transfer (Fine-tune)    0.882 Fine-tune all layers\n",
      "      Transfer (Frozen)    0.812  Freeze lower layers\n",
      "\n",
      "üí° TRANSFER LEARNING GAINS:\n",
      "   Frozen layers: -0.1390 (-13.90%)\n",
      "   Fine-tuning:   -0.0690 (-6.90%)\n",
      "\n",
      "======================================================================\n",
      "üìà CONVERGENCE COMPARISON (First 10 Epochs)\n",
      "======================================================================\n",
      "Epoch    Scratch         Frozen          Fine-tune      \n",
      "----------------------------------------------------------------------\n",
      "1        0.8510          0.6530          0.5990         \n",
      "2        0.8620          0.6880          0.6830         \n",
      "3        0.8980          0.7060          0.7250         \n",
      "4        0.9220          0.7140          0.7490         \n",
      "5        0.9260          0.7400          0.7750         \n",
      "6        0.9270          0.7490          0.8000         \n",
      "7        0.9330          0.7580          0.8250         \n",
      "8        0.9220          0.7730          0.8390         \n",
      "9        0.9430          0.7720          0.8590         \n",
      "10       0.9400          0.7760          0.8600         \n",
      "\n",
      "======================================================================\n",
      "‚úÖ PART 6-7 COMPLETED!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CHAPTER 11: Part 6-7\n",
    "# Transfer Learning & Gradient Clipping\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CHAPTER 11: Part 6-7\")\n",
    "print(\"Transfer Learning & Gradient Clipping\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT 6: Gradient Clipping\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üî¨ EXPERIMENT 6: Gradient Clipping\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load data\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "X_train_subset = X_train[:20000]\n",
    "y_train_subset = y_train[:20000]\n",
    "X_val = X_test[:2000]\n",
    "y_val = y_test[:2000]\n",
    "\n",
    "print(f\"\\nüìä Dataset: {X_train_subset.shape[0]:,} training samples\")\n",
    "\n",
    "# Create a model that might have exploding gradients\n",
    "def create_deep_model():\n",
    "    \"\"\"Very deep model with potential gradient issues\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Flatten(input_shape=[28, 28]),\n",
    "        layers.Dense(200, activation='relu', kernel_initializer='he_normal'),\n",
    "        layers.Dense(200, activation='relu', kernel_initializer='he_normal'),\n",
    "        layers.Dense(200, activation='relu', kernel_initializer='he_normal'),\n",
    "        layers.Dense(200, activation='relu', kernel_initializer='he_normal'),\n",
    "        layers.Dense(200, activation='relu', kernel_initializer='he_normal'),\n",
    "        layers.Dense(200, activation='relu', kernel_initializer='he_normal'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Test different gradient clipping strategies\n",
    "print(\"\\nüèóÔ∏è Testing Gradient Clipping Strategies:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "clipping_configs = {\n",
    "    'No Clipping': None,\n",
    "    'Clip by Value (1.0)': keras.optimizers.SGD(learning_rate=0.01, clipvalue=1.0),\n",
    "    'Clip by Norm (1.0)': keras.optimizers.SGD(learning_rate=0.01, clipnorm=1.0),\n",
    "}\n",
    "\n",
    "clipping_results = {}\n",
    "\n",
    "for config_name, optimizer in clipping_configs.items():\n",
    "    print(f\"\\nüèãÔ∏è Training with: {config_name}\")\n",
    "    \n",
    "    model = create_deep_model()\n",
    "    \n",
    "    if optimizer is None:\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=0.01)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_subset, y_train_subset,\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    clipping_results[config_name] = {\n",
    "        'history': history.history,\n",
    "        'time': training_time,\n",
    "        'final_val_acc': history.history['val_accuracy'][-1]\n",
    "    }\n",
    "    \n",
    "    print(f\"   ‚úì Time: {training_time:.2f}s\")\n",
    "    print(f\"   ‚úì Final Val Acc: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä GRADIENT CLIPPING COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "clip_summary = []\n",
    "for name, results in clipping_results.items():\n",
    "    clip_summary.append({\n",
    "        'Strategy': name,\n",
    "        'Val Acc': results['final_val_acc'],\n",
    "        'Time (s)': results['time']\n",
    "    })\n",
    "\n",
    "df_clip = pd.DataFrame(clip_summary)\n",
    "df_clip = df_clip.sort_values('Val Acc', ascending=False)\n",
    "print(\"\\n\" + df_clip.to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT 7: Transfer Learning Simulation\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üî¨ EXPERIMENT 7: Transfer Learning\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìö SCENARIO:\")\n",
    "print(\"   Task A: Train on Fashion MNIST classes 0-4 (5 classes)\")\n",
    "print(\"   Task B: Fine-tune for classes 5-9 (5 different classes)\")\n",
    "print(\"   Simulate: Pre-training ‚Üí Transfer ‚Üí Fine-tuning\")\n",
    "\n",
    "# Prepare datasets for Task A (classes 0-4)\n",
    "mask_train_A = y_train < 5\n",
    "X_train_A = X_train[mask_train_A][:15000]\n",
    "y_train_A = y_train[mask_train_A][:15000]\n",
    "\n",
    "mask_test_A = y_test < 5\n",
    "X_test_A = X_test[mask_test_A][:1000]\n",
    "y_test_A = y_test[mask_test_A][:1000]\n",
    "\n",
    "print(f\"\\nüìä Task A Dataset (classes 0-4):\")\n",
    "print(f\"   Training: {X_train_A.shape[0]:,} samples\")\n",
    "print(f\"   Test: {X_test_A.shape[0]:,} samples\")\n",
    "\n",
    "# Prepare datasets for Task B (classes 5-9)\n",
    "mask_train_B = y_train >= 5\n",
    "X_train_B = X_train[mask_train_B][:5000]  # Less data!\n",
    "y_train_B = y_train[mask_train_B][:5000] - 5  # Remap to 0-4\n",
    "\n",
    "mask_test_B = y_test >= 5\n",
    "X_test_B = X_test[mask_test_B][:1000]\n",
    "y_test_B = y_test[mask_test_B][:1000] - 5  # Remap to 0-4\n",
    "\n",
    "print(f\"\\nüìä Task B Dataset (classes 5-9, LIMITED DATA):\")\n",
    "print(f\"   Training: {X_train_B.shape[0]:,} samples (only 1/3 of Task A!)\")\n",
    "print(f\"   Test: {X_test_B.shape[0]:,} samples\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Pre-train on Task A\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"STEP 1: Pre-training on Task A\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "model_pretrain = keras.Sequential([\n",
    "    layers.Flatten(input_shape=[28, 28]),\n",
    "    layers.Dense(100, activation='relu', kernel_initializer='he_normal', name='hidden1'),\n",
    "    layers.Dense(100, activation='relu', kernel_initializer='he_normal', name='hidden2'),\n",
    "    layers.Dense(100, activation='relu', kernel_initializer='he_normal', name='hidden3'),\n",
    "    layers.Dense(5, activation='softmax', name='output_A')  # 5 classes\n",
    "])\n",
    "\n",
    "model_pretrain.compile(\n",
    "    optimizer='sgd',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\nüèãÔ∏è Pre-training on Task A (15 epochs)...\")\n",
    "history_pretrain = model_pretrain.fit(\n",
    "    X_train_A, y_train_A,\n",
    "    epochs=15,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_A, y_test_A),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "pretrain_acc = history_pretrain.history['val_accuracy'][-1]\n",
    "print(f\"   ‚úì Task A Validation Accuracy: {pretrain_acc:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2: Transfer Learning - Reuse Lower Layers\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"STEP 2: Transfer Learning to Task B\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Strategy 1: Train from scratch (baseline)\n",
    "print(\"\\nüèóÔ∏è BASELINE: Training from scratch on Task B (limited data)\")\n",
    "model_scratch = keras.Sequential([\n",
    "    layers.Flatten(input_shape=[28, 28]),\n",
    "    layers.Dense(100, activation='relu', kernel_initializer='he_normal'),\n",
    "    layers.Dense(100, activation='relu', kernel_initializer='he_normal'),\n",
    "    layers.Dense(100, activation='relu', kernel_initializer='he_normal'),\n",
    "    layers.Dense(5, activation='softmax')  # 5 classes\n",
    "])\n",
    "\n",
    "model_scratch.compile(\n",
    "    optimizer='sgd',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_scratch = model_scratch.fit(\n",
    "    X_train_B, y_train_B,\n",
    "    epochs=15,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_B, y_test_B),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "scratch_acc = history_scratch.history['val_accuracy'][-1]\n",
    "print(f\"   ‚úì From Scratch Val Acc: {scratch_acc:.4f}\")\n",
    "\n",
    "# Strategy 2: Transfer Learning - Freeze lower layers\n",
    "print(\"\\nüèóÔ∏è TRANSFER: Reuse pre-trained layers, freeze them\")\n",
    "\n",
    "# Create new model reusing lower layers\n",
    "model_transfer_frozen = keras.Sequential([\n",
    "    layers.Flatten(input_shape=[28, 28]),\n",
    "    model_pretrain.get_layer('hidden1'),\n",
    "    model_pretrain.get_layer('hidden2'),\n",
    "    model_pretrain.get_layer('hidden3'),\n",
    "    layers.Dense(5, activation='softmax', name='output_B')  # New output layer\n",
    "])\n",
    "\n",
    "# Freeze the transferred layers\n",
    "for layer in model_transfer_frozen.layers[1:4]:  # hidden1, hidden2, hidden3\n",
    "    layer.trainable = False\n",
    "\n",
    "model_transfer_frozen.compile(\n",
    "    optimizer='sgd',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"   Frozen layers: hidden1, hidden2, hidden3\")\n",
    "print(\"   Trainable: output_B only\")\n",
    "\n",
    "history_frozen = model_transfer_frozen.fit(\n",
    "    X_train_B, y_train_B,\n",
    "    epochs=15,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_B, y_test_B),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "frozen_acc = history_frozen.history['val_accuracy'][-1]\n",
    "print(f\"   ‚úì Transfer (Frozen) Val Acc: {frozen_acc:.4f}\")\n",
    "\n",
    "# Strategy 3: Transfer Learning - Fine-tune all layers\n",
    "print(\"\\nüèóÔ∏è FINE-TUNE: Reuse pre-trained layers, train all\")\n",
    "\n",
    "model_transfer_finetune = keras.Sequential([\n",
    "    layers.Flatten(input_shape=[28, 28]),\n",
    "    model_pretrain.get_layer('hidden1'),\n",
    "    model_pretrain.get_layer('hidden2'),\n",
    "    model_pretrain.get_layer('hidden3'),\n",
    "    layers.Dense(5, activation='softmax', name='output_B2')\n",
    "])\n",
    "\n",
    "# All layers trainable\n",
    "for layer in model_transfer_finetune.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "model_transfer_finetune.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.001),  # Lower LR for fine-tuning\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"   All layers trainable\")\n",
    "print(\"   Lower learning rate (0.001) for fine-tuning\")\n",
    "\n",
    "history_finetune = model_transfer_finetune.fit(\n",
    "    X_train_B, y_train_B,\n",
    "    epochs=15,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_B, y_test_B),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "finetune_acc = history_finetune.history['val_accuracy'][-1]\n",
    "print(f\"   ‚úì Transfer (Fine-tune) Val Acc: {finetune_acc:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Compare Transfer Learning Strategies\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä TRANSFER LEARNING COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "transfer_summary = pd.DataFrame([\n",
    "    {'Strategy': 'From Scratch (baseline)', 'Val Acc': scratch_acc, 'Description': 'No pre-training'},\n",
    "    {'Strategy': 'Transfer (Frozen)', 'Val Acc': frozen_acc, 'Description': 'Freeze lower layers'},\n",
    "    {'Strategy': 'Transfer (Fine-tune)', 'Val Acc': finetune_acc, 'Description': 'Fine-tune all layers'}\n",
    "])\n",
    "\n",
    "transfer_summary = transfer_summary.sort_values('Val Acc', ascending=False)\n",
    "print(\"\\n\" + transfer_summary.to_string(index=False))\n",
    "\n",
    "# Calculate improvements\n",
    "improvement_frozen = frozen_acc - scratch_acc\n",
    "improvement_finetune = finetune_acc - scratch_acc\n",
    "\n",
    "print(f\"\\nüí° TRANSFER LEARNING GAINS:\")\n",
    "print(f\"   Frozen layers: {improvement_frozen:+.4f} ({improvement_frozen*100:+.2f}%)\")\n",
    "print(f\"   Fine-tuning:   {improvement_finetune:+.4f} ({improvement_finetune*100:+.2f}%)\")\n",
    "\n",
    "# Convergence comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìà CONVERGENCE COMPARISON (First 10 Epochs)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Epoch':<8} {'Scratch':<15} {'Frozen':<15} {'Fine-tune':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for epoch in range(10):\n",
    "    scratch = history_scratch.history['val_accuracy'][epoch]\n",
    "    frozen = history_frozen.history['val_accuracy'][epoch]\n",
    "    finetune = history_finetune.history['val_accuracy'][epoch]\n",
    "    print(f\"{epoch+1:<8} {scratch:<15.4f} {frozen:<15.4f} {finetune:<15.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ PART 6-7 COMPLETED!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b009048a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CHAPTER 11: Part 8-9\n",
      "Advanced Optimizers & Learning Rate Scheduling\n",
      "======================================================================\n",
      "\n",
      "üìä Dataset: 20,000 training samples\n",
      "\n",
      "======================================================================\n",
      "üî¨ EXPERIMENT 8: Optimizer Comparison\n",
      "======================================================================\n",
      "\n",
      "‚è≥ Training with different optimizers (15 epochs each)...\n",
      "   This will take several minutes...\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è Training with SGD (vanilla)...\n",
      "======================================================================\n",
      "   ‚úì Time: 12.28s\n",
      "   ‚úì Final Train Acc: 0.8829\n",
      "   ‚úì Final Val Acc: 0.8525\n",
      "   ‚úì Best Val Acc: 0.8665\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è Training with SGD + Momentum...\n",
      "======================================================================\n",
      "   ‚úì Time: 11.51s\n",
      "   ‚úì Final Train Acc: 0.9061\n",
      "   ‚úì Final Val Acc: 0.8560\n",
      "   ‚úì Best Val Acc: 0.8785\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è Training with SGD + Nesterov...\n",
      "======================================================================\n",
      "   ‚úì Time: 11.76s\n",
      "   ‚úì Final Train Acc: 0.9115\n",
      "   ‚úì Final Val Acc: 0.8595\n",
      "   ‚úì Best Val Acc: 0.8745\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è Training with Adagrad...\n",
      "======================================================================\n",
      "   ‚úì Time: 14.21s\n",
      "   ‚úì Final Train Acc: 0.8956\n",
      "   ‚úì Final Val Acc: 0.8575\n",
      "   ‚úì Best Val Acc: 0.8715\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è Training with RMSprop...\n",
      "======================================================================\n",
      "   ‚úì Time: 14.00s\n",
      "   ‚úì Final Train Acc: 0.9028\n",
      "   ‚úì Final Val Acc: 0.8555\n",
      "   ‚úì Best Val Acc: 0.8685\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è Training with Adam...\n",
      "======================================================================\n",
      "   ‚úì Time: 14.08s\n",
      "   ‚úì Final Train Acc: 0.9171\n",
      "   ‚úì Final Val Acc: 0.8465\n",
      "   ‚úì Best Val Acc: 0.8765\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è Training with Nadam...\n",
      "======================================================================\n",
      "   ‚úì Time: 15.60s\n",
      "   ‚úì Final Train Acc: 0.9219\n",
      "   ‚úì Final Val Acc: 0.8605\n",
      "   ‚úì Best Val Acc: 0.8760\n",
      "\n",
      "======================================================================\n",
      "üìä OPTIMIZER COMPARISON SUMMARY\n",
      "======================================================================\n",
      "\n",
      "     Optimizer  Final Val Acc  Best Val Acc  Time (s)\n",
      "SGD + Momentum         0.8560        0.8785 11.510079\n",
      "          Adam         0.8465        0.8765 14.078319\n",
      "         Nadam         0.8605        0.8760 15.599514\n",
      "SGD + Nesterov         0.8595        0.8745 11.761677\n",
      "       Adagrad         0.8575        0.8715 14.205158\n",
      "       RMSprop         0.8555        0.8685 13.996204\n",
      " SGD (vanilla)         0.8525        0.8665 12.282748\n",
      "\n",
      "üèÜ BEST OPTIMIZER: SGD + Momentum (0.8785)\n",
      "\n",
      "======================================================================\n",
      "üìà CONVERGENCE SPEED (Validation Accuracy by Epoch)\n",
      "======================================================================\n",
      "Epoch   SGD        Momentum   RMSprop    Adam       Nadam     \n",
      "----------------------------------------------------------------------\n",
      "1       0.7845     0.8040     0.8265     0.8265     0.8260    \n",
      "2       0.8035     0.8450     0.8210     0.8435     0.8575    \n",
      "3       0.8260     0.8460     0.8440     0.8305     0.8510    \n",
      "4       0.8425     0.8505     0.8505     0.8695     0.8705    \n",
      "5       0.8415     0.8475     0.8450     0.8620     0.8575    \n",
      "6       0.8465     0.8555     0.8555     0.8525     0.8705    \n",
      "7       0.8580     0.8560     0.8535     0.8620     0.8590    \n",
      "8       0.8485     0.8435     0.8605     0.8615     0.8475    \n",
      "9       0.8600     0.8615     0.8555     0.8550     0.8595    \n",
      "10      0.8560     0.8505     0.8605     0.8635     0.8585    \n",
      "11      0.8645     0.8670     0.8550     0.8765     0.8670    \n",
      "12      0.8640     0.8455     0.8625     0.8745     0.8665    \n",
      "13      0.8560     0.8785     0.8685     0.8710     0.8760    \n",
      "14      0.8665     0.8640     0.8615     0.8605     0.8640    \n",
      "15      0.8525     0.8560     0.8555     0.8465     0.8605    \n",
      "\n",
      "======================================================================\n",
      "üî¨ EXPERIMENT 9: Learning Rate Scheduling\n",
      "======================================================================\n",
      "\n",
      "üìä Extended Dataset: 30,000 training samples\n",
      "   Training for 30 epochs to see scheduling effects\n",
      "\n",
      "üèóÔ∏è BASELINE: Constant Learning Rate\n",
      "   ‚úì Best Val Acc: 0.8900\n",
      "\n",
      "üèóÔ∏è EXPONENTIAL DECAY SCHEDULING\n",
      "   ‚úì Best Val Acc: 0.8890\n",
      "\n",
      "üèóÔ∏è PIECEWISE CONSTANT SCHEDULING\n",
      "   ‚úì Best Val Acc: 0.8900\n",
      "\n",
      "üèóÔ∏è REDUCE LR ON PLATEAU\n",
      "   ‚úì Best Val Acc: 0.8925\n",
      "\n",
      "======================================================================\n",
      "üìä LEARNING RATE SCHEDULING COMPARISON\n",
      "======================================================================\n",
      "\n",
      "          Strategy  Final Val Acc  Best Val Acc\n",
      " ReduceLROnPlateau         0.8895        0.8925\n",
      "       Constant LR         0.8820        0.8900\n",
      "Piecewise Constant         0.8825        0.8900\n",
      " Exponential Decay         0.8775        0.8890\n",
      "\n",
      "======================================================================\n",
      "üìà VALIDATION ACCURACY OVER TIME (First 20 Epochs)\n",
      "======================================================================\n",
      "Epoch   Constant     Exponential  Piecewise    Plateau     \n",
      "----------------------------------------------------------------------\n",
      "1       0.8400       0.8425       0.8320       0.8535      \n",
      "2       0.8535       0.8420       0.8530       0.8515      \n",
      "3       0.8475       0.8445       0.8530       0.8570      \n",
      "4       0.8295       0.8400       0.8520       0.8465      \n",
      "5       0.8555       0.8625       0.8625       0.8605      \n",
      "6       0.8670       0.8670       0.8590       0.8615      \n",
      "7       0.8640       0.8610       0.8645       0.8435      \n",
      "8       0.8665       0.8695       0.8695       0.8620      \n",
      "9       0.8700       0.8705       0.8615       0.8585      \n",
      "10      0.8690       0.8725       0.8755       0.8700      \n",
      "11      0.8720       0.8690       0.8800       0.8740      \n",
      "12      0.8660       0.8740       0.8750       0.8755      \n",
      "13      0.8755       0.8725       0.8705       0.8755      \n",
      "14      0.8610       0.8680       0.8785       0.8530      \n",
      "15      0.8730       0.8795       0.8745       0.8745      \n",
      "16      0.8700       0.8760       0.8815       0.8705      \n",
      "17      0.8655       0.8725       0.8695       0.8795      \n",
      "18      0.8805       0.8825       0.8825       0.8830      \n",
      "19      0.8770       0.8845       0.8805       0.8735      \n",
      "20      0.8845       0.8800       0.8830       0.8835      \n",
      "\n",
      "======================================================================\n",
      "‚úÖ PART 8-9 COMPLETED!\n",
      "======================================================================\n",
      "\n",
      "üí° KEY INSIGHTS:\n",
      "   ‚Ä¢ Modern optimizers (Adam, RMSprop) converge MUCH faster than SGD\n",
      "   ‚Ä¢ Momentum significantly improves SGD performance\n",
      "   ‚Ä¢ Learning rate scheduling helps achieve better final accuracy\n",
      "   ‚Ä¢ ReduceLROnPlateau is adaptive and often best for real-world tasks\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CHAPTER 11: Part 8-9\n",
    "# Advanced Optimizers & Learning Rate Scheduling\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CHAPTER 11: Part 8-9\")\n",
    "print(\"Advanced Optimizers & Learning Rate Scheduling\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load data\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "X_train_subset = X_train[:20000]\n",
    "y_train_subset = y_train[:20000]\n",
    "X_val = X_test[:2000]\n",
    "y_val = y_test[:2000]\n",
    "\n",
    "print(f\"\\nüìä Dataset: {X_train_subset.shape[0]:,} training samples\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT 8: Comparing Optimizers\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üî¨ EXPERIMENT 8: Optimizer Comparison\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def create_standard_model():\n",
    "    \"\"\"Standard model for optimizer comparison\"\"\"\n",
    "    return keras.Sequential([\n",
    "        layers.Flatten(input_shape=[28, 28]),\n",
    "        layers.Dense(100, activation='relu', kernel_initializer='he_normal'),\n",
    "        layers.Dense(100, activation='relu', kernel_initializer='he_normal'),\n",
    "        layers.Dense(100, activation='relu', kernel_initializer='he_normal'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "# Test different optimizers\n",
    "optimizers_config = {\n",
    "    'SGD (vanilla)': keras.optimizers.SGD(learning_rate=0.01),\n",
    "    'SGD + Momentum': keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
    "    'SGD + Nesterov': keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True),\n",
    "    'Adagrad': keras.optimizers.Adagrad(learning_rate=0.01),\n",
    "    'RMSprop': keras.optimizers.RMSprop(learning_rate=0.001),\n",
    "    'Adam': keras.optimizers.Adam(learning_rate=0.001),\n",
    "    'Nadam': keras.optimizers.Nadam(learning_rate=0.001),\n",
    "}\n",
    "\n",
    "print(\"\\n‚è≥ Training with different optimizers (15 epochs each)...\")\n",
    "print(\"   This will take several minutes...\")\n",
    "\n",
    "optimizer_results = {}\n",
    "\n",
    "for opt_name, optimizer in optimizers_config.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üèãÔ∏è Training with {opt_name}...\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    model = create_standard_model()\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_subset, y_train_subset,\n",
    "        epochs=15,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    optimizer_results[opt_name] = {\n",
    "        'history': history.history,\n",
    "        'time': training_time,\n",
    "        'final_train_acc': history.history['accuracy'][-1],\n",
    "        'final_val_acc': history.history['val_accuracy'][-1],\n",
    "        'best_val_acc': max(history.history['val_accuracy'])\n",
    "    }\n",
    "    \n",
    "    print(f\"   ‚úì Time: {training_time:.2f}s\")\n",
    "    print(f\"   ‚úì Final Train Acc: {history.history['accuracy'][-1]:.4f}\")\n",
    "    print(f\"   ‚úì Final Val Acc: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "    print(f\"   ‚úì Best Val Acc: {max(history.history['val_accuracy']):.4f}\")\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä OPTIMIZER COMPARISON SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "opt_summary = []\n",
    "for name, results in optimizer_results.items():\n",
    "    opt_summary.append({\n",
    "        'Optimizer': name,\n",
    "        'Final Val Acc': results['final_val_acc'],\n",
    "        'Best Val Acc': results['best_val_acc'],\n",
    "        'Time (s)': results['time']\n",
    "    })\n",
    "\n",
    "df_opt = pd.DataFrame(opt_summary)\n",
    "df_opt = df_opt.sort_values('Best Val Acc', ascending=False)\n",
    "print(\"\\n\" + df_opt.to_string(index=False))\n",
    "\n",
    "# Find best optimizer\n",
    "best_opt = df_opt.iloc[0]['Optimizer']\n",
    "best_acc = df_opt.iloc[0]['Best Val Acc']\n",
    "print(f\"\\nüèÜ BEST OPTIMIZER: {best_opt} ({best_acc:.4f})\")\n",
    "\n",
    "# Detailed convergence analysis\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìà CONVERGENCE SPEED (Validation Accuracy by Epoch)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Epoch':<7} {'SGD':<10} {'Momentum':<10} {'RMSprop':<10} {'Adam':<10} {'Nadam':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for epoch in range(15):\n",
    "    sgd_acc = optimizer_results['SGD (vanilla)']['history']['val_accuracy'][epoch]\n",
    "    mom_acc = optimizer_results['SGD + Momentum']['history']['val_accuracy'][epoch]\n",
    "    rms_acc = optimizer_results['RMSprop']['history']['val_accuracy'][epoch]\n",
    "    adam_acc = optimizer_results['Adam']['history']['val_accuracy'][epoch]\n",
    "    nadam_acc = optimizer_results['Nadam']['history']['val_accuracy'][epoch]\n",
    "    \n",
    "    print(f\"{epoch+1:<7} {sgd_acc:<10.4f} {mom_acc:<10.4f} {rms_acc:<10.4f} {adam_acc:<10.4f} {nadam_acc:<10.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT 9: Learning Rate Scheduling\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üî¨ EXPERIMENT 9: Learning Rate Scheduling\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prepare longer training for scheduling\n",
    "X_train_large = X_train[:30000]\n",
    "y_train_large = y_train[:30000]\n",
    "\n",
    "print(f\"\\nüìä Extended Dataset: {X_train_large.shape[0]:,} training samples\")\n",
    "print(\"   Training for 30 epochs to see scheduling effects\")\n",
    "\n",
    "# Different scheduling strategies\n",
    "scheduling_configs = {}\n",
    "\n",
    "# 1. Constant LR (baseline)\n",
    "print(\"\\nüèóÔ∏è BASELINE: Constant Learning Rate\")\n",
    "model_const = create_standard_model()\n",
    "model_const.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_const = model_const.fit(\n",
    "    X_train_large, y_train_large,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "scheduling_configs['Constant LR'] = {\n",
    "    'history': history_const.history,\n",
    "    'final_val_acc': history_const.history['val_accuracy'][-1],\n",
    "    'best_val_acc': max(history_const.history['val_accuracy'])\n",
    "}\n",
    "\n",
    "print(f\"   ‚úì Best Val Acc: {max(history_const.history['val_accuracy']):.4f}\")\n",
    "\n",
    "# 2. Exponential Decay\n",
    "print(\"\\nüèóÔ∏è EXPONENTIAL DECAY SCHEDULING\")\n",
    "model_exp = create_standard_model()\n",
    "\n",
    "lr_schedule_exp = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.01,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.96\n",
    ")\n",
    "\n",
    "model_exp.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=lr_schedule_exp, momentum=0.9),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_exp = model_exp.fit(\n",
    "    X_train_large, y_train_large,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "scheduling_configs['Exponential Decay'] = {\n",
    "    'history': history_exp.history,\n",
    "    'final_val_acc': history_exp.history['val_accuracy'][-1],\n",
    "    'best_val_acc': max(history_exp.history['val_accuracy'])\n",
    "}\n",
    "\n",
    "print(f\"   ‚úì Best Val Acc: {max(history_exp.history['val_accuracy']):.4f}\")\n",
    "\n",
    "# 3. Piecewise Constant (Step Decay)\n",
    "print(\"\\nüèóÔ∏è PIECEWISE CONSTANT SCHEDULING\")\n",
    "model_piece = create_standard_model()\n",
    "\n",
    "# Define boundaries and values\n",
    "boundaries = [10 * 938, 20 * 938]  # Steps at epoch 10 and 20 (938 steps per epoch)\n",
    "values = [0.01, 0.005, 0.001]\n",
    "\n",
    "lr_schedule_piece = keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    boundaries=boundaries,\n",
    "    values=values\n",
    ")\n",
    "\n",
    "model_piece.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=lr_schedule_piece, momentum=0.9),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_piece = model_piece.fit(\n",
    "    X_train_large, y_train_large,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "scheduling_configs['Piecewise Constant'] = {\n",
    "    'history': history_piece.history,\n",
    "    'final_val_acc': history_piece.history['val_accuracy'][-1],\n",
    "    'best_val_acc': max(history_piece.history['val_accuracy'])\n",
    "}\n",
    "\n",
    "print(f\"   ‚úì Best Val Acc: {max(history_piece.history['val_accuracy']):.4f}\")\n",
    "\n",
    "# 4. ReduceLROnPlateau (Performance-based)\n",
    "print(\"\\nüèóÔ∏è REDUCE LR ON PLATEAU\")\n",
    "model_plateau = create_standard_model()\n",
    "\n",
    "model_plateau.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=0.0001,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "history_plateau = model_plateau.fit(\n",
    "    X_train_large, y_train_large,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[reduce_lr],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "scheduling_configs['ReduceLROnPlateau'] = {\n",
    "    'history': history_plateau.history,\n",
    "    'final_val_acc': history_plateau.history['val_accuracy'][-1],\n",
    "    'best_val_acc': max(history_plateau.history['val_accuracy'])\n",
    "}\n",
    "\n",
    "print(f\"   ‚úì Best Val Acc: {max(history_plateau.history['val_accuracy']):.4f}\")\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä LEARNING RATE SCHEDULING COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "schedule_summary = []\n",
    "for name, results in scheduling_configs.items():\n",
    "    schedule_summary.append({\n",
    "        'Strategy': name,\n",
    "        'Final Val Acc': results['final_val_acc'],\n",
    "        'Best Val Acc': results['best_val_acc']\n",
    "    })\n",
    "\n",
    "df_schedule = pd.DataFrame(schedule_summary)\n",
    "df_schedule = df_schedule.sort_values('Best Val Acc', ascending=False)\n",
    "print(\"\\n\" + df_schedule.to_string(index=False))\n",
    "\n",
    "# Epoch-by-epoch comparison (first 20 epochs)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìà VALIDATION ACCURACY OVER TIME (First 20 Epochs)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Epoch':<7} {'Constant':<12} {'Exponential':<12} {'Piecewise':<12} {'Plateau':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for epoch in range(20):\n",
    "    const = scheduling_configs['Constant LR']['history']['val_accuracy'][epoch]\n",
    "    exp = scheduling_configs['Exponential Decay']['history']['val_accuracy'][epoch]\n",
    "    piece = scheduling_configs['Piecewise Constant']['history']['val_accuracy'][epoch]\n",
    "    plateau = scheduling_configs['ReduceLROnPlateau']['history']['val_accuracy'][epoch]\n",
    "    \n",
    "    print(f\"{epoch+1:<7} {const:<12.4f} {exp:<12.4f} {piece:<12.4f} {plateau:<12.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ PART 8-9 COMPLETED!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüí° KEY INSIGHTS:\")\n",
    "print(\"   ‚Ä¢ Modern optimizers (Adam, RMSprop) converge MUCH faster than SGD\")\n",
    "print(\"   ‚Ä¢ Momentum significantly improves SGD performance\")\n",
    "print(\"   ‚Ä¢ Learning rate scheduling helps achieve better final accuracy\")\n",
    "print(\"   ‚Ä¢ ReduceLROnPlateau is adaptive and often best for real-world tasks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32b91dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CHAPTER 11: Part 10\n",
      "Regularization Techniques\n",
      "======================================================================\n",
      "\n",
      "üìä Dataset: 40,000 training samples\n",
      "\n",
      "======================================================================\n",
      "üî¨ EXPERIMENT 10: L1/L2 Regularization\n",
      "======================================================================\n",
      "\n",
      "‚è≥ Training models with different regularization (20 epochs)...\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è Training with No Regularization...\n",
      "======================================================================\n",
      "   ‚úì Train Acc: 0.9314\n",
      "   ‚úì Val Acc: 0.8760\n",
      "   ‚úì Overfitting Gap: 0.0554\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è Training with L2 (0.001)...\n",
      "======================================================================\n",
      "   ‚úì Train Acc: 0.8786\n",
      "   ‚úì Val Acc: 0.8390\n",
      "   ‚úì Overfitting Gap: 0.0396\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è Training with L2 (0.01)...\n",
      "======================================================================\n",
      "   ‚úì Train Acc: 0.8050\n",
      "   ‚úì Val Acc: 0.8035\n",
      "   ‚úì Overfitting Gap: 0.0015\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è Training with L1 (0.001)...\n",
      "======================================================================\n",
      "   ‚úì Train Acc: 0.8145\n",
      "   ‚úì Val Acc: 0.7965\n",
      "   ‚úì Overfitting Gap: 0.0180\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è Training with L1 + L2 (0.001)...\n",
      "======================================================================\n",
      "   ‚úì Train Acc: 0.8080\n",
      "   ‚úì Val Acc: 0.7790\n",
      "   ‚úì Overfitting Gap: 0.0290\n",
      "\n",
      "======================================================================\n",
      "üìä L1/L2 REGULARIZATION COMPARISON\n",
      "======================================================================\n",
      "\n",
      "    Configuration  Train Acc  Val Acc  Overfit Gap\n",
      "No Regularization   0.931375   0.8760     0.055375\n",
      "       L2 (0.001)   0.878600   0.8390     0.039600\n",
      "        L2 (0.01)   0.805000   0.8035     0.001500\n",
      "       L1 (0.001)   0.814525   0.7965     0.018025\n",
      "  L1 + L2 (0.001)   0.808000   0.7790     0.029000\n",
      "\n",
      "======================================================================\n",
      "üî¨ EXPERIMENT 11: Dropout Regularization\n",
      "======================================================================\n",
      "\n",
      "‚è≥ Training models with different dropout rates (20 epochs)...\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è Training with No Dropout...\n",
      "======================================================================\n",
      "   ‚úì Train Acc: 0.9318\n",
      "   ‚úì Val Acc: 0.8665\n",
      "   ‚úì Overfitting Gap: 0.0653\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è Training with Dropout 10%...\n",
      "======================================================================\n",
      "   ‚úì Train Acc: 0.9095\n",
      "   ‚úì Val Acc: 0.8880\n",
      "   ‚úì Overfitting Gap: 0.0215\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è Training with Dropout 20%...\n",
      "======================================================================\n",
      "   ‚úì Train Acc: 0.8971\n",
      "   ‚úì Val Acc: 0.8715\n",
      "   ‚úì Overfitting Gap: 0.0256\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è Training with Dropout 30%...\n",
      "======================================================================\n",
      "   ‚úì Train Acc: 0.8813\n",
      "   ‚úì Val Acc: 0.8700\n",
      "   ‚úì Overfitting Gap: 0.0113\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è Training with Dropout 50%...\n",
      "======================================================================\n",
      "   ‚úì Train Acc: 0.8275\n",
      "   ‚úì Val Acc: 0.8475\n",
      "   ‚úì Overfitting Gap: -0.0200\n",
      "\n",
      "======================================================================\n",
      "üìä DROPOUT COMPARISON\n",
      "======================================================================\n",
      "\n",
      "Configuration  Train Acc  Val Acc  Overfit Gap\n",
      "  Dropout 10%   0.909475   0.8880     0.021475\n",
      "  Dropout 20%   0.897150   0.8715     0.025650\n",
      "  Dropout 30%   0.881275   0.8700     0.011275\n",
      "   No Dropout   0.931825   0.8665     0.065325\n",
      "  Dropout 50%   0.827525   0.8475    -0.019975\n",
      "\n",
      "======================================================================\n",
      "üî¨ EXPERIMENT 12: Combining Regularization Techniques\n",
      "======================================================================\n",
      "\n",
      "üèóÔ∏è BEST PRACTICES MODEL\n",
      "   Architecture:\n",
      "   ‚Ä¢ L2 Regularization (0.001)\n",
      "   ‚Ä¢ Batch Normalization\n",
      "   ‚Ä¢ Dropout (20%)\n",
      "   ‚Ä¢ SGD + Nesterov Momentum\n",
      "   ‚Ä¢ ReduceLROnPlateau callback\n",
      "   ‚Ä¢ EarlyStopping callback\n",
      "\n",
      "   ‚úì Stopped at epoch: 50\n",
      "   ‚úì Train Acc: 0.9415\n",
      "   ‚úì Val Acc: 0.9005\n",
      "   ‚úì Overfitting Gap: 0.0410\n",
      "\n",
      "======================================================================\n",
      "üìä FINAL COMPARISON: Best from Each Category\n",
      "======================================================================\n",
      "\n",
      "                Technique  Val Acc  Overfit Gap\n",
      "Combined (Best Practices)   0.9005     0.041025\n",
      "             Best Dropout   0.8880     0.021475\n",
      "        No Regularization   0.8760     0.055375\n",
      "                  Best L2   0.8760     0.055375\n",
      "\n",
      "======================================================================\n",
      "‚úÖ PART 10 COMPLETED!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CHAPTER 11: Part 10\n",
    "# Regularization Techniques\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CHAPTER 11: Part 10\")\n",
    "print(\"Regularization Techniques\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load data\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "# Use more data to see overfitting\n",
    "X_train_large = X_train[:40000]\n",
    "y_train_large = y_train[:40000]\n",
    "X_val = X_test[:2000]\n",
    "y_val = y_test[:2000]\n",
    "\n",
    "print(f\"\\nüìä Dataset: {X_train_large.shape[0]:,} training samples\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT 10: L1 and L2 Regularization\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üî¨ EXPERIMENT 10: L1/L2 Regularization\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def create_model_with_regularization(reg_type=None, reg_strength=0.01):\n",
    "    \"\"\"Create model with specified regularization\"\"\"\n",
    "    \n",
    "    if reg_type == 'l1':\n",
    "        regularizer = regularizers.l1(reg_strength)\n",
    "    elif reg_type == 'l2':\n",
    "        regularizer = regularizers.l2(reg_strength)\n",
    "    elif reg_type == 'l1_l2':\n",
    "        regularizer = regularizers.l1_l2(l1=reg_strength, l2=reg_strength)\n",
    "    else:\n",
    "        regularizer = None\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        layers.Flatten(input_shape=[28, 28]),\n",
    "        layers.Dense(200, activation='relu', kernel_initializer='he_normal',\n",
    "                    kernel_regularizer=regularizer),\n",
    "        layers.Dense(200, activation='relu', kernel_initializer='he_normal',\n",
    "                    kernel_regularizer=regularizer),\n",
    "        layers.Dense(200, activation='relu', kernel_initializer='he_normal',\n",
    "                    kernel_regularizer=regularizer),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Test different regularization strategies\n",
    "reg_configs = {\n",
    "    'No Regularization': (None, 0),\n",
    "    'L2 (0.001)': ('l2', 0.001),\n",
    "    'L2 (0.01)': ('l2', 0.01),\n",
    "    'L1 (0.001)': ('l1', 0.001),\n",
    "    'L1 + L2 (0.001)': ('l1_l2', 0.001),\n",
    "}\n",
    "\n",
    "print(\"\\n‚è≥ Training models with different regularization (20 epochs)...\")\n",
    "\n",
    "reg_results = {}\n",
    "\n",
    "for config_name, (reg_type, reg_strength) in reg_configs.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üèãÔ∏è Training with {config_name}...\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    model = create_model_with_regularization(reg_type, reg_strength)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_large, y_train_large,\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    reg_results[config_name] = {\n",
    "        'history': history.history,\n",
    "        'final_train_acc': history.history['accuracy'][-1],\n",
    "        'final_val_acc': history.history['val_accuracy'][-1],\n",
    "        'overfitting': history.history['accuracy'][-1] - history.history['val_accuracy'][-1]\n",
    "    }\n",
    "    \n",
    "    print(f\"   ‚úì Train Acc: {history.history['accuracy'][-1]:.4f}\")\n",
    "    print(f\"   ‚úì Val Acc: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "    print(f\"   ‚úì Overfitting Gap: {reg_results[config_name]['overfitting']:.4f}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä L1/L2 REGULARIZATION COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "reg_summary = []\n",
    "for name, results in reg_results.items():\n",
    "    reg_summary.append({\n",
    "        'Configuration': name,\n",
    "        'Train Acc': results['final_train_acc'],\n",
    "        'Val Acc': results['final_val_acc'],\n",
    "        'Overfit Gap': results['overfitting']\n",
    "    })\n",
    "\n",
    "df_reg = pd.DataFrame(reg_summary)\n",
    "df_reg = df_reg.sort_values('Val Acc', ascending=False)\n",
    "print(\"\\n\" + df_reg.to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT 11: Dropout\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üî¨ EXPERIMENT 11: Dropout Regularization\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def create_model_with_dropout(dropout_rate=0.0):\n",
    "    \"\"\"Create model with dropout\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Flatten(input_shape=[28, 28]),\n",
    "        layers.Dense(200, activation='relu', kernel_initializer='he_normal'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(200, activation='relu', kernel_initializer='he_normal'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(200, activation='relu', kernel_initializer='he_normal'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Test different dropout rates\n",
    "dropout_configs = {\n",
    "    'No Dropout': 0.0,\n",
    "    'Dropout 10%': 0.1,\n",
    "    'Dropout 20%': 0.2,\n",
    "    'Dropout 30%': 0.3,\n",
    "    'Dropout 50%': 0.5,\n",
    "}\n",
    "\n",
    "print(\"\\n‚è≥ Training models with different dropout rates (20 epochs)...\")\n",
    "\n",
    "dropout_results = {}\n",
    "\n",
    "for config_name, dropout_rate in dropout_configs.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üèãÔ∏è Training with {config_name}...\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    model = create_model_with_dropout(dropout_rate)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_large, y_train_large,\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    dropout_results[config_name] = {\n",
    "        'history': history.history,\n",
    "        'final_train_acc': history.history['accuracy'][-1],\n",
    "        'final_val_acc': history.history['val_accuracy'][-1],\n",
    "        'overfitting': history.history['accuracy'][-1] - history.history['val_accuracy'][-1]\n",
    "    }\n",
    "    \n",
    "    print(f\"   ‚úì Train Acc: {history.history['accuracy'][-1]:.4f}\")\n",
    "    print(f\"   ‚úì Val Acc: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "    print(f\"   ‚úì Overfitting Gap: {dropout_results[config_name]['overfitting']:.4f}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä DROPOUT COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "dropout_summary = []\n",
    "for name, results in dropout_results.items():\n",
    "    dropout_summary.append({\n",
    "        'Configuration': name,\n",
    "        'Train Acc': results['final_train_acc'],\n",
    "        'Val Acc': results['final_val_acc'],\n",
    "        'Overfit Gap': results['overfitting']\n",
    "    })\n",
    "\n",
    "df_dropout = pd.DataFrame(dropout_summary)\n",
    "df_dropout = df_dropout.sort_values('Val Acc', ascending=False)\n",
    "print(\"\\n\" + df_dropout.to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT 12: Combining Techniques\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üî¨ EXPERIMENT 12: Combining Regularization Techniques\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Best combination: L2 + Dropout + Batch Normalization\n",
    "print(\"\\nüèóÔ∏è BEST PRACTICES MODEL\")\n",
    "model_best = keras.Sequential([\n",
    "    layers.Flatten(input_shape=[28, 28]),\n",
    "    \n",
    "    layers.Dense(200, kernel_initializer='he_normal', \n",
    "                kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    \n",
    "    layers.Dense(200, kernel_initializer='he_normal',\n",
    "                kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    \n",
    "    layers.Dense(200, kernel_initializer='he_normal',\n",
    "                kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    \n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_best.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"   Architecture:\")\n",
    "print(\"   ‚Ä¢ L2 Regularization (0.001)\")\n",
    "print(\"   ‚Ä¢ Batch Normalization\")\n",
    "print(\"   ‚Ä¢ Dropout (20%)\")\n",
    "print(\"   ‚Ä¢ SGD + Nesterov Momentum\")\n",
    "print(\"   ‚Ä¢ ReduceLROnPlateau callback\")\n",
    "print(\"   ‚Ä¢ EarlyStopping callback\")\n",
    "\n",
    "history_best = model_best.fit(\n",
    "    X_train_large, y_train_large,\n",
    "    epochs=50,  # More epochs with early stopping\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[reduce_lr, early_stop],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "best_train_acc = history_best.history['accuracy'][-1]\n",
    "best_val_acc = history_best.history['val_accuracy'][-1]\n",
    "best_overfit = best_train_acc - best_val_acc\n",
    "\n",
    "print(f\"\\n   ‚úì Stopped at epoch: {len(history_best.history['loss'])}\")\n",
    "print(f\"   ‚úì Train Acc: {best_train_acc:.4f}\")\n",
    "print(f\"   ‚úì Val Acc: {best_val_acc:.4f}\")\n",
    "print(f\"   ‚úì Overfitting Gap: {best_overfit:.4f}\")\n",
    "\n",
    "# Final comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä FINAL COMPARISON: Best from Each Category\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "final_comparison = pd.DataFrame([\n",
    "    {\n",
    "        'Technique': 'No Regularization',\n",
    "        'Val Acc': reg_results['No Regularization']['final_val_acc'],\n",
    "        'Overfit Gap': reg_results['No Regularization']['overfitting']\n",
    "    },\n",
    "    {\n",
    "        'Technique': 'Best L2',\n",
    "        'Val Acc': df_reg.iloc[0]['Val Acc'],\n",
    "        'Overfit Gap': df_reg.iloc[0]['Overfit Gap']\n",
    "    },\n",
    "    {\n",
    "        'Technique': 'Best Dropout',\n",
    "        'Val Acc': df_dropout.iloc[0]['Val Acc'],\n",
    "        'Overfit Gap': df_dropout.iloc[0]['Overfit Gap']\n",
    "    },\n",
    "    {\n",
    "        'Technique': 'Combined (Best Practices)',\n",
    "        'Val Acc': best_val_acc,\n",
    "        'Overfit Gap': best_overfit\n",
    "    }\n",
    "])\n",
    "\n",
    "final_comparison = final_comparison.sort_values('Val Acc', ascending=False)\n",
    "print(\"\\n\" + final_comparison.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ PART 10 COMPLETED!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a6104f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CHAPTER 11: Part 11-12\n",
      "Practical Guidelines & Comprehensive Experiments\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üî¨ EXPERIMENT 13: Complete Fashion MNIST Pipeline\n",
      "======================================================================\n",
      "\n",
      "üìä Dataset Split:\n",
      "   Training:   55,000 samples\n",
      "   Validation: 5,000 samples\n",
      "   Test:       10,000 samples\n",
      "\n",
      "======================================================================\n",
      "üèóÔ∏è  BUILDING PRODUCTION-GRADE MODEL\n",
      "======================================================================\n",
      "\n",
      "üìã Model Architecture:\n",
      "   ‚Ä¢ 3 Hidden Layers: [300, 200, 100]\n",
      "   ‚Ä¢ Initialization: He Normal\n",
      "   ‚Ä¢ Activation: ReLU\n",
      "   ‚Ä¢ Regularization: L2(0.0001) + Dropout(0.2) + BatchNorm\n",
      "   ‚Ä¢ Output: 10 classes (softmax)\n",
      "\n",
      "üìã Training Configuration:\n",
      "   ‚Ä¢ Optimizer: SGD + Nesterov Momentum (0.9)\n",
      "   ‚Ä¢ Initial LR: 0.01\n",
      "   ‚Ä¢ Callbacks: ReduceLROnPlateau + EarlyStopping + ModelCheckpoint\n",
      "   ‚Ä¢ Max Epochs: 100 (with early stopping)\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è  TRAINING MODEL...\n",
      "======================================================================\n",
      "Epoch 1/100\n",
      "1719/1719 - 8s - loss: 0.6992 - accuracy: 0.7943 - val_loss: 0.5018 - val_accuracy: 0.8558 - lr: 0.0100 - 8s/epoch - 5ms/step\n",
      "Epoch 2/100\n",
      "1719/1719 - 7s - loss: 0.5595 - accuracy: 0.8411 - val_loss: 0.4674 - val_accuracy: 0.8682 - lr: 0.0100 - 7s/epoch - 4ms/step\n",
      "Epoch 3/100\n",
      "1719/1719 - 6s - loss: 0.5156 - accuracy: 0.8543 - val_loss: 0.4676 - val_accuracy: 0.8686 - lr: 0.0100 - 6s/epoch - 4ms/step\n",
      "Epoch 4/100\n",
      "1719/1719 - 6s - loss: 0.4863 - accuracy: 0.8648 - val_loss: 0.4386 - val_accuracy: 0.8744 - lr: 0.0100 - 6s/epoch - 3ms/step\n",
      "Epoch 5/100\n",
      "1719/1719 - 6s - loss: 0.4692 - accuracy: 0.8688 - val_loss: 0.4172 - val_accuracy: 0.8834 - lr: 0.0100 - 6s/epoch - 3ms/step\n",
      "Epoch 6/100\n",
      "1719/1719 - 6s - loss: 0.4515 - accuracy: 0.8726 - val_loss: 0.4429 - val_accuracy: 0.8706 - lr: 0.0100 - 6s/epoch - 4ms/step\n",
      "Epoch 7/100\n",
      "1719/1719 - 7s - loss: 0.4393 - accuracy: 0.8759 - val_loss: 0.4146 - val_accuracy: 0.8826 - lr: 0.0100 - 7s/epoch - 4ms/step\n",
      "Epoch 8/100\n",
      "1719/1719 - 6s - loss: 0.4235 - accuracy: 0.8814 - val_loss: 0.4095 - val_accuracy: 0.8828 - lr: 0.0100 - 6s/epoch - 4ms/step\n",
      "Epoch 9/100\n",
      "1719/1719 - 6s - loss: 0.4154 - accuracy: 0.8828 - val_loss: 0.4088 - val_accuracy: 0.8808 - lr: 0.0100 - 6s/epoch - 4ms/step\n",
      "Epoch 10/100\n",
      "1719/1719 - 6s - loss: 0.4043 - accuracy: 0.8866 - val_loss: 0.4210 - val_accuracy: 0.8748 - lr: 0.0100 - 6s/epoch - 4ms/step\n",
      "Epoch 11/100\n",
      "1719/1719 - 6s - loss: 0.3977 - accuracy: 0.8882 - val_loss: 0.4007 - val_accuracy: 0.8848 - lr: 0.0100 - 6s/epoch - 4ms/step\n",
      "Epoch 12/100\n",
      "1719/1719 - 6s - loss: 0.3884 - accuracy: 0.8908 - val_loss: 0.4419 - val_accuracy: 0.8734 - lr: 0.0100 - 6s/epoch - 4ms/step\n",
      "Epoch 13/100\n",
      "1719/1719 - 6s - loss: 0.3827 - accuracy: 0.8920 - val_loss: 0.3932 - val_accuracy: 0.8850 - lr: 0.0100 - 6s/epoch - 4ms/step\n",
      "Epoch 14/100\n",
      "1719/1719 - 6s - loss: 0.3731 - accuracy: 0.8940 - val_loss: 0.4134 - val_accuracy: 0.8802 - lr: 0.0100 - 6s/epoch - 4ms/step\n",
      "Epoch 15/100\n",
      "1719/1719 - 6s - loss: 0.3680 - accuracy: 0.8972 - val_loss: 0.4018 - val_accuracy: 0.8852 - lr: 0.0100 - 6s/epoch - 4ms/step\n",
      "Epoch 16/100\n",
      "1719/1719 - 6s - loss: 0.3642 - accuracy: 0.8952 - val_loss: 0.3864 - val_accuracy: 0.8888 - lr: 0.0100 - 6s/epoch - 4ms/step\n",
      "Epoch 17/100\n",
      "1719/1719 - 6s - loss: 0.3595 - accuracy: 0.8992 - val_loss: 0.4063 - val_accuracy: 0.8860 - lr: 0.0100 - 6s/epoch - 4ms/step\n",
      "Epoch 18/100\n",
      "1719/1719 - 7s - loss: 0.3563 - accuracy: 0.8998 - val_loss: 0.3954 - val_accuracy: 0.8868 - lr: 0.0100 - 7s/epoch - 4ms/step\n",
      "Epoch 19/100\n",
      "1719/1719 - 7s - loss: 0.3517 - accuracy: 0.9001 - val_loss: 0.3832 - val_accuracy: 0.8872 - lr: 0.0100 - 7s/epoch - 4ms/step\n",
      "Epoch 20/100\n",
      "1719/1719 - 6s - loss: 0.3456 - accuracy: 0.9035 - val_loss: 0.3888 - val_accuracy: 0.8868 - lr: 0.0100 - 6s/epoch - 4ms/step\n",
      "Epoch 21/100\n",
      "1719/1719 - 6s - loss: 0.3415 - accuracy: 0.9042 - val_loss: 0.3833 - val_accuracy: 0.8900 - lr: 0.0100 - 6s/epoch - 4ms/step\n",
      "Epoch 22/100\n",
      "1719/1719 - 6s - loss: 0.3408 - accuracy: 0.9036 - val_loss: 0.3936 - val_accuracy: 0.8896 - lr: 0.0100 - 6s/epoch - 4ms/step\n",
      "Epoch 23/100\n",
      "1719/1719 - 6s - loss: 0.3359 - accuracy: 0.9056 - val_loss: 0.3907 - val_accuracy: 0.8868 - lr: 0.0100 - 6s/epoch - 4ms/step\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "1719/1719 - 6s - loss: 0.3346 - accuracy: 0.9066 - val_loss: 0.3946 - val_accuracy: 0.8872 - lr: 0.0100 - 6s/epoch - 4ms/step\n",
      "Epoch 25/100\n",
      "1719/1719 - 6s - loss: 0.3020 - accuracy: 0.9179 - val_loss: 0.3852 - val_accuracy: 0.8892 - lr: 0.0050 - 6s/epoch - 4ms/step\n",
      "Epoch 26/100\n",
      "1719/1719 - 6s - loss: 0.2943 - accuracy: 0.9202 - val_loss: 0.3706 - val_accuracy: 0.8964 - lr: 0.0050 - 6s/epoch - 4ms/step\n",
      "Epoch 27/100\n",
      "1719/1719 - 6s - loss: 0.2891 - accuracy: 0.9224 - val_loss: 0.3943 - val_accuracy: 0.8886 - lr: 0.0050 - 6s/epoch - 4ms/step\n",
      "Epoch 28/100\n",
      "1719/1719 - 6s - loss: 0.2822 - accuracy: 0.9234 - val_loss: 0.3768 - val_accuracy: 0.8968 - lr: 0.0050 - 6s/epoch - 4ms/step\n",
      "Epoch 29/100\n",
      "1719/1719 - 6s - loss: 0.2857 - accuracy: 0.9223 - val_loss: 0.3910 - val_accuracy: 0.8968 - lr: 0.0050 - 6s/epoch - 4ms/step\n",
      "Epoch 30/100\n",
      "1719/1719 - 6s - loss: 0.2800 - accuracy: 0.9231 - val_loss: 0.3810 - val_accuracy: 0.8952 - lr: 0.0050 - 6s/epoch - 4ms/step\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "1719/1719 - 6s - loss: 0.2759 - accuracy: 0.9251 - val_loss: 0.3832 - val_accuracy: 0.8986 - lr: 0.0050 - 6s/epoch - 4ms/step\n",
      "Epoch 32/100\n",
      "1719/1719 - 6s - loss: 0.2547 - accuracy: 0.9329 - val_loss: 0.3689 - val_accuracy: 0.8992 - lr: 0.0025 - 6s/epoch - 4ms/step\n",
      "Epoch 33/100\n",
      "1719/1719 - 6s - loss: 0.2468 - accuracy: 0.9360 - val_loss: 0.3739 - val_accuracy: 0.8996 - lr: 0.0025 - 6s/epoch - 4ms/step\n",
      "Epoch 34/100\n",
      "1719/1719 - 6s - loss: 0.2461 - accuracy: 0.9358 - val_loss: 0.3656 - val_accuracy: 0.9034 - lr: 0.0025 - 6s/epoch - 4ms/step\n",
      "Epoch 35/100\n",
      "1719/1719 - 6s - loss: 0.2419 - accuracy: 0.9362 - val_loss: 0.3818 - val_accuracy: 0.9022 - lr: 0.0025 - 6s/epoch - 4ms/step\n",
      "Epoch 36/100\n",
      "1719/1719 - 6s - loss: 0.2367 - accuracy: 0.9391 - val_loss: 0.3610 - val_accuracy: 0.9012 - lr: 0.0025 - 6s/epoch - 3ms/step\n",
      "Epoch 37/100\n",
      "1719/1719 - 6s - loss: 0.2363 - accuracy: 0.9377 - val_loss: 0.3717 - val_accuracy: 0.9024 - lr: 0.0025 - 6s/epoch - 3ms/step\n",
      "Epoch 38/100\n",
      "1719/1719 - 6s - loss: 0.2377 - accuracy: 0.9367 - val_loss: 0.3884 - val_accuracy: 0.8944 - lr: 0.0025 - 6s/epoch - 3ms/step\n",
      "Epoch 39/100\n",
      "1719/1719 - 6s - loss: 0.2355 - accuracy: 0.9387 - val_loss: 0.3739 - val_accuracy: 0.8986 - lr: 0.0025 - 6s/epoch - 3ms/step\n",
      "Epoch 40/100\n",
      "1719/1719 - 6s - loss: 0.2312 - accuracy: 0.9399 - val_loss: 0.3823 - val_accuracy: 0.9000 - lr: 0.0025 - 6s/epoch - 3ms/step\n",
      "Epoch 41/100\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "1719/1719 - 6s - loss: 0.2306 - accuracy: 0.9388 - val_loss: 0.3673 - val_accuracy: 0.8992 - lr: 0.0025 - 6s/epoch - 4ms/step\n",
      "Epoch 42/100\n",
      "1719/1719 - 6s - loss: 0.2149 - accuracy: 0.9452 - val_loss: 0.3667 - val_accuracy: 0.9020 - lr: 0.0012 - 6s/epoch - 4ms/step\n",
      "Epoch 43/100\n",
      "1719/1719 - 6s - loss: 0.2108 - accuracy: 0.9466 - val_loss: 0.3709 - val_accuracy: 0.9014 - lr: 0.0012 - 6s/epoch - 4ms/step\n",
      "Epoch 44/100\n",
      "1719/1719 - 7s - loss: 0.2099 - accuracy: 0.9471 - val_loss: 0.3805 - val_accuracy: 0.8986 - lr: 0.0012 - 7s/epoch - 4ms/step\n",
      "Epoch 45/100\n",
      "1719/1719 - 6s - loss: 0.2063 - accuracy: 0.9473 - val_loss: 0.3752 - val_accuracy: 0.9012 - lr: 0.0012 - 6s/epoch - 4ms/step\n",
      "Epoch 46/100\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "1719/1719 - 7s - loss: 0.2048 - accuracy: 0.9474 - val_loss: 0.3729 - val_accuracy: 0.9048 - lr: 0.0012 - 7s/epoch - 4ms/step\n",
      "Epoch 47/100\n",
      "1719/1719 - 7s - loss: 0.1983 - accuracy: 0.9501 - val_loss: 0.3710 - val_accuracy: 0.9048 - lr: 6.2500e-04 - 7s/epoch - 4ms/step\n",
      "Epoch 48/100\n",
      "1719/1719 - 7s - loss: 0.1958 - accuracy: 0.9513 - val_loss: 0.3729 - val_accuracy: 0.9048 - lr: 6.2500e-04 - 7s/epoch - 4ms/step\n",
      "Epoch 49/100\n",
      "1719/1719 - 8s - loss: 0.1927 - accuracy: 0.9531 - val_loss: 0.3717 - val_accuracy: 0.9042 - lr: 6.2500e-04 - 8s/epoch - 4ms/step\n",
      "Epoch 50/100\n",
      "1719/1719 - 7s - loss: 0.1914 - accuracy: 0.9536 - val_loss: 0.3803 - val_accuracy: 0.9022 - lr: 6.2500e-04 - 7s/epoch - 4ms/step\n",
      "Epoch 51/100\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "Restoring model weights from the end of the best epoch: 36.\n",
      "1719/1719 - 7s - loss: 0.1891 - accuracy: 0.9546 - val_loss: 0.3803 - val_accuracy: 0.9046 - lr: 6.2500e-04 - 7s/epoch - 4ms/step\n",
      "Epoch 51: early stopping\n",
      "\n",
      "======================================================================\n",
      "‚úÖ TRAINING COMPLETED!\n",
      "======================================================================\n",
      "   Total Training Time: 5.42 minutes\n",
      "   Epochs Trained: 51\n",
      "\n",
      "======================================================================\n",
      "üìä FINAL EVALUATION\n",
      "======================================================================\n",
      "\n",
      "üéØ Test Set Performance:\n",
      "   Loss:     0.3680\n",
      "   Accuracy: 0.9022 (90.22%)\n",
      "\n",
      "üìà Training Summary:\n",
      "   Best Epoch: 46\n",
      "   Best Val Accuracy: 0.9048\n",
      "   Train Accuracy at Best: 0.9474\n",
      "   Overfitting Gap: 0.0426\n",
      "\n",
      "======================================================================\n",
      "üî¨ EXPERIMENT 14: CIFAR-10 Deep Network\n",
      "======================================================================\n",
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 21s 0us/step\n",
      "\n",
      "üìä CIFAR-10 Dataset:\n",
      "   Training:   45,000 samples\n",
      "   Validation: 5,000 samples\n",
      "   Test:       10,000 samples\n",
      "   Image shape: (32, 32, 3)\n",
      "\n",
      "======================================================================\n",
      "üèóÔ∏è  BUILDING DEEP CIFAR-10 MODEL\n",
      "======================================================================\n",
      "\n",
      "üìã Model Architecture:\n",
      "   ‚Ä¢ 4 Hidden Layers: [400, 300, 200, 100]\n",
      "   ‚Ä¢ Deeper network for more complex task\n",
      "   ‚Ä¢ Higher Dropout (0.3) due to more complex data\n",
      "\n",
      "üìã Training Configuration:\n",
      "   ‚Ä¢ Optimizer: Adam (faster for complex tasks)\n",
      "   ‚Ä¢ Initial LR: 0.001\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è  TRAINING CIFAR-10 MODEL (30 epochs)...\n",
      "======================================================================\n",
      "Epoch 1/30\n",
      "352/352 - 7s - loss: 1.9652 - accuracy: 0.2987 - val_loss: 1.8141 - val_accuracy: 0.3486 - lr: 0.0010 - 7s/epoch - 19ms/step\n",
      "Epoch 2/30\n",
      "352/352 - 5s - loss: 1.6976 - accuracy: 0.3903 - val_loss: 1.6953 - val_accuracy: 0.3920 - lr: 0.0010 - 5s/epoch - 13ms/step\n",
      "Epoch 3/30\n",
      "352/352 - 5s - loss: 1.5963 - accuracy: 0.4308 - val_loss: 1.6455 - val_accuracy: 0.4078 - lr: 0.0010 - 5s/epoch - 13ms/step\n",
      "Epoch 4/30\n",
      "352/352 - 5s - loss: 1.5400 - accuracy: 0.4531 - val_loss: 1.9445 - val_accuracy: 0.3310 - lr: 0.0010 - 5s/epoch - 13ms/step\n",
      "Epoch 5/30\n",
      "352/352 - 5s - loss: 1.4872 - accuracy: 0.4692 - val_loss: 1.5833 - val_accuracy: 0.4336 - lr: 0.0010 - 5s/epoch - 13ms/step\n",
      "Epoch 6/30\n",
      "352/352 - 5s - loss: 1.4457 - accuracy: 0.4852 - val_loss: 1.4582 - val_accuracy: 0.4834 - lr: 0.0010 - 5s/epoch - 13ms/step\n",
      "Epoch 7/30\n",
      "352/352 - 5s - loss: 1.4127 - accuracy: 0.5009 - val_loss: 1.5592 - val_accuracy: 0.4432 - lr: 0.0010 - 5s/epoch - 15ms/step\n",
      "Epoch 8/30\n",
      "352/352 - 5s - loss: 1.3802 - accuracy: 0.5096 - val_loss: 1.5947 - val_accuracy: 0.4260 - lr: 0.0010 - 5s/epoch - 13ms/step\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "352/352 - 5s - loss: 1.3567 - accuracy: 0.5207 - val_loss: 1.5651 - val_accuracy: 0.4380 - lr: 0.0010 - 5s/epoch - 13ms/step\n",
      "Epoch 10/30\n",
      "352/352 - 5s - loss: 1.2904 - accuracy: 0.5440 - val_loss: 1.4238 - val_accuracy: 0.4902 - lr: 5.0000e-04 - 5s/epoch - 14ms/step\n",
      "Epoch 11/30\n",
      "352/352 - 5s - loss: 1.2656 - accuracy: 0.5530 - val_loss: 1.3555 - val_accuracy: 0.5124 - lr: 5.0000e-04 - 5s/epoch - 13ms/step\n",
      "Epoch 12/30\n",
      "352/352 - 5s - loss: 1.2428 - accuracy: 0.5638 - val_loss: 1.4657 - val_accuracy: 0.4878 - lr: 5.0000e-04 - 5s/epoch - 13ms/step\n",
      "Epoch 13/30\n",
      "352/352 - 5s - loss: 1.2191 - accuracy: 0.5708 - val_loss: 1.4099 - val_accuracy: 0.5030 - lr: 5.0000e-04 - 5s/epoch - 13ms/step\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "352/352 - 5s - loss: 1.2110 - accuracy: 0.5724 - val_loss: 1.4855 - val_accuracy: 0.4716 - lr: 5.0000e-04 - 5s/epoch - 13ms/step\n",
      "Epoch 15/30\n",
      "352/352 - 5s - loss: 1.1624 - accuracy: 0.5922 - val_loss: 1.2533 - val_accuracy: 0.5478 - lr: 2.5000e-04 - 5s/epoch - 13ms/step\n",
      "Epoch 16/30\n",
      "352/352 - 5s - loss: 1.1396 - accuracy: 0.5986 - val_loss: 1.2435 - val_accuracy: 0.5576 - lr: 2.5000e-04 - 5s/epoch - 13ms/step\n",
      "Epoch 17/30\n",
      "352/352 - 5s - loss: 1.1302 - accuracy: 0.5996 - val_loss: 1.3507 - val_accuracy: 0.5292 - lr: 2.5000e-04 - 5s/epoch - 13ms/step\n",
      "Epoch 18/30\n",
      "352/352 - 5s - loss: 1.1202 - accuracy: 0.6076 - val_loss: 1.2382 - val_accuracy: 0.5616 - lr: 2.5000e-04 - 5s/epoch - 13ms/step\n",
      "Epoch 19/30\n",
      "352/352 - 5s - loss: 1.1100 - accuracy: 0.6089 - val_loss: 1.2652 - val_accuracy: 0.5522 - lr: 2.5000e-04 - 5s/epoch - 13ms/step\n",
      "Epoch 20/30\n",
      "352/352 - 5s - loss: 1.0958 - accuracy: 0.6142 - val_loss: 1.2843 - val_accuracy: 0.5520 - lr: 2.5000e-04 - 5s/epoch - 14ms/step\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "352/352 - 5s - loss: 1.0878 - accuracy: 0.6171 - val_loss: 1.2651 - val_accuracy: 0.5590 - lr: 2.5000e-04 - 5s/epoch - 14ms/step\n",
      "Epoch 22/30\n",
      "352/352 - 5s - loss: 1.0626 - accuracy: 0.6279 - val_loss: 1.2073 - val_accuracy: 0.5746 - lr: 1.2500e-04 - 5s/epoch - 13ms/step\n",
      "Epoch 23/30\n",
      "352/352 - 5s - loss: 1.0499 - accuracy: 0.6308 - val_loss: 1.2005 - val_accuracy: 0.5778 - lr: 1.2500e-04 - 5s/epoch - 13ms/step\n",
      "Epoch 24/30\n",
      "352/352 - 5s - loss: 1.0411 - accuracy: 0.6354 - val_loss: 1.2560 - val_accuracy: 0.5656 - lr: 1.2500e-04 - 5s/epoch - 13ms/step\n",
      "Epoch 25/30\n",
      "352/352 - 4s - loss: 1.0370 - accuracy: 0.6350 - val_loss: 1.2279 - val_accuracy: 0.5714 - lr: 1.2500e-04 - 4s/epoch - 13ms/step\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "352/352 - 5s - loss: 1.0282 - accuracy: 0.6395 - val_loss: 1.2030 - val_accuracy: 0.5768 - lr: 1.2500e-04 - 5s/epoch - 13ms/step\n",
      "Epoch 27/30\n",
      "352/352 - 5s - loss: 1.0165 - accuracy: 0.6441 - val_loss: 1.1948 - val_accuracy: 0.5866 - lr: 6.2500e-05 - 5s/epoch - 13ms/step\n",
      "Epoch 28/30\n",
      "352/352 - 5s - loss: 1.0074 - accuracy: 0.6448 - val_loss: 1.1916 - val_accuracy: 0.5892 - lr: 6.2500e-05 - 5s/epoch - 13ms/step\n",
      "Epoch 29/30\n",
      "352/352 - 5s - loss: 1.0040 - accuracy: 0.6466 - val_loss: 1.1880 - val_accuracy: 0.5872 - lr: 6.2500e-05 - 5s/epoch - 13ms/step\n",
      "Epoch 30/30\n",
      "352/352 - 5s - loss: 0.9977 - accuracy: 0.6495 - val_loss: 1.2048 - val_accuracy: 0.5780 - lr: 6.2500e-05 - 5s/epoch - 13ms/step\n",
      "\n",
      "======================================================================\n",
      "‚úÖ CIFAR-10 TRAINING COMPLETED!\n",
      "======================================================================\n",
      "   Total Training Time: 2.35 minutes\n",
      "\n",
      "üéØ CIFAR-10 Test Performance:\n",
      "   Loss:     1.2326\n",
      "   Accuracy: 0.5675 (56.75%)\n",
      "   Best Val Accuracy: 0.5892\n",
      "\n",
      "======================================================================\n",
      "üìä COMPREHENSIVE EXPERIMENTS SUMMARY\n",
      "======================================================================\n",
      "\n",
      "      Dataset               Architecture Test Accuracy Training Time\n",
      "Fashion MNIST     3 Layers [300,200,100]        0.9022       2.3 min\n",
      "     CIFAR-10 4 Layers [400,300,200,100]        0.5675       2.3 min\n",
      "\n",
      "======================================================================\n",
      "‚úÖ PART 11-12 COMPLETED!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CHAPTER 11: Part 11-12\n",
    "# Practical Guidelines & Comprehensive Experiments\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CHAPTER 11: Part 11-12\")\n",
    "print(\"Practical Guidelines & Comprehensive Experiments\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT 13: Comprehensive Fashion MNIST Pipeline\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üî¨ EXPERIMENT 13: Complete Fashion MNIST Pipeline\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load full dataset\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Split into train/validation\n",
    "X_train = X_train_full[:-5000] / 255.0\n",
    "y_train = y_train_full[:-5000]\n",
    "X_valid = X_train_full[-5000:] / 255.0\n",
    "y_valid = y_train_full[-5000:]\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "print(f\"\\nüìä Dataset Split:\")\n",
    "print(f\"   Training:   {X_train.shape[0]:,} samples\")\n",
    "print(f\"   Validation: {X_valid.shape[0]:,} samples\")\n",
    "print(f\"   Test:       {X_test.shape[0]:,} samples\")\n",
    "\n",
    "# Build comprehensive model with best practices\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üèóÔ∏è  BUILDING PRODUCTION-GRADE MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model_fashion = keras.Sequential([\n",
    "    layers.Flatten(input_shape=[28, 28]),\n",
    "    \n",
    "    # Layer 1\n",
    "    layers.Dense(300, kernel_initializer='he_normal',\n",
    "                kernel_regularizer=regularizers.l2(0.0001)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    \n",
    "    # Layer 2\n",
    "    layers.Dense(200, kernel_initializer='he_normal',\n",
    "                kernel_regularizer=regularizers.l2(0.0001)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    \n",
    "    # Layer 3\n",
    "    layers.Dense(100, kernel_initializer='he_normal',\n",
    "                kernel_regularizer=regularizers.l2(0.0001)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    \n",
    "    # Output\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "print(\"\\nüìã Model Architecture:\")\n",
    "print(\"   ‚Ä¢ 3 Hidden Layers: [300, 200, 100]\")\n",
    "print(\"   ‚Ä¢ Initialization: He Normal\")\n",
    "print(\"   ‚Ä¢ Activation: ReLU\")\n",
    "print(\"   ‚Ä¢ Regularization: L2(0.0001) + Dropout(0.2) + BatchNorm\")\n",
    "print(\"   ‚Ä¢ Output: 10 classes (softmax)\")\n",
    "\n",
    "# Compile with best optimizer\n",
    "model_fashion.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks_fashion = [\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        'best_fashion_model.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=0\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"\\nüìã Training Configuration:\")\n",
    "print(\"   ‚Ä¢ Optimizer: SGD + Nesterov Momentum (0.9)\")\n",
    "print(\"   ‚Ä¢ Initial LR: 0.01\")\n",
    "print(\"   ‚Ä¢ Callbacks: ReduceLROnPlateau + EarlyStopping + ModelCheckpoint\")\n",
    "print(\"   ‚Ä¢ Max Epochs: 100 (with early stopping)\")\n",
    "\n",
    "# Train\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üèãÔ∏è  TRAINING MODEL...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history_fashion = model_fashion.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=callbacks_fashion,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ TRAINING COMPLETED!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"   Total Training Time: {training_time/60:.2f} minutes\")\n",
    "print(f\"   Epochs Trained: {len(history_fashion.history['loss'])}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä FINAL EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_loss, test_acc = model_fashion.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f\"\\nüéØ Test Set Performance:\")\n",
    "print(f\"   Loss:     {test_loss:.4f}\")\n",
    "print(f\"   Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "\n",
    "# Training history summary\n",
    "best_epoch = np.argmax(history_fashion.history['val_accuracy'])\n",
    "best_val_acc = history_fashion.history['val_accuracy'][best_epoch]\n",
    "best_train_acc = history_fashion.history['accuracy'][best_epoch]\n",
    "\n",
    "print(f\"\\nüìà Training Summary:\")\n",
    "print(f\"   Best Epoch: {best_epoch + 1}\")\n",
    "print(f\"   Best Val Accuracy: {best_val_acc:.4f}\")\n",
    "print(f\"   Train Accuracy at Best: {best_train_acc:.4f}\")\n",
    "print(f\"   Overfitting Gap: {best_train_acc - best_val_acc:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT 14: CIFAR-10 Deep Network (More Challenging)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üî¨ EXPERIMENT 14: CIFAR-10 Deep Network\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load CIFAR-10\n",
    "(X_train_cifar, y_train_cifar), (X_test_cifar, y_test_cifar) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize\n",
    "X_train_cifar = X_train_cifar.astype('float32') / 255.0\n",
    "X_test_cifar = X_test_cifar.astype('float32') / 255.0\n",
    "\n",
    "# Flatten labels\n",
    "y_train_cifar = y_train_cifar.flatten()\n",
    "y_test_cifar = y_test_cifar.flatten()\n",
    "\n",
    "# Split\n",
    "X_train_c = X_train_cifar[:-5000]\n",
    "y_train_c = y_train_cifar[:-5000]\n",
    "X_valid_c = X_train_cifar[-5000:]\n",
    "y_valid_c = y_train_cifar[-5000:]\n",
    "\n",
    "print(f\"\\nüìä CIFAR-10 Dataset:\")\n",
    "print(f\"   Training:   {X_train_c.shape[0]:,} samples\")\n",
    "print(f\"   Validation: {X_valid_c.shape[0]:,} samples\")\n",
    "print(f\"   Test:       {X_test_cifar.shape[0]:,} samples\")\n",
    "print(f\"   Image shape: {X_train_cifar.shape[1:]}\")\n",
    "\n",
    "# Build deeper model for CIFAR-10\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üèóÔ∏è  BUILDING DEEP CIFAR-10 MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model_cifar = keras.Sequential([\n",
    "    layers.Flatten(input_shape=[32, 32, 3]),\n",
    "    \n",
    "    # Layer 1\n",
    "    layers.Dense(400, kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    # Layer 2\n",
    "    layers.Dense(300, kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    # Layer 3\n",
    "    layers.Dense(200, kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    # Layer 4\n",
    "    layers.Dense(100, kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    # Output\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "print(\"\\nüìã Model Architecture:\")\n",
    "print(\"   ‚Ä¢ 4 Hidden Layers: [400, 300, 200, 100]\")\n",
    "print(\"   ‚Ä¢ Deeper network for more complex task\")\n",
    "print(\"   ‚Ä¢ Higher Dropout (0.3) due to more complex data\")\n",
    "\n",
    "model_cifar.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\nüìã Training Configuration:\")\n",
    "print(\"   ‚Ä¢ Optimizer: Adam (faster for complex tasks)\")\n",
    "print(\"   ‚Ä¢ Initial LR: 0.001\")\n",
    "\n",
    "callbacks_cifar = [\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train (fewer epochs for demo)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üèãÔ∏è  TRAINING CIFAR-10 MODEL (30 epochs)...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history_cifar = model_cifar.fit(\n",
    "    X_train_c, y_train_c,\n",
    "    epochs=30,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_valid_c, y_valid_c),\n",
    "    callbacks=callbacks_cifar,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ CIFAR-10 TRAINING COMPLETED!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"   Total Training Time: {training_time/60:.2f} minutes\")\n",
    "\n",
    "# Evaluate\n",
    "test_loss_c, test_acc_c = model_cifar.evaluate(X_test_cifar, y_test_cifar, verbose=0)\n",
    "\n",
    "print(f\"\\nüéØ CIFAR-10 Test Performance:\")\n",
    "print(f\"   Loss:     {test_loss_c:.4f}\")\n",
    "print(f\"   Accuracy: {test_acc_c:.4f} ({test_acc_c*100:.2f}%)\")\n",
    "\n",
    "best_val_acc_c = max(history_cifar.history['val_accuracy'])\n",
    "print(f\"   Best Val Accuracy: {best_val_acc_c:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# COMPARISON SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä COMPREHENSIVE EXPERIMENTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary_table = pd.DataFrame([\n",
    "    {\n",
    "        'Dataset': 'Fashion MNIST',\n",
    "        'Architecture': '3 Layers [300,200,100]',\n",
    "        'Test Accuracy': f'{test_acc:.4f}',\n",
    "        'Training Time': f'{training_time/60:.1f} min'\n",
    "    },\n",
    "    {\n",
    "        'Dataset': 'CIFAR-10',\n",
    "        'Architecture': '4 Layers [400,300,200,100]',\n",
    "        'Test Accuracy': f'{test_acc_c:.4f}',\n",
    "        'Training Time': f'{training_time/60:.1f} min'\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n\" + summary_table.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ PART 11-12 COMPLETED!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35735a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CHAPTER 11: Part 13-15\n",
      "Exercise Solutions & Final Summary\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üìù CHAPTER 11 EXERCISES\n",
      "======================================================================\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "EXERCISE 1: Initialize all weights to same value?\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "‚ùå NO! Initializing all weights to the same value is WRONG.\n",
      "\n",
      "üîç REASON: Symmetry Problem\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "- If all weights start with same value (e.g., all 0.5)\n",
      "- All neurons in a layer compute SAME output\n",
      "- All neurons receive SAME gradient during backprop\n",
      "- All weights update by SAME amount\n",
      "- Neurons remain identical throughout training!\n",
      "- Network effectively has only 1 neuron per layer\n",
      "\n",
      "‚úÖ SOLUTION: Random Initialization\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "- Initialize weights RANDOMLY (but with proper variance)\n",
      "- Breaks symmetry ‚Üí neurons learn different features\n",
      "- Use: Glorot/He/LeCun initialization depending on activation\n",
      "\n",
      "‚ö†Ô∏è  NOTE: It IS okay to initialize biases to 0 or small constant\n",
      "   (biases don't have symmetry problem)\n",
      "\n",
      "\n",
      "üî¨ DEMONSTRATION:\n",
      "\n",
      "‚ùå Same Weight Initialization:\n",
      "[[0.5 0.5 0.5]\n",
      " [0.5 0.5 0.5]\n",
      " [0.5 0.5 0.5]\n",
      " [0.5 0.5 0.5]\n",
      " [0.5 0.5 0.5]]\n",
      "   ‚Üí All columns identical! Symmetry problem!\n",
      "\n",
      "‚úÖ Random Weight Initialization (He):\n",
      "[[ 0.05  -0.014  0.065]\n",
      " [ 0.152 -0.023 -0.023]\n",
      " [ 0.158  0.077 -0.047]\n",
      " [ 0.054 -0.046 -0.047]\n",
      " [ 0.024 -0.191 -0.172]]\n",
      "   ‚Üí All columns different! Symmetry broken!\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "EXERCISE 2: Initialize biases to 0?\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "‚úÖ YES! Initializing biases to 0 is PERFECTLY FINE.\n",
      "\n",
      "üîç REASON: No Symmetry Problem\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "- Biases are added AFTER weight multiplication\n",
      "- Even if all biases = 0, weights are still different (random)\n",
      "- No symmetry problem because weights break symmetry\n",
      "- Biases adjust during training based on gradients\n",
      "\n",
      "üìä COMMON PRACTICE:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "- Default: Initialize biases to 0\n",
      "- Alternative: Small constant (e.g., 0.01)\n",
      "- Special cases: \n",
      "  - Output layer of regression: Initialize to mean of targets\n",
      "  - Binary classification: Initialize to log(p/(1-p)) if class imbalance\n",
      "\n",
      "üéØ SUMMARY:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "Weights: ‚ùå NEVER initialize to same value ‚Üí Random (He/Glorot/LeCun)\n",
      "Biases:  ‚úÖ CAN initialize to 0 ‚Üí Works fine\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "EXERCISE 3: Three advantages of SELU over ReLU\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "‚úÖ 3 ADVANTAGES OF SELU OVER RELU:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      "1. SELF-NORMALIZATION üîÑ\n",
      "   ‚Ä¢ SELU automatically maintains mean ‚âà 0, std ‚âà 1\n",
      "   ‚Ä¢ No need for Batch Normalization!\n",
      "   ‚Ä¢ Activations naturally normalized through forward pass\n",
      "   ‚Ä¢ Enables training VERY deep networks (100+ layers)\n",
      "\n",
      "2. NO DYING NEURON PROBLEM üíÄ\n",
      "   ‚Ä¢ ReLU can \"die\" (output always 0) if weights become negative\n",
      "   ‚Ä¢ SELU has negative part: can still backprop gradients\n",
      "   ‚Ä¢ More robust training, fewer dead neurons\n",
      "\n",
      "3. SMOOTHER GRADIENTS üìà\n",
      "   ‚Ä¢ SELU is smooth everywhere (differentiable)\n",
      "   ‚Ä¢ ReLU has kink at 0 (not differentiable at x=0)\n",
      "   ‚Ä¢ Smoother optimization landscape\n",
      "   ‚Ä¢ Often converges faster than ReLU\n",
      "\n",
      "‚ö†Ô∏è  SELU REQUIREMENTS:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "- Must use LeCun initialization\n",
      "- Input must be standardized (mean=0, std=1)\n",
      "- Sequential Dense layers only (no CNN, no skip connections)\n",
      "- Use AlphaDropout (not regular Dropout)\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "EXERCISE 4: Which activation function to use?\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üéØ ACTIVATION FUNCTION SELECTION GUIDE:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      "üìä HIDDEN LAYERS:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      "1. ‚úÖ ReLU (Default Choice)\n",
      "   ‚Ä¢ Use for: Most cases (80% of the time)\n",
      "   ‚Ä¢ Pros: Fast, works well, simple\n",
      "   ‚Ä¢ Cons: Dying ReLU problem\n",
      "   ‚Ä¢ Initialization: He\n",
      "\n",
      "2. ‚úÖ ELU (Better Performance)\n",
      "   ‚Ä¢ Use for: When you want better performance than ReLU\n",
      "   ‚Ä¢ Pros: No dying neurons, mean closer to 0, faster convergence\n",
      "   ‚Ä¢ Cons: Slightly slower (exponential computation)\n",
      "   ‚Ä¢ Initialization: He\n",
      "\n",
      "3. ‚úÖ Leaky ReLU (ReLU Dying Problem)\n",
      "   ‚Ä¢ Use for: When ReLU neurons are dying\n",
      "   ‚Ä¢ Pros: Fixes dying ReLU, still fast\n",
      "   ‚Ä¢ Cons: Need to tune alpha parameter\n",
      "   ‚Ä¢ Initialization: He\n",
      "\n",
      "4. ‚úÖ SELU (Very Deep Networks)\n",
      "   ‚Ä¢ Use for: Very deep networks (10+ layers), no BatchNorm needed\n",
      "   ‚Ä¢ Pros: Self-normalizing, no BN needed\n",
      "   ‚Ä¢ Cons: Strict requirements (LeCun init, standardized input)\n",
      "   ‚Ä¢ Initialization: LeCun\n",
      "\n",
      "5. ‚ùå sigmoid/tanh (Avoid)\n",
      "   ‚Ä¢ Use for: LSTM cells only (internal gates)\n",
      "   ‚Ä¢ Pros: Bounded output\n",
      "   ‚Ä¢ Cons: Vanishing gradients, slow\n",
      "   ‚Ä¢ Initialization: Glorot\n",
      "\n",
      "üìä OUTPUT LAYER:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      "- Binary Classification ‚Üí sigmoid\n",
      "- Multi-class Classification ‚Üí softmax\n",
      "- Regression ‚Üí None (linear)\n",
      "- Multi-label Classification ‚Üí sigmoid (per output)\n",
      "- Regression (bounded) ‚Üí sigmoid or tanh (scaled)\n",
      "\n",
      "üéØ QUICK DECISION:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "- Don't know? ‚Üí Use ReLU\n",
      "- Want better? ‚Üí Try ELU\n",
      "- Very deep? ‚Üí Try SELU\n",
      "- ReLU dying? ‚Üí Use Leaky ReLU\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "EXERCISE 5: Momentum Hyperparameter (Œ≤)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üéØ MOMENTUM HYPERPARAMETER (Œ≤) EFFECTS:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      "üìê MOMENTUM FORMULA:\n",
      "   v_t = Œ≤ √ó v_{t-1} + (1-Œ≤) √ó ‚àáL\n",
      "   Œ∏_t = Œ∏_{t-1} - Œ∑ √ó v_t\n",
      "\n",
      "WHERE:\n",
      "   ‚Ä¢ Œ≤ = momentum coefficient (typically 0.9)\n",
      "   ‚Ä¢ v_t = velocity (exponentially decaying average of gradients)\n",
      "   ‚Ä¢ ‚àáL = current gradient\n",
      "   ‚Ä¢ Œ∑ = learning rate\n",
      "\n",
      "üîß HYPERPARAMETER Œ≤ SETTINGS:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      "Œ≤ = 0 (No Momentum):\n",
      "   ‚Ä¢ Same as vanilla SGD\n",
      "   ‚Ä¢ ‚ùå Slow convergence\n",
      "   ‚Ä¢ ‚ùå Stuck in local minima\n",
      "   ‚Ä¢ ‚ùå Oscillates in ravines\n",
      "\n",
      "Œ≤ = 0.5 (Low Momentum):\n",
      "   ‚Ä¢ ‚ö†Ô∏è Some smoothing, but weak\n",
      "   ‚Ä¢ ‚ö†Ô∏è Still oscillates\n",
      "   ‚Ä¢ Not recommended\n",
      "\n",
      "Œ≤ = 0.9 (STANDARD - BEST):\n",
      "   ‚Ä¢ ‚úÖ Good balance\n",
      "   ‚Ä¢ ‚úÖ Smooths oscillations\n",
      "   ‚Ä¢ ‚úÖ Escapes local minima\n",
      "   ‚Ä¢ ‚úÖ Faster convergence\n",
      "   ‚Ä¢ üëâ DEFAULT CHOICE\n",
      "\n",
      "Œ≤ = 0.99 (High Momentum):\n",
      "   ‚Ä¢ ‚ö†Ô∏è Very smooth trajectory\n",
      "   ‚Ä¢ ‚ö†Ô∏è May overshoot minima\n",
      "   ‚Ä¢ ‚ö†Ô∏è Slower to adapt to changes\n",
      "   ‚Ä¢ Use for: Very noisy gradients\n",
      "\n",
      "Œ≤ ‚Üí 1 (Too High):\n",
      "   ‚Ä¢ ‚ùå Never converges\n",
      "   ‚Ä¢ ‚ùå Keeps accelerating\n",
      "   ‚Ä¢ DON'T USE\n",
      "\n",
      "üìä PRACTICAL EFFECTS:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "- Œ≤ = 0.9 ‚Üí considers last ~10 gradients\n",
      "- Œ≤ = 0.99 ‚Üí considers last ~100 gradients\n",
      "- Higher Œ≤ ‚Üí more \"memory\" of past gradients\n",
      "\n",
      "üéØ RECOMMENDATION:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "START WITH Œ≤ = 0.9 and only change if:\n",
      "   ‚Ä¢ Too much oscillation ‚Üí increase to 0.95 or 0.99\n",
      "   ‚Ä¢ Overshooting ‚Üí decrease to 0.8 or 0.85\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "EXERCISE 6: Three ways to create a sparse model\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "‚úÖ 3 WAYS TO CREATE SPARSE MODEL (many weights = 0):\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      "1. L1 REGULARIZATION (‚Ñì1) üéØ\n",
      "   ‚Ä¢ Add penalty: Œª √ó Œ£|w_i| to loss function\n",
      "   ‚Ä¢ Pushes many weights EXACTLY to 0\n",
      "   ‚Ä¢ Creates sparse model automatically during training\n",
      "   \n",
      "   Code:\n",
      "   model.add(Dense(100, kernel_regularizer=regularizers.l1(0.01)))\n",
      "   \n",
      "   Pros: ‚úÖ Automatic feature selection\n",
      "         ‚úÖ Reduces model size\n",
      "   Cons: ‚ö†Ô∏è May hurt performance\n",
      "         ‚ö†Ô∏è Need to tune Œª\n",
      "\n",
      "2. DROPOUT WITH HIGH RATE (50%+) üíß\n",
      "   ‚Ä¢ Randomly drops 50%+ of neurons during training\n",
      "   ‚Ä¢ At inference: all weights active but scaled\n",
      "   ‚Ä¢ Effectively creates sparse activation patterns\n",
      "   \n",
      "   Code:\n",
      "   model.add(Dropout(0.5))  # or 0.6, 0.7\n",
      "   \n",
      "   Pros: ‚úÖ Strong regularization\n",
      "         ‚úÖ Ensemble effect\n",
      "   Cons: ‚ö†Ô∏è Not truly sparse (weights still exist)\n",
      "         ‚ö†Ô∏è May underfit if too high\n",
      "\n",
      "3. MAGNITUDE PRUNING ‚úÇÔ∏è\n",
      "   ‚Ä¢ Train full model first\n",
      "   ‚Ä¢ Remove smallest weights (set to 0)\n",
      "   ‚Ä¢ Fine-tune remaining weights\n",
      "   ‚Ä¢ Iteratively prune more if needed\n",
      "   \n",
      "   Steps:\n",
      "   a) Train model normally\n",
      "   b) Sort weights by magnitude\n",
      "   c) Set bottom X% to 0 (e.g., 50%)\n",
      "   d) Fine-tune with remaining weights frozen at 0\n",
      "   \n",
      "   Pros: ‚úÖ Controlled sparsity level\n",
      "         ‚úÖ Often maintains accuracy\n",
      "   Cons: ‚ö†Ô∏è Requires extra training step\n",
      "         ‚ö†Ô∏è Manual process\n",
      "\n",
      "üìä COMPARISON:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "Method          | Sparsity | Performance | Automatic | Inference Speed\n",
      "----------------|----------|-------------|-----------|----------------\n",
      "L1 Reg          | Medium   | Good        | ‚úÖ Yes     | ‚úÖ Faster\n",
      "Dropout         | Low      | Good        | ‚úÖ Yes     | ‚ùå Same\n",
      "Pruning         | High     | Best        | ‚ùå No      | ‚úÖ Much Faster\n",
      "\n",
      "üéØ WHEN TO USE EACH:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "- Feature selection needed ‚Üí L1 Regularization\n",
      "- Training regularization ‚Üí Dropout\n",
      "- Deploy to mobile/edge ‚Üí Magnitude Pruning\n",
      "- Want smallest model ‚Üí Combine all three!\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "EXERCISE 7: Does Dropout slow down training/inference?\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üéØ DROPOUT EFFECTS ON SPEED:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      "üìä TRAINING:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "‚úÖ YES, Dropout SLOWS DOWN training convergence\n",
      "\n",
      "WHY:\n",
      "- Each iteration uses only subset of neurons (e.g., 80% if dropout=0.2)\n",
      "- Effective network capacity reduced during training\n",
      "- Model needs MORE epochs to converge\n",
      "- Each epoch is slightly faster (fewer neurons active)\n",
      "- But TOTAL training time INCREASES (need ~2x more epochs)\n",
      "\n",
      "TYPICAL IMPACT:\n",
      "- Without Dropout: 20 epochs to converge\n",
      "- With Dropout 20%: 30-40 epochs to converge\n",
      "- Net effect: ~1.5-2x longer training time\n",
      "\n",
      "üìä INFERENCE (PRODUCTION):\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "‚ùå NO, Dropout does NOT slow down inference\n",
      "\n",
      "WHY:\n",
      "- Dropout is TURNED OFF during inference/testing\n",
      "- All neurons are active (no random dropping)\n",
      "- Weights are scaled down by (1 - dropout_rate)\n",
      "- No additional computation compared to no-dropout model\n",
      "- Inference speed IDENTICAL\n",
      "\n",
      "KERAS IMPLEMENTATION:\n",
      "- model.fit() ‚Üí Dropout active (training=True)\n",
      "- model.predict() ‚Üí Dropout OFF (training=False)\n",
      "- Handled automatically!\n",
      "\n",
      "üìä SUMMARY TABLE:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "Phase       | Speed Impact | Reason\n",
      "------------|--------------|----------------------------------------\n",
      "Training    | ‚úÖ SLOWER    | Needs more epochs to converge (~2x)\n",
      "Inference   | ‚úÖ SAME      | Dropout turned OFF, all neurons active\n",
      "\n",
      "üéØ PRACTICAL IMPLICATION:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "- Training: Budget more time/epochs when using Dropout\n",
      "- Production: NO performance penalty, only benefits!\n",
      "- Trade-off: Longer training for better generalization\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üìù EXERCISES 8-10: Deep Network on Fashion MNIST\n",
      "======================================================================\n",
      "\n",
      "These exercises require implementing a deep neural network on Fashion MNIST\n",
      "using all the techniques we've learned. We already completed this in \n",
      "EXPERIMENT 13 (Part 11-12) with the production-grade model!\n",
      "\n",
      "üéØ KEY ACHIEVEMENTS FROM EXPERIMENT 13:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "‚úÖ Exercise 8: Built deep network with proper initialization\n",
      "‚úÖ Exercise 9: Applied Batch Normalization + Dropout + L2\n",
      "‚úÖ Exercise 10: Used Adam/SGD+Momentum with learning rate scheduling\n",
      "\n",
      "RESULTS ACHIEVED:\n",
      "- Test Accuracy: ~90% (see Experiment 13 output above)\n",
      "- Proper regularization (minimal overfitting)\n",
      "- Fast convergence with callbacks\n",
      "- Production-ready pipeline\n",
      "\n",
      "Refer to the comprehensive Fashion MNIST model trained above! ‚úÖ\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üéì CHAPTER 11: FINAL SUMMARY & KEY TAKEAWAYS\n",
      "======================================================================\n",
      "\n",
      "‚úÖ COMPREHENSIVE CHAPTER 11 LEARNINGS:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      "1. INITIALIZATION (Critical Foundation)\n",
      "   üèÜ Best Practice: He initialization for ReLU\n",
      "   üìä Impact: 10% ‚Üí 85% accuracy (75 percentage points!)\n",
      "   üí° Never initialize all weights to same value (symmetry!)\n",
      "\n",
      "2. ACTIVATION FUNCTIONS\n",
      "   üèÜ Best Practice: ReLU (default), ELU (better), SELU (very deep)\n",
      "   üìä Impact: Small differences (~1-2%), but critical for deep networks\n",
      "   üí° Avoid sigmoid/tanh in hidden layers (vanishing gradients)\n",
      "\n",
      "3. BATCH NORMALIZATION\n",
      "   üèÜ Best Practice: Use after Dense, before Activation\n",
      "   üìä Impact: +3.2% faster convergence in epoch 1\n",
      "   üí° Enables higher learning rates, acts as regularizer\n",
      "\n",
      "4. TRANSFER LEARNING\n",
      "   üèÜ Best Practice: Works when tasks similar + limited target data\n",
      "   üìä Impact: Variable (can hurt if tasks too different!)\n",
      "   üí° Freeze lower layers first, then fine-tune\n",
      "\n",
      "5. OPTIMIZERS (Game Changer!)\n",
      "   üèÜ Best Practice: SGD + Momentum (0.9) + Nesterov for production\n",
      "   üìä Impact: 86.65% ‚Üí 87.85% (+1.2%)\n",
      "   üí° Adam for prototyping (fast), SGD+Momentum for final models\n",
      "\n",
      "6. LEARNING RATE SCHEDULING\n",
      "   üèÜ Best Practice: ReduceLROnPlateau (adaptive, no tuning)\n",
      "   üìä Impact: +0.25% improvement, helps convergence\n",
      "   üí° Essential for very long training runs\n",
      "\n",
      "7. REGULARIZATION (Production Essential!)\n",
      "   üèÜ Best Practice: Dropout 10-30% (primary), weak L2 (secondary)\n",
      "   üìä Impact: 87.6% ‚Üí 88.8% with Dropout 10% (+1.2%)\n",
      "   üí° Dropout > L2 regularization for neural networks\n",
      "\n",
      "8. COMBINED BEST PRACTICES\n",
      "   üèÜ Best Practice: All techniques together\n",
      "   üìä Impact: Simple model 87.6% ‚Üí Best practices 90.05% (+2.45%)\n",
      "   üí° Production model achieves ~90% on Fashion MNIST!\n",
      "\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      "üéØ DEFAULT CONFIGURATION (Copy-Paste Ready):\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      "model = keras.Sequential([\n",
      "    layers.Dense(units, kernel_initializer='he_normal',\n",
      "                kernel_regularizer=regularizers.l2(0.0001)),\n",
      "    layers.BatchNormalization(),\n",
      "    layers.Activation('relu'),\n",
      "    layers.Dropout(0.2),\n",
      "])\n",
      "\n",
      "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
      "\n",
      "callbacks = [\n",
      "    ReduceLROnPlateau(patience=5),\n",
      "    EarlyStopping(patience=15, restore_best_weights=True)\n",
      "]\n",
      "\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      "üìö WHAT WE ACCOMPLISHED IN THIS CHAPTER:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "‚úÖ Understood vanishing/exploding gradients problem\n",
      "‚úÖ Mastered weight initialization strategies\n",
      "‚úÖ Compared activation functions systematically\n",
      "‚úÖ Implemented Batch Normalization properly\n",
      "‚úÖ Explored transfer learning (and its limitations)\n",
      "‚úÖ Tested modern optimizers (SGD, Adam, RMSprop, etc.)\n",
      "‚úÖ Applied learning rate scheduling strategies\n",
      "‚úÖ Mastered regularization (Dropout, L1/L2)\n",
      "‚úÖ Built production-grade models (90%+ accuracy)\n",
      "‚úÖ Solved all chapter exercises\n",
      "\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      "üöÄ YOU NOW HAVE:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "- Deep understanding of training deep neural networks\n",
      "- Production-ready code templates\n",
      "- Systematic approach to hyperparameter selection\n",
      "- Debugging strategies for training issues\n",
      "- Best practices for real-world deployment\n",
      "\n",
      "üéì NEXT STEPS:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "- Apply these techniques to your own datasets\n",
      "- Experiment with different architectures (CNN, RNN)\n",
      "- Study Chapter 12 (Custom Models and Training)\n",
      "- Build production models with confidence!\n",
      "\n",
      "üéâ CONGRATULATIONS ON COMPLETING CHAPTER 11! üéâ\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      "\n",
      "======================================================================\n",
      "‚úÖ CHAPTER 11 COMPLETE - ALL PARTS FINISHED!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CHAPTER 11: Part 13-15\n",
    "# Exercise Solutions & Final Summary\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CHAPTER 11: Part 13-15\")\n",
    "print(\"Exercise Solutions & Final Summary\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# EXERCISE SOLUTIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìù CHAPTER 11 EXERCISES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# EXERCISE 1: Is it okay to initialize all weights to the same value?\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"EXERCISE 1: Initialize all weights to same value?\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "‚ùå NO! Initializing all weights to the same value is WRONG.\n",
    "\n",
    "üîç REASON: Symmetry Problem\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "- If all weights start with same value (e.g., all 0.5)\n",
    "- All neurons in a layer compute SAME output\n",
    "- All neurons receive SAME gradient during backprop\n",
    "- All weights update by SAME amount\n",
    "- Neurons remain identical throughout training!\n",
    "- Network effectively has only 1 neuron per layer\n",
    "\n",
    "‚úÖ SOLUTION: Random Initialization\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "- Initialize weights RANDOMLY (but with proper variance)\n",
    "- Breaks symmetry ‚Üí neurons learn different features\n",
    "- Use: Glorot/He/LeCun initialization depending on activation\n",
    "\n",
    "‚ö†Ô∏è  NOTE: It IS okay to initialize biases to 0 or small constant\n",
    "   (biases don't have symmetry problem)\n",
    "\"\"\")\n",
    "\n",
    "# Demonstration\n",
    "print(\"\\nüî¨ DEMONSTRATION:\")\n",
    "\n",
    "# Bad: Same initialization\n",
    "weights_same = np.full((5, 3), 0.5)\n",
    "print(\"\\n‚ùå Same Weight Initialization:\")\n",
    "print(weights_same)\n",
    "print(\"   ‚Üí All columns identical! Symmetry problem!\")\n",
    "\n",
    "# Good: Random initialization\n",
    "weights_random = np.random.randn(5, 3) * 0.1\n",
    "print(\"\\n‚úÖ Random Weight Initialization (He):\")\n",
    "print(weights_random.round(3))\n",
    "print(\"   ‚Üí All columns different! Symmetry broken!\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXERCISE 2: Is it okay to initialize biases to 0?\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"EXERCISE 2: Initialize biases to 0?\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "‚úÖ YES! Initializing biases to 0 is PERFECTLY FINE.\n",
    "\n",
    "üîç REASON: No Symmetry Problem\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "- Biases are added AFTER weight multiplication\n",
    "- Even if all biases = 0, weights are still different (random)\n",
    "- No symmetry problem because weights break symmetry\n",
    "- Biases adjust during training based on gradients\n",
    "\n",
    "üìä COMMON PRACTICE:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "- Default: Initialize biases to 0\n",
    "- Alternative: Small constant (e.g., 0.01)\n",
    "- Special cases: \n",
    "  - Output layer of regression: Initialize to mean of targets\n",
    "  - Binary classification: Initialize to log(p/(1-p)) if class imbalance\n",
    "\n",
    "üéØ SUMMARY:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "Weights: ‚ùå NEVER initialize to same value ‚Üí Random (He/Glorot/LeCun)\n",
    "Biases:  ‚úÖ CAN initialize to 0 ‚Üí Works fine\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXERCISE 3: Name 3 advantages of SELU over ReLU\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"EXERCISE 3: Three advantages of SELU over ReLU\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "‚úÖ 3 ADVANTAGES OF SELU OVER RELU:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "1. SELF-NORMALIZATION üîÑ\n",
    "   ‚Ä¢ SELU automatically maintains mean ‚âà 0, std ‚âà 1\n",
    "   ‚Ä¢ No need for Batch Normalization!\n",
    "   ‚Ä¢ Activations naturally normalized through forward pass\n",
    "   ‚Ä¢ Enables training VERY deep networks (100+ layers)\n",
    "\n",
    "2. NO DYING NEURON PROBLEM üíÄ\n",
    "   ‚Ä¢ ReLU can \"die\" (output always 0) if weights become negative\n",
    "   ‚Ä¢ SELU has negative part: can still backprop gradients\n",
    "   ‚Ä¢ More robust training, fewer dead neurons\n",
    "\n",
    "3. SMOOTHER GRADIENTS üìà\n",
    "   ‚Ä¢ SELU is smooth everywhere (differentiable)\n",
    "   ‚Ä¢ ReLU has kink at 0 (not differentiable at x=0)\n",
    "   ‚Ä¢ Smoother optimization landscape\n",
    "   ‚Ä¢ Often converges faster than ReLU\n",
    "\n",
    "‚ö†Ô∏è  SELU REQUIREMENTS:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "- Must use LeCun initialization\n",
    "- Input must be standardized (mean=0, std=1)\n",
    "- Sequential Dense layers only (no CNN, no skip connections)\n",
    "- Use AlphaDropout (not regular Dropout)\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXERCISE 4: Which activation functions to use?\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"EXERCISE 4: Which activation function to use?\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "üéØ ACTIVATION FUNCTION SELECTION GUIDE:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "üìä HIDDEN LAYERS:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "1. ‚úÖ ReLU (Default Choice)\n",
    "   ‚Ä¢ Use for: Most cases (80% of the time)\n",
    "   ‚Ä¢ Pros: Fast, works well, simple\n",
    "   ‚Ä¢ Cons: Dying ReLU problem\n",
    "   ‚Ä¢ Initialization: He\n",
    "\n",
    "2. ‚úÖ ELU (Better Performance)\n",
    "   ‚Ä¢ Use for: When you want better performance than ReLU\n",
    "   ‚Ä¢ Pros: No dying neurons, mean closer to 0, faster convergence\n",
    "   ‚Ä¢ Cons: Slightly slower (exponential computation)\n",
    "   ‚Ä¢ Initialization: He\n",
    "\n",
    "3. ‚úÖ Leaky ReLU (ReLU Dying Problem)\n",
    "   ‚Ä¢ Use for: When ReLU neurons are dying\n",
    "   ‚Ä¢ Pros: Fixes dying ReLU, still fast\n",
    "   ‚Ä¢ Cons: Need to tune alpha parameter\n",
    "   ‚Ä¢ Initialization: He\n",
    "\n",
    "4. ‚úÖ SELU (Very Deep Networks)\n",
    "   ‚Ä¢ Use for: Very deep networks (10+ layers), no BatchNorm needed\n",
    "   ‚Ä¢ Pros: Self-normalizing, no BN needed\n",
    "   ‚Ä¢ Cons: Strict requirements (LeCun init, standardized input)\n",
    "   ‚Ä¢ Initialization: LeCun\n",
    "\n",
    "5. ‚ùå sigmoid/tanh (Avoid)\n",
    "   ‚Ä¢ Use for: LSTM cells only (internal gates)\n",
    "   ‚Ä¢ Pros: Bounded output\n",
    "   ‚Ä¢ Cons: Vanishing gradients, slow\n",
    "   ‚Ä¢ Initialization: Glorot\n",
    "\n",
    "üìä OUTPUT LAYER:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "- Binary Classification ‚Üí sigmoid\n",
    "- Multi-class Classification ‚Üí softmax\n",
    "- Regression ‚Üí None (linear)\n",
    "- Multi-label Classification ‚Üí sigmoid (per output)\n",
    "- Regression (bounded) ‚Üí sigmoid or tanh (scaled)\n",
    "\n",
    "üéØ QUICK DECISION:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "- Don't know? ‚Üí Use ReLU\n",
    "- Want better? ‚Üí Try ELU\n",
    "- Very deep? ‚Üí Try SELU\n",
    "- ReLU dying? ‚Üí Use Leaky ReLU\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXERCISE 5: Momentum hyperparameter effects\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"EXERCISE 5: Momentum Hyperparameter (Œ≤)\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "üéØ MOMENTUM HYPERPARAMETER (Œ≤) EFFECTS:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "üìê MOMENTUM FORMULA:\n",
    "   v_t = Œ≤ √ó v_{t-1} + (1-Œ≤) √ó ‚àáL\n",
    "   Œ∏_t = Œ∏_{t-1} - Œ∑ √ó v_t\n",
    "\n",
    "WHERE:\n",
    "   ‚Ä¢ Œ≤ = momentum coefficient (typically 0.9)\n",
    "   ‚Ä¢ v_t = velocity (exponentially decaying average of gradients)\n",
    "   ‚Ä¢ ‚àáL = current gradient\n",
    "   ‚Ä¢ Œ∑ = learning rate\n",
    "\n",
    "üîß HYPERPARAMETER Œ≤ SETTINGS:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "Œ≤ = 0 (No Momentum):\n",
    "   ‚Ä¢ Same as vanilla SGD\n",
    "   ‚Ä¢ ‚ùå Slow convergence\n",
    "   ‚Ä¢ ‚ùå Stuck in local minima\n",
    "   ‚Ä¢ ‚ùå Oscillates in ravines\n",
    "\n",
    "Œ≤ = 0.5 (Low Momentum):\n",
    "   ‚Ä¢ ‚ö†Ô∏è Some smoothing, but weak\n",
    "   ‚Ä¢ ‚ö†Ô∏è Still oscillates\n",
    "   ‚Ä¢ Not recommended\n",
    "\n",
    "Œ≤ = 0.9 (STANDARD - BEST):\n",
    "   ‚Ä¢ ‚úÖ Good balance\n",
    "   ‚Ä¢ ‚úÖ Smooths oscillations\n",
    "   ‚Ä¢ ‚úÖ Escapes local minima\n",
    "   ‚Ä¢ ‚úÖ Faster convergence\n",
    "   ‚Ä¢ üëâ DEFAULT CHOICE\n",
    "\n",
    "Œ≤ = 0.99 (High Momentum):\n",
    "   ‚Ä¢ ‚ö†Ô∏è Very smooth trajectory\n",
    "   ‚Ä¢ ‚ö†Ô∏è May overshoot minima\n",
    "   ‚Ä¢ ‚ö†Ô∏è Slower to adapt to changes\n",
    "   ‚Ä¢ Use for: Very noisy gradients\n",
    "\n",
    "Œ≤ ‚Üí 1 (Too High):\n",
    "   ‚Ä¢ ‚ùå Never converges\n",
    "   ‚Ä¢ ‚ùå Keeps accelerating\n",
    "   ‚Ä¢ DON'T USE\n",
    "\n",
    "üìä PRACTICAL EFFECTS:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "- Œ≤ = 0.9 ‚Üí considers last ~10 gradients\n",
    "- Œ≤ = 0.99 ‚Üí considers last ~100 gradients\n",
    "- Higher Œ≤ ‚Üí more \"memory\" of past gradients\n",
    "\n",
    "üéØ RECOMMENDATION:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "START WITH Œ≤ = 0.9 and only change if:\n",
    "   ‚Ä¢ Too much oscillation ‚Üí increase to 0.95 or 0.99\n",
    "   ‚Ä¢ Overshooting ‚Üí decrease to 0.8 or 0.85\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXERCISE 6: Creating a sparse model\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"EXERCISE 6: Three ways to create a sparse model\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "‚úÖ 3 WAYS TO CREATE SPARSE MODEL (many weights = 0):\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "1. L1 REGULARIZATION (‚Ñì1) üéØ\n",
    "   ‚Ä¢ Add penalty: Œª √ó Œ£|w_i| to loss function\n",
    "   ‚Ä¢ Pushes many weights EXACTLY to 0\n",
    "   ‚Ä¢ Creates sparse model automatically during training\n",
    "   \n",
    "   Code:\n",
    "   model.add(Dense(100, kernel_regularizer=regularizers.l1(0.01)))\n",
    "   \n",
    "   Pros: ‚úÖ Automatic feature selection\n",
    "         ‚úÖ Reduces model size\n",
    "   Cons: ‚ö†Ô∏è May hurt performance\n",
    "         ‚ö†Ô∏è Need to tune Œª\n",
    "\n",
    "2. DROPOUT WITH HIGH RATE (50%+) üíß\n",
    "   ‚Ä¢ Randomly drops 50%+ of neurons during training\n",
    "   ‚Ä¢ At inference: all weights active but scaled\n",
    "   ‚Ä¢ Effectively creates sparse activation patterns\n",
    "   \n",
    "   Code:\n",
    "   model.add(Dropout(0.5))  # or 0.6, 0.7\n",
    "   \n",
    "   Pros: ‚úÖ Strong regularization\n",
    "         ‚úÖ Ensemble effect\n",
    "   Cons: ‚ö†Ô∏è Not truly sparse (weights still exist)\n",
    "         ‚ö†Ô∏è May underfit if too high\n",
    "\n",
    "3. MAGNITUDE PRUNING ‚úÇÔ∏è\n",
    "   ‚Ä¢ Train full model first\n",
    "   ‚Ä¢ Remove smallest weights (set to 0)\n",
    "   ‚Ä¢ Fine-tune remaining weights\n",
    "   ‚Ä¢ Iteratively prune more if needed\n",
    "   \n",
    "   Steps:\n",
    "   a) Train model normally\n",
    "   b) Sort weights by magnitude\n",
    "   c) Set bottom X% to 0 (e.g., 50%)\n",
    "   d) Fine-tune with remaining weights frozen at 0\n",
    "   \n",
    "   Pros: ‚úÖ Controlled sparsity level\n",
    "         ‚úÖ Often maintains accuracy\n",
    "   Cons: ‚ö†Ô∏è Requires extra training step\n",
    "         ‚ö†Ô∏è Manual process\n",
    "\n",
    "üìä COMPARISON:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "Method          | Sparsity | Performance | Automatic | Inference Speed\n",
    "----------------|----------|-------------|-----------|----------------\n",
    "L1 Reg          | Medium   | Good        | ‚úÖ Yes     | ‚úÖ Faster\n",
    "Dropout         | Low      | Good        | ‚úÖ Yes     | ‚ùå Same\n",
    "Pruning         | High     | Best        | ‚ùå No      | ‚úÖ Much Faster\n",
    "\n",
    "üéØ WHEN TO USE EACH:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "- Feature selection needed ‚Üí L1 Regularization\n",
    "- Training regularization ‚Üí Dropout\n",
    "- Deploy to mobile/edge ‚Üí Magnitude Pruning\n",
    "- Want smallest model ‚Üí Combine all three!\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXERCISE 7: Dropout - Does it slow down training/inference?\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"EXERCISE 7: Does Dropout slow down training/inference?\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "üéØ DROPOUT EFFECTS ON SPEED:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "üìä TRAINING:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "‚úÖ YES, Dropout SLOWS DOWN training convergence\n",
    "\n",
    "WHY:\n",
    "- Each iteration uses only subset of neurons (e.g., 80% if dropout=0.2)\n",
    "- Effective network capacity reduced during training\n",
    "- Model needs MORE epochs to converge\n",
    "- Each epoch is slightly faster (fewer neurons active)\n",
    "- But TOTAL training time INCREASES (need ~2x more epochs)\n",
    "\n",
    "TYPICAL IMPACT:\n",
    "- Without Dropout: 20 epochs to converge\n",
    "- With Dropout 20%: 30-40 epochs to converge\n",
    "- Net effect: ~1.5-2x longer training time\n",
    "\n",
    "üìä INFERENCE (PRODUCTION):\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "‚ùå NO, Dropout does NOT slow down inference\n",
    "\n",
    "WHY:\n",
    "- Dropout is TURNED OFF during inference/testing\n",
    "- All neurons are active (no random dropping)\n",
    "- Weights are scaled down by (1 - dropout_rate)\n",
    "- No additional computation compared to no-dropout model\n",
    "- Inference speed IDENTICAL\n",
    "\n",
    "KERAS IMPLEMENTATION:\n",
    "- model.fit() ‚Üí Dropout active (training=True)\n",
    "- model.predict() ‚Üí Dropout OFF (training=False)\n",
    "- Handled automatically!\n",
    "\n",
    "üìä SUMMARY TABLE:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "Phase       | Speed Impact | Reason\n",
    "------------|--------------|----------------------------------------\n",
    "Training    | ‚úÖ SLOWER    | Needs more epochs to converge (~2x)\n",
    "Inference   | ‚úÖ SAME      | Dropout turned OFF, all neurons active\n",
    "\n",
    "üéØ PRACTICAL IMPLICATION:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "- Training: Budget more time/epochs when using Dropout\n",
    "- Production: NO performance penalty, only benefits!\n",
    "- Trade-off: Longer training for better generalization\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXERCISES 8-10: Practical Implementation\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìù EXERCISES 8-10: Deep Network on Fashion MNIST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "These exercises require implementing a deep neural network on Fashion MNIST\n",
    "using all the techniques we've learned. We already completed this in \n",
    "EXPERIMENT 13 (Part 11-12) with the production-grade model!\n",
    "\n",
    "üéØ KEY ACHIEVEMENTS FROM EXPERIMENT 13:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "‚úÖ Exercise 8: Built deep network with proper initialization\n",
    "‚úÖ Exercise 9: Applied Batch Normalization + Dropout + L2\n",
    "‚úÖ Exercise 10: Used Adam/SGD+Momentum with learning rate scheduling\n",
    "\n",
    "RESULTS ACHIEVED:\n",
    "- Test Accuracy: ~90% (see Experiment 13 output above)\n",
    "- Proper regularization (minimal overfitting)\n",
    "- Fast convergence with callbacks\n",
    "- Production-ready pipeline\n",
    "\n",
    "Refer to the comprehensive Fashion MNIST model trained above! ‚úÖ\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéì CHAPTER 11: FINAL SUMMARY & KEY TAKEAWAYS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "‚úÖ COMPREHENSIVE CHAPTER 11 LEARNINGS:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "1. INITIALIZATION (Critical Foundation)\n",
    "   üèÜ Best Practice: He initialization for ReLU\n",
    "   üìä Impact: 10% ‚Üí 85% accuracy (75 percentage points!)\n",
    "   üí° Never initialize all weights to same value (symmetry!)\n",
    "\n",
    "2. ACTIVATION FUNCTIONS\n",
    "   üèÜ Best Practice: ReLU (default), ELU (better), SELU (very deep)\n",
    "   üìä Impact: Small differences (~1-2%), but critical for deep networks\n",
    "   üí° Avoid sigmoid/tanh in hidden layers (vanishing gradients)\n",
    "\n",
    "3. BATCH NORMALIZATION\n",
    "   üèÜ Best Practice: Use after Dense, before Activation\n",
    "   üìä Impact: +3.2% faster convergence in epoch 1\n",
    "   üí° Enables higher learning rates, acts as regularizer\n",
    "\n",
    "4. TRANSFER LEARNING\n",
    "   üèÜ Best Practice: Works when tasks similar + limited target data\n",
    "   üìä Impact: Variable (can hurt if tasks too different!)\n",
    "   üí° Freeze lower layers first, then fine-tune\n",
    "\n",
    "5. OPTIMIZERS (Game Changer!)\n",
    "   üèÜ Best Practice: SGD + Momentum (0.9) + Nesterov for production\n",
    "   üìä Impact: 86.65% ‚Üí 87.85% (+1.2%)\n",
    "   üí° Adam for prototyping (fast), SGD+Momentum for final models\n",
    "\n",
    "6. LEARNING RATE SCHEDULING\n",
    "   üèÜ Best Practice: ReduceLROnPlateau (adaptive, no tuning)\n",
    "   üìä Impact: +0.25% improvement, helps convergence\n",
    "   üí° Essential for very long training runs\n",
    "\n",
    "7. REGULARIZATION (Production Essential!)\n",
    "   üèÜ Best Practice: Dropout 10-30% (primary), weak L2 (secondary)\n",
    "   üìä Impact: 87.6% ‚Üí 88.8% with Dropout 10% (+1.2%)\n",
    "   üí° Dropout > L2 regularization for neural networks\n",
    "\n",
    "8. COMBINED BEST PRACTICES\n",
    "   üèÜ Best Practice: All techniques together\n",
    "   üìä Impact: Simple model 87.6% ‚Üí Best practices 90.05% (+2.45%)\n",
    "   üí° Production model achieves ~90% on Fashion MNIST!\n",
    "\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "üéØ DEFAULT CONFIGURATION (Copy-Paste Ready):\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(units, kernel_initializer='he_normal',\n",
    "                kernel_regularizer=regularizers.l2(0.0001)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dropout(0.2),\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(patience=5),\n",
    "    EarlyStopping(patience=15, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "üìö WHAT WE ACCOMPLISHED IN THIS CHAPTER:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "‚úÖ Understood vanishing/exploding gradients problem\n",
    "‚úÖ Mastered weight initialization strategies\n",
    "‚úÖ Compared activation functions systematically\n",
    "‚úÖ Implemented Batch Normalization properly\n",
    "‚úÖ Explored transfer learning (and its limitations)\n",
    "‚úÖ Tested modern optimizers (SGD, Adam, RMSprop, etc.)\n",
    "‚úÖ Applied learning rate scheduling strategies\n",
    "‚úÖ Mastered regularization (Dropout, L1/L2)\n",
    "‚úÖ Built production-grade models (90%+ accuracy)\n",
    "‚úÖ Solved all chapter exercises\n",
    "\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "üöÄ YOU NOW HAVE:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "- Deep understanding of training deep neural networks\n",
    "- Production-ready code templates\n",
    "- Systematic approach to hyperparameter selection\n",
    "- Debugging strategies for training issues\n",
    "- Best practices for real-world deployment\n",
    "\n",
    "üéì NEXT STEPS:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "- Apply these techniques to your own datasets\n",
    "- Experiment with different architectures (CNN, RNN)\n",
    "- Study Chapter 12 (Custom Models and Training)\n",
    "- Build production models with confidence!\n",
    "\n",
    "üéâ CONGRATULATIONS ON COMPLETING CHAPTER 11! üéâ\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ CHAPTER 11 COMPLETE - ALL PARTS FINISHED!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraud_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
